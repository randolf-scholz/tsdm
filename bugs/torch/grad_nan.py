#!/usr/bin/env python
# +
# # [Incorrect gradients in NaN-ignoring MSE #89543](https://github.com/pytorch/pytorch/issues/89543)
# +
import torch

N = 4
# create some observations with missing values
y = torch.tensor([1, 2, float("nan"), float("nan")])
x = torch.nn.Parameter(torch.randn(N), requires_grad=True)
w = torch.randn(N)
# -

# ## Absolute value works

# variant 1 correctly produces NaN-free gradient
x.grad = None
r = x - y
m = torch.isnan(y)
r = torch.where(m, 0.0, r)
r = w * r.abs()
loss = r.sum() / m.sum()
loss.backward()
assert not any(torch.isnan(x.grad))  # âœ”

# variant 2 incorrectly produces NaN-gradients
x.grad = None
r = x - y
r = w * r.abs()
m = torch.isnan(y)
r = torch.where(m, 0.0, r)
loss = r.sum() / m.sum()
loss.backward()
assert not any(torch.isnan(x.grad))  # âœ˜


# ## squared error does not work

# variant 1 correctly produces NaN-free gradient
x.grad = None
r = x - y
m = torch.isnan(y)
r = torch.where(m, 0.0, r)
r = w * r**2
loss = r.sum() / m.sum()
loss.backward()
assert not any(torch.isnan(x.grad))  # âœ”


# variant 2 incorrectly produces NaN-gradients
x.grad = None
r = x - y
r = w * r**2
m = torch.isnan(y)
r = torch.where(m, 0.0, r)
loss = r.sum() / m.sum()
loss.backward()
assert not any(torch.isnan(x.grad))  # âœ˜


# # with jacfwd

# +
import functorch
from torch import nn, tensor

x = nn.Parameter(tensor([1.1, 1.2, 1.3]))
y = tensor([1, 2, float("nan")])


def f(x):
    r = x - y
    r = r**2
    m = torch.isnan(y)
    r = torch.where(m, 0.0, r)
    return r.sum() / m.sum()


df = functorch.jacfwd(f)
print(df(x))
df = functorch.jacrev(f)
print(df(x))

# +
import functorch
from torch import nn, tensor

x = nn.Parameter(tensor([1.1, 1.2, 1.3]))
y = tensor([1, 2, float("nan")])


def f(x):
    r = x - y
    m = torch.isnan(y)
    r = torch.where(m, 0.0, r)
    r = r**2
    return r.sum() / m.sum()


df = functorch.jacfwd(f)
print(df(x))
df = functorch.jacrev(f)
print(df(x))

# +
import torch

x = torch.nn.Parameter(torch.tensor([1.1, 1.2, 1.3]))
y = torch.tensor([2, 3, float("nan")])
m = torch.tensor([True, True, False])

x.grad = None
r = (x - y) ** 2
r = torch.where(m, r, 0.0)
loss = r.sum() / m.sum()
loss.backward()
print(x.grad)
assert not any(torch.isnan(x.grad))  # âœ”

# +
import jax.numpy as np

x = np.array([1.2, 1.3, 1.4])
y = np.array([2, 3, 4])
m = np.array([False, False, True])


def f(x):
    r = x - y
    r = r**2
    # m = np.isnan(y)
    r = np.where(m, 0.0, r)
    return np.mean(r)


jax.grad(f)(x)
# -

# ## Calculation by hand

# - Case 1:  $f(x) = âˆ‘_i ([m \mathbin{?} x-y : 0]_i)^2$
# - Case 2:  $f(x) = âˆ‘_i [m \mathbin{?} (x-y)^2 : 0]_i$

# Gradient in case 1.
#
# Define intermediate quantities: $r = x - y$, $z = [m \mathbin{?} r : ğŸ]$, $y = âˆ‘ z_i^2$
#
#
# $$\begin{aligned}
# \frac{âˆ‚ f(x)}{âˆ‚x}
#    &= \frac{âˆ‚y}{âˆ‚z}âˆ˜\frac{âˆ‚z}{âˆ‚r}âˆ˜\frac{âˆ‚r}{âˆ‚x}
# \\ &= [âˆ†z â†¦ âŸ¨2zâˆ£âˆ†zâŸ©] âˆ˜ [âˆ†r â†¦ [m \mathbin{?} âˆ†r : ğŸ ]] âˆ˜ [âˆ†x â†¦ âˆ†x]  & g_z &= 2z
# \\ &= [âˆ†r â†¦ âŸ¨2zâˆ£[m \mathbin{?} âˆ†r : ğŸ ]âŸ©] âˆ˜ [âˆ†x â†¦ âˆ†x]
# \\ \text{(dubious step)} &= [âˆ†r â†¦ âŸ¨[m \mathbin{?}  2z : ğŸ]âˆ£ âˆ†râŸ©]  âˆ˜ [âˆ†x â†¦ âˆ†x]
# \\ &= [âˆ†r â†¦ âŸ¨[m \mathbin{?} 2[m \mathbin{?} x -y : ğŸ] : ğŸ]âˆ£ âˆ†râŸ©]  âˆ˜ [âˆ†x â†¦ âˆ†x]
# \\ &= [âˆ†r â†¦ âŸ¨[m \mathbin{?} 2(x -y) : ğŸ]âˆ£ âˆ†râŸ©]  âˆ˜ [âˆ†x â†¦ âˆ†x]   & g_r &= [m \mathbin{?}  2(x-y) : ğŸ]
# \\ &= [âˆ†x â†¦ âŸ¨[m \mathbin{?}  2(x-y) : ğŸ]âˆ£ âˆ†xâŸ©]                         & g_x &= [m \mathbin{?}  2(x-y) : ğŸ]
# \end{aligned}$$

# Gradient in case 2.
#
# Define intermediate quantities: $s = (x-y)^{âŠ™2}$, $z = [m \mathbin{?} s : ğŸ]$, $y = âˆ‘ z_i$
#
#
# $$\begin{aligned}
# \frac{âˆ‚ f(x)}{âˆ‚x}
#    &= \frac{âˆ‚y}{âˆ‚z}âˆ˜\frac{âˆ‚z}{âˆ‚s}âˆ˜\frac{âˆ‚s}{âˆ‚x}
# \\ &= [âˆ†z â†¦ âŸ¨ğŸâˆ£âˆ†zâŸ©] âˆ˜ [âˆ†s â†¦ [m \mathbin{?} âˆ†s : ğŸ ]] âˆ˜ [âˆ†x â†¦ 2(x-y)âŠ™âˆ†x]  & g_z &= ğŸ
# \\ &= [âˆ†s â†¦ âŸ¨ğŸâˆ£[m \mathbin{?} âˆ†s : ğŸ ]âŸ©] âˆ˜ [âˆ†x â†¦  2(x-y)âŠ™âˆ†x]
# \\ \text{(dubious step)} &= [âˆ†s â†¦ âŸ¨[m \mathbin{?}  ğŸ : ğŸ]âˆ£ âˆ†sâŸ©] âˆ˜ [âˆ†x â†¦ 2(x-y)âŠ™âˆ†x]      & g_s &= [m \mathbin{?}  ğŸ : ğŸ]
# \\ &= [âˆ†x â†¦ âŸ¨[m \mathbin{?}  ğŸ : ğŸ]âˆ£ 2(x-y)âŠ™âˆ†xâŸ©]
# \\ &= [âˆ†x â†¦ âŸ¨[m \mathbin{?}  ğŸ : ğŸ]âŠ™2(x-y)âˆ£ âˆ†xâŸ©]
# \\\text{(incorrect)} &= [âˆ†x â†¦ âŸ¨[m \mathbin{?}  ğŸâŠ™2(x-y) : ğŸâŠ™2(x-y)]âˆ£ âˆ†xâŸ©]           & g_x &= [m \mathbin{?}  2(x-y) : ğŸâŠ™2(x-y)]
# \\\text{(correct)} &= [âˆ†x â†¦ âŸ¨[m \mathbin{?}  ğŸâŠ™2(x-y) : ğŸ]âˆ£ âˆ†xâŸ©]           & g_x &= [m \mathbin{?}  2(x-y) : ğŸ]
# \end{aligned}$$

# ## Correct backwad of the `where` operator
#
# I suspect this is due to an error in the definition of the where backward. Thinking about it a bit, I came up with the following formula for the correct backward of the `where` operator:
#
# Let $y=[m \mathbin{?} a : b]$ and $a$ and $b$ functions of $x$. then
#
# $$\begin{aligned}
# \frac{âˆ‚y}{âˆ‚x} &= \frac{âˆ‚y}{âˆ‚(a, b)} âˆ˜ \frac{âˆ‚(a,b)}{âˆ‚x} \\
# &= \Bigl[ (âˆ†a, âˆ†b) âŸ¼ [m  \mathbin{?} {\color{green}{[m \mathbin{?} âˆ†a : ğŸ ]}} : {\color{green}{[m \mathbin{?} ğŸ : âˆ†b ]}}\Bigr]âˆ˜[âˆ†x âŸ¼ \bigl(\tfrac{âˆ‚a}{âˆ‚x}{âˆ†x}ï¼Œ \tfrac{âˆ‚b}{âˆ‚x}{âˆ†x}\bigr) ] \\
# \text{(correct)}&= \Bigl[ âˆ†x  âŸ¼ [m  \mathbin{?} {\color{green}{[m \mathbin{?} \tfrac{âˆ‚a}{âˆ‚x}{âˆ†x} : ğŸ ]}} : {\color{green}{[m \mathbin{?} ğŸ : \tfrac{âˆ‚b}{âˆ‚x}{âˆ†x} ]}}\Bigr] \\
# \text{(incorrect)}&= \Bigl[ âˆ†x  âŸ¼ [m  \mathbin{?} {\color{red}{\tfrac{âˆ‚a}{âˆ‚x}{âˆ†x}}} : {\color{red}{\tfrac{âˆ‚b}{âˆ‚x}{âˆ†x}}}]\Bigr]   \qquad\textbf{this simplification is incorrect if there are NaN values!}
# \end{aligned}$$

# ## Correct vector-Jacobian product for the backward
#
#
# $$\begin{aligned}
# \frac{âˆ‚â„“}{âˆ‚y}\frac{âˆ‚y}{âˆ‚x} &= [âˆ†y â†¦ \frac{âˆ‚â„“}{âˆ‚y}{âˆ†y}] âˆ˜ \Bigl[ âˆ†x  âŸ¼ [m  \mathbin{?} {\color{green}{[m \mathbin{?} \tfrac{âˆ‚a}{âˆ‚x}{âˆ†x} : ğŸ ]}} : {\color{green}{[m \mathbin{?} ğŸ : \tfrac{âˆ‚b}{âˆ‚x}{âˆ†x} ]}}\Bigr] \\
# &= \Bigl[ (âˆ†a, âˆ†b) âŸ¼ [m  \mathbin{?} {\color{green}{[m \mathbin{?} âˆ†a : ğŸ ]}} : {\color{green}{[m \mathbin{?} ğŸ : âˆ†b ]}}\Bigr]âˆ˜[âˆ†x âŸ¼ \bigl(\tfrac{âˆ‚a}{âˆ‚x}{âˆ†x}ï¼Œ \tfrac{âˆ‚b}{âˆ‚x}{âˆ†x}\bigr) ] \\
# \text{(correct)}&= \Bigl[ âˆ†x  âŸ¼ [m  \mathbin{?} {\color{green}{[m \mathbin{?} \tfrac{âˆ‚a}{âˆ‚x}{âˆ†x} : ğŸ ]}} : {\color{green}{[m \mathbin{?} ğŸ : \tfrac{âˆ‚b}{âˆ‚x}{âˆ†x} ]}}\Bigr] \\
# \text{(incorrect)}&= \Bigl[ âˆ†x  âŸ¼ [m  \mathbin{?} {\color{red}{\tfrac{âˆ‚a}{âˆ‚x}{âˆ†x}}} : {\color{red}{\tfrac{âˆ‚b}{âˆ‚x}{âˆ†x}}}]\Bigr]
# \end{aligned}$$
#
#
