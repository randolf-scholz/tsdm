DataLoaders
===========

There are two ways of using dataloaders:

1. Small Datasets (fits in RAM)

   - pre-encode the whole dataset and apply dataloader to the result
   - here the dataloader already acts on a torch tensor

2. Large Datasets (does not fit in RAM)

   - load batch from disk
   - encode batch on the fly

Time-Series Specifics
---------------------

In time series, especially forecasting, we typically want to sample time slices.
Those timeslices can either be generated by index, or by the actual timestamps.

Note that the latter automatically leads to non-uniform batch sizes during training.

1.  Subclass `torch.dataset`, implement `__init__`, `__len__` and `__getitem__`.

Note that by construction, the standard dataloader with fixed batch-size will just sample fixed size
batches -> not what we want!

.. code-block:: python

   DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
              batch_sampler=None, num_workers=0, collate_fn=None,
              pin_memory=False, drop_last=False, timeout=0,
              worker_init_fn=None, *, prefetch_factor=2,
              persistent_workers=False)

Thankfully, we can create an appropriate training dataloader by writing

- `sampler` – defines the strategy to draw samples from the dataset
- `batch_sampler`: a torch.utils.data.Sampler subclass that is responsible for collecting batches
   returns list of items (uses `__getitem__` of dataset!)
- `collate_fn`: responsible for compiling the list of items into a `Union[Tensor, tuple[Tensor, ...]]`


Now, consider the case when the dataset consists of multiple multivariate time series
:math:`T_k = (t_i^{(k)}, x_i^{(k)}, y_i^{(k)})_{i=1:n_k}`,
In general, a single training instance will be created by randomly sampling a time point $τ$ and
then selecting all data point for which :math:`t_i^\text{train} ∈ [τ- {∆t}_\text{obs}, τ]`, and
:math:`t_i^\text{test}\in [τ, τ+ {∆t}_\text{pred}]`.

Thus, the different samples will generally have different lengths, making stacking them a non-trivial task.
Towards this end there are 3 approaches:

1. avoid batching altogether - perform single instance forward passes and accumulate gradients instead.
2. Use a padded format
3. Use a packed format
