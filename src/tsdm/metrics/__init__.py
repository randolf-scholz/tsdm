r"""Implementation of loss functions.

Theory
------
We define the following

1. A metric is a  function

    .. math:: ùî™Ôºö ùìü_0(ùì®√óùì®) ‚ü∂ ‚Ñù_{‚â•0}

    such that $ùî™(\{ (y_i, \hat{y}_i) ‚à£ i=1:n \}) = 0$ if and only if $y_i=\hat{y}_i‚àÄi$

2. A metric is called decomposable, if it can be written as a function

    .. math
        ùî™ = Œ®‚àò(‚Ñì√óùóÇùñΩ)
        ‚ÑìÔºö ùì®√óùì® ‚ü∂ ‚Ñù_{‚â•0}
        Œ®Ôºö ùìü_0(‚Ñù_{‚â•0}) ‚ü∂ ‚Ñù_{‚â•0}

    I.e. the function $‚Ñì$ is applied element-wise to all pairs $(y, \hat{y}$ and the function $Œ®$
    "accumulates" the results. Oftentimes, $Œ®$ is just the sum/mean/expectation value, although
    other accumulations such as the median value are also possible.

3. A metric is called instance-wise, if it can be written in the form

    .. math::
        ùî™Ôºö ùìü_0(ùì®√óùì®) ‚ü∂ ‚Ñù_{‚â• 0}, ùî™(\{(y_i, \hat{y}_i) ‚à£  i=1:n \})
        = ‚àë_{i=1}^n œâ(i, n)‚Ñì(y_i, \hat{y}_i)

4. A metric is called a loss-function, if and only if

   - It is differentiable almost everywhere.
   - It is non-constant, at least on some open set.

Note that in the context of time-series, we allow the accumulator to depend on the time variable.

Notes
-----
Contains losses in both modular and functional form.
  - See `tsdm.losses.functional` for functional implementations.
  - See `tsdm.losses` for modular implementations.
"""

__all__ = [
    # Sub-Modules
    "functional",
    # Types
    "Loss",
    "FunctionalLoss",
    "ModularLoss",
    # Constants
    "LOSSES",
    "FUNCTIONAL_LOSSES",
    "ModularLosses",
    # Classes
    "ND",
    "NRMSE",
    "Q_Quantile",
    "Q_Quantile_Loss",
    "WRMSE",
    "RMSE",
    # Functions
    "nd",
    "nrmse",
    "rmse",
    "q_quantile",
    "q_quantile_loss",
]


from typing import Final, TypeAlias

from torch import nn

from tsdm.metrics._modular import ND, NRMSE, RMSE, WRMSE, Q_Quantile, Q_Quantile_Loss
from tsdm.metrics.functional import (
    FUNCTIONAL_LOSSES,
    FunctionalLoss,
    nd,
    nrmse,
    q_quantile,
    q_quantile_loss,
    rmse,
)

ModularLoss: TypeAlias = nn.Module
r"""Type hint for modular losses."""

Loss: TypeAlias = FunctionalLoss | ModularLoss
r"""Type hint for losses."""

TORCH_LOSSES: Final[dict[str, type[nn.Module]]] = {
    "L1": nn.L1Loss,
    "CosineEmbedding": nn.CosineEmbeddingLoss,
    "CrossEntropy": nn.CrossEntropyLoss,
    "CTC": nn.CTCLoss,
    "NLL": nn.NLLLoss,
    "PoissonNLL": nn.PoissonNLLLoss,
    "GaussianNLL": nn.GaussianNLLLoss,
    "KLDiv": nn.KLDivLoss,
    "BCE": nn.BCELoss,
    "BCEWithLogits": nn.BCEWithLogitsLoss,
    "MarginRanking": nn.MarginRankingLoss,
    "MSE": nn.MSELoss,
    "HingeEmbedding": nn.HingeEmbeddingLoss,
    "Huber": nn.HuberLoss,
    "SmoothL1": nn.SmoothL1Loss,
    "SoftMargin": nn.SoftMarginLoss,
    "MultiMargin": nn.MultiMarginLoss,
    "MultiLabelMargin": nn.MultiLabelMarginLoss,
    "MultiLabelSoftMargin": nn.MultiLabelSoftMarginLoss,
    "TripletMargin": nn.TripletMarginLoss,
    "TripletMarginWithDistance": nn.TripletMarginWithDistanceLoss,
}
r"""Dictionary of all available modular losses in torch."""

TORCH_ALIASES: Final[dict[str, type[nn.Module]]] = {
    "MAE": nn.L1Loss,
    "L2": nn.MSELoss,
    "XENT": nn.CrossEntropyLoss,
    "KL": nn.KLDivLoss,
}
r"""Dictionary containing additional aliases for modular losses in torch."""

ModularLosses: Final[dict[str, type[nn.Module]]] = {
    "ND": ND,
    "NRMSE": NRMSE,
    "Q_Quantile": Q_Quantile,
    "Q_Quantile_Loss": Q_Quantile_Loss,
    "RMSE": RMSE,
} | (TORCH_LOSSES | TORCH_ALIASES)
r"""Dictionary of all available modular losses."""


LOSSES: Final[dict[str, FunctionalLoss | type[ModularLoss]]] = {
    **FUNCTIONAL_LOSSES,
    **ModularLosses,
}
r"""Dictionary of all available losses."""
