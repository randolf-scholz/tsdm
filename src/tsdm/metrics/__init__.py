r"""Implementation of loss functions.

Theory
------
We define the following

1. A metric is a  function

    .. math:: ùî™Ôºö ùìü_0(ùì®√óùì®) ‚ü∂ ‚Ñù_{‚â•0}

    such that $ùî™(\{ (y_i, \hat{y}_i) ‚à£ i=1:n \}) = 0$ if and only if $y_i=\hat{y}_i‚àÄi$

2. A metric is called decomposable, if it can be written as a function

    .. math
        ùî™ = Œ®‚àò(‚Ñì√óùóÇùñΩ)
        ‚ÑìÔºö ùì®√óùì® ‚ü∂ ‚Ñù_{‚â•0}
        Œ®Ôºö ùìü_0(‚Ñù_{‚â•0}) ‚ü∂ ‚Ñù_{‚â•0}

    I.e. the function $‚Ñì$ is applied element-wise to all pairs $(y, \hat{y}$ and the function $Œ®$
    "accumulates" the results. Oftentimes, $Œ®$ is just the sum/mean/expectation value, although
    other accumulations such as the median value are also possible.

3. A metric is called instance-wise, if it can be written in the form

    .. math::
        ùî™Ôºö ùìü_0(ùì®√óùì®) ‚ü∂ ‚Ñù_{‚â• 0}, ùî™(\{(y_i, \hat{y}_i) ‚à£  i=1:n \})
        = ‚àë_{i=1}^n œâ(i, n)‚Ñì(y_i, \hat{y}_i)

4. A metric is called a loss-function, if and only if

   - It is differentiable almost everywhere.
   - It is non-constant, at least on some open set.

Note that in the context of time-series, we allow the accumulator to depend on the time variable.

Notes
-----
Contains losses in both modular and functional form.

- See `tsdm.losses.functional` for functional implementations.
- See `tsdm.losses` for modular implementations.
"""

__all__ = [
    # Sub-Modules
    "functional",
    # Types
    "BaseLoss",
    "FunctionalLoss",
    # Constants
    "LOSSES",
    "FUNCTIONAL_LOSSES",
    "MODULAR_LOSSES",
    # Base Classes
    "BaseLoss",
    "WeightedLoss",
    # Classes
    "MAE",
    "MSE",
    "RMSE",
    "WMAE",
    "WMSE",
    "WRMSE",
    # Functions
    "nd",
    "nrmse",
    "rmse",
    "q_quantile",
    "q_quantile_loss",
]


from abc import ABCMeta

from torch import nn

from tsdm.metrics._modular import (
    MAE,
    MSE,
    RMSE,
    WMAE,
    WMSE,
    WRMSE,
    BaseLoss,
    Loss,
    WeightedLoss,
)
from tsdm.metrics._timeseries import (
    ND,
    NRMSE,
    Q_Quantile,
    Q_Quantile_Loss,
    TimeSeriesMSE,
    TimeSeriesWMSE,
)
from tsdm.metrics.functional import (
    FUNCTIONAL_LOSSES,
    FunctionalLoss,
    nd,
    nrmse,
    q_quantile,
    q_quantile_loss,
    rmse,
)

TORCH_ALIASES: dict[str, type[nn.Module]] = {
    "MAE": nn.L1Loss,
    "L2": nn.MSELoss,
    "XENT": nn.CrossEntropyLoss,
    "KL": nn.KLDivLoss,
}
r"""Dictionary containing additional aliases for modular losses in torch."""


TORCH_LOSSES: dict[str, type[nn.Module]] = {
    "L1": nn.L1Loss,
    "CosineEmbedding": nn.CosineEmbeddingLoss,
    "CrossEntropy": nn.CrossEntropyLoss,
    "CTC": nn.CTCLoss,
    "NLL": nn.NLLLoss,
    "PoissonNLL": nn.PoissonNLLLoss,
    "GaussianNLL": nn.GaussianNLLLoss,
    "KLDiv": nn.KLDivLoss,
    "BCE": nn.BCELoss,
    "BCEWithLogits": nn.BCEWithLogitsLoss,
    "MarginRanking": nn.MarginRankingLoss,
    "MSE": nn.MSELoss,
    "HingeEmbedding": nn.HingeEmbeddingLoss,
    "Huber": nn.HuberLoss,
    "SmoothL1": nn.SmoothL1Loss,
    "SoftMargin": nn.SoftMarginLoss,
    "MultiMargin": nn.MultiMarginLoss,
    "MultiLabelMargin": nn.MultiLabelMarginLoss,
    "MultiLabelSoftMargin": nn.MultiLabelSoftMarginLoss,
    "TripletMargin": nn.TripletMarginLoss,
    "TripletMarginWithDistance": nn.TripletMarginWithDistanceLoss,
} | TORCH_ALIASES
r"""Dictionary of all available modular losses in torch."""


MODULAR_LOSSES: dict[str, type[Loss]] = {
    "ND": ND,
    "NRMSE": NRMSE,
    "Q_Quantile": Q_Quantile,
    "Q_Quantile_Loss": Q_Quantile_Loss,
    "MAE": MAE,
    "MSE": MSE,
    "RMSE": RMSE,
    "WMAE": WMAE,
    "WMSE": WMSE,
    "WRMSE": WRMSE,
    "TimeSeriesMSE": TimeSeriesMSE,
    "TimeSeriesWMSE": TimeSeriesWMSE,
}
r"""Dictionary of all available modular losses."""


LOSSES: dict[str, FunctionalLoss | type[Loss]] = {
    **FUNCTIONAL_LOSSES,
    **MODULAR_LOSSES,
}
r"""Dictionary of all available losses."""

del nn, ABCMeta
