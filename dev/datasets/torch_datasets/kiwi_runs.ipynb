{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiwi Runs - conceptual\n",
    "\n",
    "Conceptually, this dataset is a `Mapping[TimeSeriesDataset]`, or even a `Mapping[Mapping[TimeSeriesDataset]]`, depending on how we treat the `run_id`, `experiment_id` Multi-Index.\n",
    "\n",
    "Where a `TimeSeriesDataset` consists of \n",
    "  - time series data (typically `tuple[TimeTensor]`)\n",
    "  - and static metadata (typically `tuple[Tensor]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4, floatmode=\"fixed\", suppress=True)\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections.abc import *\n",
    "from itertools import chain, count\n",
    "from typing import Any, NamedTuple, Optional, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pandas import DataFrame, Index, Interval, MultiIndex, Series, Timedelta, Timestamp\n",
    "from torch import Tensor\n",
    "from torch.utils.data import *\n",
    "from torch.utils.data import Dataset as Torch_Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from tsdm.datasets import KIWI_RUNS\n",
    "from tsdm.datasets.torch import TimeSeriesDataset, TimeTensor\n",
    "from tsdm.encoders import *\n",
    "from tsdm.utils.strings import repr_mapping, repr_sequence\n",
    "\n",
    "IndexedArray = Union[Series, DataFrame, TimeTensor]\n",
    "r\"\"\"Type Hint for IndexedArrays.\"\"\"\n",
    "\n",
    "_IndexedArray = (Series, DataFrame, TimeTensor)\n",
    "r\"\"\"TODO: replace with python 3.10\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = KIWI_RUNS()\n",
    "ts = ds.timeseries.drop([355, 445, 482]).astype(float)\n",
    "md = ds.metadata.drop([355, 445, 482])\n",
    "target = \"Fluo_GFP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Timestamp Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_induction_time(s: Series) -> Timestamp:\n",
    "    # Compute the induction time\n",
    "    # s = ts.loc[run_id, experiment_id]\n",
    "    inducer = s[\"InducerConcentration\"]\n",
    "    total_induction = inducer[-1] - inducer[0]\n",
    "\n",
    "    if pd.isna(total_induction) or total_induction == 0:\n",
    "        return pd.NA\n",
    "\n",
    "    diff = inducer.diff()\n",
    "    mask = pd.notna(diff) & (diff != 0.0)\n",
    "    inductions = inducer[mask]\n",
    "    assert len(inductions) == 1, \"Multiple Inductions occur!\"\n",
    "    return inductions.first_valid_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_product(s: Series, target) -> Timestamp:\n",
    "    # Final and target times\n",
    "    targets = s[target]\n",
    "    mask = pd.notna(targets)\n",
    "    targets = targets[mask]\n",
    "    assert len(targets) >= 1, f\"not enough target observations {targets}\"\n",
    "    return targets.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_table(ts: DataFrame, target=\"Fluo_GFP\", t_min=\"0.6h\", delta_t=\"5m\"):\n",
    "    columns = [\n",
    "        \"slice\",\n",
    "        \"t_min\",\n",
    "        \"t_induction\",\n",
    "        \"t_max\",\n",
    "        \"t_target\",\n",
    "    ]\n",
    "    index = ts.reset_index(level=[2]).index.unique()\n",
    "    df = DataFrame(index=index, columns=columns)\n",
    "\n",
    "    min_wait = Timedelta(t_min)\n",
    "\n",
    "    for idx, slc in tqdm(ts.groupby(level=[0, 1])):\n",
    "        slc = slc.reset_index(level=[0, 1], drop=True)\n",
    "        # display(slc)\n",
    "        t_induction = get_induction_time(slc)\n",
    "        t_target = get_final_product(slc, target=target)\n",
    "        if pd.isna(t_induction):\n",
    "            print(f\"{idx}: no t_induction!\")\n",
    "            t_max = get_final_product(slc.loc[slc.index < t_target], target=target)\n",
    "            assert t_max < t_target\n",
    "        else:\n",
    "            assert t_induction < t_target, f\"{t_induction=} after {t_target}!\"\n",
    "            t_max = t_induction\n",
    "        df.loc[idx, \"t_max\"] = t_max\n",
    "\n",
    "        df.loc[idx, \"t_min\"] = t_min = slc.index[0] + min_wait\n",
    "        df.loc[idx, \"t_induction\"] = t_induction\n",
    "        df.loc[idx, \"t_target\"] = t_target\n",
    "        df.loc[idx, \"slice\"] = slice(t_min, t_max)\n",
    "        # = t_final\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_product_times = get_time_table(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Vector Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vec = {}\n",
    "for idx in md.index:\n",
    "    t_target = final_product_times.loc[idx, \"t_target\"]\n",
    "    final_vec[(*idx, t_target)] = ts.loc[idx].loc[t_target]\n",
    "\n",
    "final_vec = DataFrame.from_dict(final_vec, orient=\"index\")\n",
    "final_vec.index = final_vec.index.set_names(ts.index.names)\n",
    "final_vec = final_vec[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Dataset Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMapping(Mapping, Torch_Dataset):\n",
    "    r\"\"\"Represents a ``mapping[index → torch.Datasets]``.\n",
    "\n",
    "    All tensors must have a shared index,\n",
    "    in the sense that index.unique() is identical for all inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset: dict[Any, Torch_Dataset]\n",
    "    \"\"\"The dataset\"\"\"\n",
    "\n",
    "    def __init__(self, indexed_datasets: Mapping[Any, Torch_Dataset]):\n",
    "        super().__init__()\n",
    "        self.dataset = dict(indexed_datasets)\n",
    "        self.index = self.dataset.keys()\n",
    "        self.keys = self.dataset.keys  # type: ignore[assignment]\n",
    "        self.values = self.dataset.values  # type: ignore[assignment]\n",
    "        self.items = self.dataset.items  # type: ignore[assignment]\n",
    "\n",
    "    def __len__(self):\n",
    "        r\"\"\"Length of the dataset.\"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        r\"\"\"Hierarchical lookup.\"\"\"\n",
    "        # test for hierarchical indexing\n",
    "        if isinstance(item, Sequence):\n",
    "            first, rest = item[0], item[1:]\n",
    "            if isinstance(first, (Iterable, slice)):\n",
    "                # pass remaining indices to sub-object\n",
    "                value = self.dataset[first]\n",
    "                return value[rest]\n",
    "\n",
    "        # no hierarchical indexing\n",
    "        return self.dataset[item]\n",
    "\n",
    "    def __iter__(self):\n",
    "        r\"\"\"Iterate over the dataset.\"\"\"\n",
    "        for key in self.index:\n",
    "            yield self.dataset[key]\n",
    "\n",
    "    def __repr__(self):\n",
    "        r\"\"\"Representation of the dataset.\"\"\"\n",
    "        return repr_mapping(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeLikeTensor = Any\n",
    "StaticTensor = Any\n",
    "\n",
    "\n",
    "def repr_array(obj, title: Optional[str] = None):\n",
    "    if hasattr(obj, \"shape\"):\n",
    "        return type(obj).__name__ + \"[\" + str(obj.shape) + \"]\"\n",
    "    return \"[\" + \", \".join(repr_array(x) for x in obj) + \"]\"\n",
    "\n",
    "\n",
    "def repr_singleton_or_tuple(obj, repr_fun: Optional[Callable] = None):\n",
    "    to_string = repr if repr_fun is None else repr_fun\n",
    "\n",
    "    if isinstance(obj, tuple):\n",
    "        return repr_sequence(obj, repr_fun=to_string)\n",
    "    return to_string(obj)\n",
    "\n",
    "\n",
    "class TimeSeriesSlice(NamedTuple):\n",
    "    timeseries: tuple[TimeLikeTensor, ...]\n",
    "    metadata: tuple[StaticTensor, ...]\n",
    "\n",
    "    def __repr__(self):\n",
    "        pad = \" \" * 2\n",
    "        string = type(self).__name__ + \"(\"\n",
    "        string += (\n",
    "            \"\\n\"\n",
    "            + pad\n",
    "            + \"timeseries=\"\n",
    "            + repr_singleton_or_tuple(self.timeseries, repr_array)\n",
    "        )\n",
    "        string += (\n",
    "            \"\\n\"\n",
    "            + pad\n",
    "            + \"metadata=\"\n",
    "            + repr_singleton_or_tuple(self.metadata, repr_array)\n",
    "        )\n",
    "        string += \"\\n)\"\n",
    "        return string\n",
    "\n",
    "\n",
    "def tensor_info(x: Tensor) -> str:\n",
    "    r\"\"\"Print useful information about Tensor.\"\"\"\n",
    "    return f\"{x.__class__.__name__}[{tuple(x.shape)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(TensorDataset):\n",
    "    \"\"\"A general Time Series Dataset.\n",
    "\n",
    "    Consists of 2 things\n",
    "    - timeseries: TimeTensor / tuple[TimeTensor]\n",
    "    - metadata: Tensor / tuple[Tensor]\n",
    "\n",
    "    When retrieving items, we generally use slices:\n",
    "\n",
    "    - ds[timestamp] = ds[timestamp:timestamp]\n",
    "    - ds[t₀:t₁] = tuple[X[t₀:t₁] for X in self.timeseries], metadata\n",
    "    \"\"\"\n",
    "\n",
    "    timeseries: Union[TimeLikeTensor, tuple[TimeLikeTensor, ...]]\n",
    "    metadata: Optional[Union[StaticTensor, tuple[StaticTensor, ...]]] = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        timeseries: Union[IndexedArray, Collection[IndexedArray]],\n",
    "        *timetensors: IndexedArray,\n",
    "        metadata: Optional[Union[Tensor, Collection[Tensor]]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        ts_tensors = (\n",
    "            [timeseries] if isinstance(timeseries, _IndexedArray) else list(timeseries)\n",
    "        )\n",
    "        ts_tensors.extend(timetensors)\n",
    "        self.timeseries = ts_tensors[0] if len(ts_tensors) == 1 else tuple(ts_tensors)\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        r\"\"\"Pretty print.\"\"\"\n",
    "        pad = r\"  \"\n",
    "\n",
    "        if isinstance(self.timeseries, tuple):\n",
    "            ts_lines = [tensor_info(tensor) for tensor in self.timeseries]\n",
    "        else:\n",
    "            ts_lines = [tensor_info(self.timeseries)]\n",
    "\n",
    "        if self.metadata is None:\n",
    "            md_lines = [f\"{None}\"]\n",
    "        elif isinstance(self.metadata, tuple):\n",
    "            md_lines = [tensor_info(tensor) for tensor in self.metadata]\n",
    "        else:\n",
    "            md_lines = [tensor_info(self.metadata)]\n",
    "\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(\"\n",
    "            + \"\".join([\"\\n\" + 2 * pad + line for line in ts_lines])\n",
    "            + \"\\n\"\n",
    "            + pad\n",
    "            + \"metadata:\"\n",
    "            + \"\".join([\"\\n\" + 2 * pad + line for line in md_lines])\n",
    "            + \"\\n\"\n",
    "            + \")\"\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        r\"\"\"Return the length of the longest timeseries.\"\"\"\n",
    "        if isinstance(self.timeseries, tuple):\n",
    "            minval = min(min(ts.index) for ts in self.timeseries)\n",
    "            maxval = max(max(ts.index) for ts in self.timeseries)\n",
    "            return maxval - minval\n",
    "        minval = min(self.timeseries.index)\n",
    "        maxval = max(self.timeseries.index)\n",
    "        return maxval - minval\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        r\"\"\"Return corresponding slice from each tensor.\"\"\"\n",
    "        if isinstance(self.timeseries, tuple):\n",
    "            ts = tuple(tensor.loc[item] for tensor in self.timeseries)\n",
    "        else:\n",
    "            ts = self.timeseries.loc[item]\n",
    "        md = self.metadata\n",
    "        return TimeSeriesSlice(ts, md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MappingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingDataset(Torch_Dataset, Mapping):\n",
    "    r\"\"\"Represents a Mapping[Key, Dataset].\"\"\"\n",
    "\n",
    "    def __init__(self, data: Mapping, prepend_key: bool = False):\n",
    "        super().__init__()\n",
    "        assert isinstance(data, Mapping)\n",
    "        if isinstance(data, Mapping):\n",
    "            self.index = data.keys()\n",
    "            self.data = data\n",
    "        self.prepend_key = prepend_key\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        return iter(self.index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def _lookup(self, key):\n",
    "        if not isinstance(key, tuple):\n",
    "            return self.data[key]\n",
    "        try:\n",
    "            outer = self.data[key[0]]\n",
    "            return outer[key[1:]]\n",
    "        except KeyError:\n",
    "            return self.data[key]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if self.prepend_key:\n",
    "            return key, self._lookup(key)\n",
    "        return self._lookup(key)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dataframe(df: DataFrame, levels: Optional[list[str]] = None):\n",
    "        if levels is not None:\n",
    "            mindex = df.index.to_frame()\n",
    "            subidx = MultiIndex.from_frame(mindex[levels])\n",
    "            index = subidx.unique()\n",
    "        else:\n",
    "            index = df.index\n",
    "\n",
    "        return MappingDataset({idx: df.loc[idx] for idx in index})\n",
    "\n",
    "    def __repr__(self):\n",
    "        r\"\"\"Representation of the dataset.\"\"\"\n",
    "        return repr_mapping(self)  # , repr_fun=repr_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Sampler Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MappingSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingSampler(Sampler):\n",
    "    def __init__(self, data_source: Mapping, shuffle: bool = True):\n",
    "        # super().__init__(data_source)\n",
    "        self.data = data_source\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        perm = np.random.permutation(list(self.data.keys()))\n",
    "        for k in perm:\n",
    "            yield self.data[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IntervalSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimedeltaLike = TypeVar(\"TimedeltaLike\", int, float, Timedelta)\n",
    "TimestampLike = TypeVar(\"TimestampLike\", int, float, Timestamp)\n",
    "\n",
    "\n",
    "def grid(\n",
    "    xmin: TimestampLike,\n",
    "    xmax: TimestampLike,\n",
    "    delta: TimedeltaLike,\n",
    "    xoffset: Optional[TimestampLike] = None,\n",
    ") -> list[int]:\n",
    "    \"\"\"Computes `\\{k∈ℤ∣ xₘᵢₙ ≤ x₀+k⋅Δ ≤ xₘₐₓ\\}`.\n",
    "\n",
    "    Special case: if Δ=0, returns [0]\n",
    "    \"\"\"\n",
    "\n",
    "    xo = xmin if xoffset is None else xoffset\n",
    "    zero = type(delta)(0)\n",
    "\n",
    "    if delta == zero:\n",
    "        return [0]\n",
    "\n",
    "    assert delta > zero, \"Assumption delta>0 violated!\"\n",
    "    assert xmin <= xoffset <= xmax, \"Assumption: xmin≤xoffset≤xmax violated!\"\n",
    "\n",
    "    a = xmin - xoffset\n",
    "    b = xmax - xoffset\n",
    "    kmax = b // delta\n",
    "    kmin = a // delta\n",
    "\n",
    "    assert xmin <= xo + kmin * delta\n",
    "    assert xmin > xo + (kmin - 1) * delta\n",
    "    assert xmax >= xo + kmax * delta\n",
    "    assert xmax < xo + (kmax + 1) * delta\n",
    "\n",
    "    return list(range(kmin, kmax + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = TypeVar(\"V\")\n",
    "\n",
    "Boxed = Union[\n",
    "    Sequence[V],\n",
    "    Mapping[int, V],\n",
    "    Callable[[int], V],\n",
    "]\n",
    "\n",
    "dt_type = Union[\n",
    "    TimedeltaLike,\n",
    "    Sequence[TimedeltaLike],\n",
    "    Mapping[int, TimedeltaLike],\n",
    "    Callable[[int], TimedeltaLike],\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "\n",
    "class IntervalSampler(\n",
    "    Sampler,\n",
    "):\n",
    "    \"\"\"Returns all intervals `[a, b]` such that:\n",
    "\n",
    "    - `a = t₀ + i⋅sₖ`\n",
    "    - `b = t₀ + i⋅sₖ + Δtₖ`\n",
    "    - `i, k ∈ ℤ`\n",
    "    - `a ≥ t_min`\n",
    "    - `b ≤ t_max`\n",
    "    - `sₖ` is the stride corresponding to intervals of size `Δtₖ`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xmin,\n",
    "        xmax,\n",
    "        deltax: dt_type,\n",
    "        stride: Optional[dt_type] = None,\n",
    "        levels: Optional[Sequence[int]] = None,\n",
    "        offset: Optional[dt_type] = None,\n",
    "        multiples: bool = True,\n",
    "        shuffle: bool = True,\n",
    "    ) -> None:\n",
    "        # set stride and offset\n",
    "        zero = 0 * (xmax - xmin)\n",
    "        stride = zero if stride is None else stride\n",
    "        offset = xmin if offset is None else offset\n",
    "\n",
    "        # validate bounds\n",
    "        assert xmin <= offset <= xmax, \"Assumption: xmin≤xoffset≤xmax violated!\"\n",
    "\n",
    "        # determine delta_max\n",
    "        delta_max = max(offset - xmin, xmax - offset)\n",
    "\n",
    "        # determine levels\n",
    "        if levels is None:\n",
    "            if isinstance(deltax, Mapping):\n",
    "                levels = [k for k in deltax.keys() if deltax[k] <= delta_max]\n",
    "            elif isinstance(deltax, Sequence):\n",
    "                levels = [k for k in range(len(deltax)) if deltax[k] <= delta_max]\n",
    "            elif isinstance(deltax, Callable):\n",
    "                levels = []\n",
    "                for k in count():\n",
    "                    dt = self._get_value(deltax, k)\n",
    "                    if dt == zero:\n",
    "                        continue\n",
    "                    if dt > delta_max:\n",
    "                        break\n",
    "                    levels.append(k)\n",
    "            else:\n",
    "                levels = [0]\n",
    "        else:\n",
    "            levels = [k for k in levels if self._get_value(deltax, k) <= delta_max]\n",
    "\n",
    "        # validate levels\n",
    "        assert all(self._get_value(deltax, k) <= delta_max for k in levels)\n",
    "\n",
    "        # compute valid intervals\n",
    "        intervals: list[Interval] = []\n",
    "\n",
    "        # for each level, get all intervals\n",
    "        for k in levels:\n",
    "            dt = self._get_value(deltax, k)\n",
    "            st = self._get_value(stride, k)\n",
    "            x0 = self._get_value(offset, k)\n",
    "\n",
    "            # get valid interval bounds, probably there is an easier way to do it...\n",
    "            stridesa = grid(xmin, xmax, st, x0)\n",
    "            stridesb = grid(xmin, xmax, st, x0 + dt)\n",
    "            valid_strides = set.intersection(set(stridesa), set(stridesb))\n",
    "\n",
    "            if not valid_strides:\n",
    "                break\n",
    "\n",
    "            intervals.extend([\n",
    "                (x0 + i * st, x0 + i * st + dt, dt, st) for i in valid_strides\n",
    "            ])\n",
    "\n",
    "        # set variables\n",
    "        self.offset = offset\n",
    "        self.deltax = deltax\n",
    "        self.stride = stride\n",
    "        self.shuffle = shuffle\n",
    "        self.intervals = DataFrame(\n",
    "            intervals, columns=[\"left\", \"right\", \"delta\", \"stride\"]\n",
    "        )\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        if self.shuffle:\n",
    "            perm = np.random.permutation(len(self))\n",
    "        else:\n",
    "            perm = np.arange(len(self))\n",
    "\n",
    "        for k in perm:\n",
    "            yield slice(self.loc[k, \"left\"], self.loc[k, \"right\"])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.intervals)\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        return self.intervals.__getattr__(key)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.intervals[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_value(obj: Union[V, Boxed[V]], k: int) -> V:\n",
    "        if isinstance(obj, Callable):\n",
    "            return obj(k)\n",
    "        if isinstance(obj, Sequence):\n",
    "            return obj[k]\n",
    "        # Fallback: multiple!\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HierarchicalSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalSampler(Sampler):\n",
    "    r\"\"\"Samples a single random dataset from a collection of dataset.\n",
    "\n",
    "    Optionally, we can delegate a subsampler to then sample from the randomly drawn dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    idx: Index\n",
    "    r\"\"\"The shared index.\"\"\"\n",
    "    subsamplers: Mapping[Any, Sampler]\n",
    "    r\"\"\"The subsamplers to sample from the collection.\"\"\"\n",
    "    early_stop: bool = False\n",
    "    r\"\"\"Whether to stop sampling when the index is exhausted.\"\"\"\n",
    "    shuffle: bool = True\n",
    "    r\"\"\"Whether to sample in random order.\"\"\"\n",
    "    sizes: Series\n",
    "    r\"\"\"The sizes of the subsamplers.\"\"\"\n",
    "    partition: Series\n",
    "    r\"\"\"Contains each key a number of times equal to the size of the subsampler.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_source: Dataset,\n",
    "        subsamplers: Mapping[Any, Sampler],\n",
    "        shuffle: bool = True,\n",
    "        early_stop: bool = False,\n",
    "    ):\n",
    "        super().__init__(data_source)\n",
    "        self.data = data_source\n",
    "        self.idx = data_source.keys()\n",
    "        self.subsamplers = dict(subsamplers)\n",
    "        self.sizes = Series({key: len(self.subsamplers[key]) for key in self.idx})\n",
    "        self.shuffle = shuffle\n",
    "        self.early_stop = early_stop\n",
    "\n",
    "        if early_stop:\n",
    "            partition = list(chain(*([key] * min(self.sizes) for key in self.idx)))\n",
    "        else:\n",
    "            partition = list(chain(*([key] * self.sizes[key] for key in self.idx)))\n",
    "        self.partition = Series(partition)\n",
    "\n",
    "    def __len__(self):\n",
    "        r\"\"\"Return the maximum allowed index.\"\"\"\n",
    "        if self.early_stop:\n",
    "            return min(self.sizes) * len(self.subsamplers)\n",
    "        return sum(self.sizes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        r\"\"\"Return indices of the samples.\n",
    "\n",
    "        When ``early_stop=True``, it will sample precisely min() * len(subsamplers) samples.\n",
    "        When ``early_stop=False``, it will sample all samples.\n",
    "        \"\"\"\n",
    "        activate_iterators = {\n",
    "            key: iter(sampler) for key, sampler in self.subsamplers.items()\n",
    "        }\n",
    "\n",
    "        if self.shuffle:\n",
    "            perm = np.random.permutation(self.partition)\n",
    "        else:\n",
    "            perm = self.partition\n",
    "\n",
    "        for key in perm:\n",
    "            yield key, next(activate_iterators[key])\n",
    "\n",
    "    def __getitem__(self, key: Any) -> Sampler:\n",
    "        r\"\"\"Return the subsampler for the given key.\"\"\"\n",
    "        return self.subsamplers[key]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr_mapping(self.subsamplers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plugging everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {}\n",
    "# for idx, slc in ts.groupby([\"run_id\", \"experiment_id\"]):\n",
    "#     slc = slc.reset_index([0, 1], drop=True)\n",
    "#     lower = slc.index[0]\n",
    "#     slc.index = slc.index - slc.index[0]\n",
    "#     d[idx] = slc\n",
    "# ts = pandas.concat(d, names=[\"run_id\", \"experiment_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outer_index = md.index\n",
    "TSDs = {}\n",
    "for idx in md.index:\n",
    "    TSDs[idx] = TimeSeriesDataset(\n",
    "        ts.loc[idx],\n",
    "        metadata=(md.loc[idx], final_vec.loc[idx]),\n",
    "    )\n",
    "\n",
    "DS = MappingDataset(TSDs, prepend_key=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samplers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the subsampler dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = Timedelta(\"5m\")\n",
    "\n",
    "subsamplers = {}\n",
    "\n",
    "for key in TSDs:\n",
    "    subsampler = IntervalSampler(\n",
    "        xmin=final_product_times.loc[key, \"t_min\"],\n",
    "        xmax=final_product_times.loc[key, \"t_max\"],\n",
    "        # offset=t_0,\n",
    "        deltax=lambda k: k * delta_t,\n",
    "        stride=None,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    subsamplers[key] = subsampler\n",
    "sampler = HierarchicalSampler(TSDs, subsamplers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.loc[439].loc[\n",
    "    15325\n",
    "]  # .loc[Timedelta('0 days 00:32:18'): Timedelta('0 days 01:28:47')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = next(iter(sampler))\n",
    "ts.loc[(439, 15325)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store indices to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = DataFrame(list(iter(sampler)), columns=[\"idx\", \"slice\"])\n",
    "# df[\"start\"] = df[\"slice\"].apply(lambda x: x.start) / Timedelta(\"1h\")\n",
    "# df[\"stop\"] = df[\"slice\"].apply(lambda x: x.stop) / Timedelta(\"1h\")\n",
    "# df[\"run_id\"], df[\"experiment_id\"] = zip(*df.idx)\n",
    "# df = df[[\"run_id\", \"experiment_id\", \"start\", \"stop\"]].round(2)\n",
    "# df = df.sort_values([\"run_id\", \"experiment_id\", \"start\", \"stop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple (no collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "dloader = DataLoader(DS, sampler=sampler, collate_fn=collate, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Post-Processing\n",
    "\n",
    "Batch ⟶ Post-Processed Batch\n",
    "\n",
    "- convert things to `tensor`s if not already done\n",
    "- combine `list[Tensor]` to `Tensor` / `PaddedTensor` / `PackedTensor`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific Example: Final Product Value\n",
    "\n",
    "1. Data Lookup:\n",
    "    - main data encoding: \n",
    "        - main data: Standardization \n",
    "        - index: MinMax\n",
    "        - triplet encoding\n",
    "    \n",
    "    - target data lookup:\n",
    "        - Seperate Dataset?\n",
    "        - TupleDataset? <= why not? i.e. lookup both inputs and target at once!\n",
    "            - This one only needs the first level of index, so just drop the other?\n",
    "            - Treat it as different kind of metadata? ✓ Sounds good!\n",
    "\n",
    "2. Encoding\n",
    "    - main data: just use the encoder as advertised.\n",
    "    - target data: need a separate \"target_encoder\"\n",
    "        - could be a slice of the regular encoder\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ChainedEncoder(\n",
    "    # ConcatEncoder(axis=-1),\n",
    "    TensorEncoder(names=(\"time\", \"value\", \"index\")),\n",
    "    DataFrameEncoder(\n",
    "        column_encoders={\n",
    "            \"value\": IdentityEncoder(),\n",
    "            tuple(ts.columns): FloatEncoder(\"float32\"),\n",
    "        },\n",
    "        index_encoders=MinMaxScaler() @ DateTimeEncoder(unit=\"h\"),\n",
    "    ),\n",
    "    TripletEncoder(sparse=True),\n",
    "    # DataFrameEncoder(\n",
    "    Standardizer(),\n",
    "    # index_encoders = MinMaxScaler() @ DateTimeEncoder(unit=\"h\"),\n",
    "    # ),\n",
    ")\n",
    "\n",
    "encoder = preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = ts.loc[439, 15325].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.fit(ts.reset_index([0, 1], drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoder.encode(ts.reset_index([0, 1], drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.encode(original.iloc[[-10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vec.loc[439, 15325]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(NamedTuple):\n",
    "    index: Tensor\n",
    "    timeseries: Tensor\n",
    "    metadata: Tensor\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr_mapping(\n",
    "            self._asdict(), title=self.__class__.__name__, repr_fun=repr_array\n",
    "        )\n",
    "\n",
    "\n",
    "def mycollate(batch: list):\n",
    "    index: list[Tensor] = []\n",
    "    timeseries: list[Tensor] = []\n",
    "    metadata: list[Tensor] = []\n",
    "\n",
    "    for idx, (ts, md) in batch:\n",
    "        index.append(torch.tensor(idx[0], dtype=int))\n",
    "        timeseries.append(ts)\n",
    "        metadata.append(md)\n",
    "    return Batch(torch.stack(index), timeseries, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = DataLoader(DS, sampler=sampler, collate_fn=mycollate, batch_size=8)\n",
    "batch = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encode Target Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = TensorEncoder() @ FloatEncoder() @ Standardizer(axis=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ts[target]\n",
    "target_idx = ts.columns.get_loc(target)\n",
    "target_encoder.fit(target_col)\n",
    "result = target_encoder.encode(target_col)\n",
    "mask = torch.isnan(result)\n",
    "result[~mask]\n",
    "# mask = pd.notna(result)\n",
    "# result[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse the other encoder via slicing\n",
    "target_encoder = TensorEncoder() @ FloatEncoder() @ encoder[-1][target_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = target_encoder.encode(target_col)\n",
    "mask = torch.isnan(result)\n",
    "result[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The **Final** result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS[next(iter(sampler))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = []\n",
    "metadata = []\n",
    "targets = []\n",
    "encoded_targets = []\n",
    "\n",
    "for ts_data, (md_data, target) in zip(batch.timeseries, batch.metadata):\n",
    "    timeseries.append(preprocessor.encode(ts_data))\n",
    "    metadata.append(md_data)\n",
    "    targets.append(target)\n",
    "    encoded_targets.append(target_encoder.encode(target))\n",
    "\n",
    "targets = pandas.concat(targets)\n",
    "encoded_targets = torch.concat(encoded_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_from_triplet(triplet, idx: int):\n",
    "    time, value, index = triplet\n",
    "    indices = torch.argmax(index, dim=1) == target_index\n",
    "    idx = torch.argmax(indices.to(int))\n",
    "    return value[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(NamedTuple):\n",
    "    index: Tensor\n",
    "    timeseries: Tensor\n",
    "    metadata: Tensor\n",
    "    targets: Tensor\n",
    "    encoded_targets: Tensor\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr_mapping(\n",
    "            self._asdict(), title=self.__class__.__name__, repr_fun=repr_array\n",
    "        )\n",
    "\n",
    "\n",
    "def mycollate(batch: list):\n",
    "    index: list[Tensor] = []\n",
    "    timeseries = []\n",
    "    metadata = []\n",
    "    targets = []\n",
    "    encoded_targets = []\n",
    "\n",
    "    for idx, (ts_data, (md_data, target)) in batch:\n",
    "        index.append(torch.tensor(idx[0]))\n",
    "        timeseries.append(preprocessor.encode(ts_data))\n",
    "        metadata.append(md_data)\n",
    "        targets.append(target)\n",
    "        encoded_targets.append(target_encoder.encode(target))\n",
    "\n",
    "    index = torch.stack(index)\n",
    "    targets = pandas.concat(targets)\n",
    "    encoded_targets = torch.concat(encoded_targets)\n",
    "\n",
    "    return Batch(index, timeseries, metadata, targets, encoded_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = DataLoader(DS, sampler=sampler, collate_fn=mycollate, batch_size=8)\n",
    "batch = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = next(iter(dloader.sampler))\n",
    "sample = dloader.dataset[key]\n",
    "\n",
    "(key, slc), (ts, (md, target)) = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(dloader): ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsdm.models import SetFuncTS\n",
    "\n",
    "model = SetFuncTS(17, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, v, m = batch.timeseries[0]\n",
    "model(t, v, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.batch_forward(batch.timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified DataFrameEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameEncoder(BaseEncoder):\n",
    "    r\"\"\"Combine multiple encoders into a single one.\n",
    "\n",
    "    It is assumed that the DataFrame Modality doesn't change.\n",
    "    \"\"\"\n",
    "\n",
    "    column_encoders: Union[BaseEncoder, Mapping[Any, BaseEncoder]]\n",
    "    r\"\"\"Encoders for the columns.\"\"\"\n",
    "    index_encoders: Optional[Union[BaseEncoder, Mapping[Any, BaseEncoder]]] = None\n",
    "    r\"\"\"Optional Encoder for the index.\"\"\"\n",
    "    colspec: Series = None\n",
    "    r\"\"\"The columns-specification of the DataFrame.\"\"\"\n",
    "    encode_index: bool\n",
    "    r\"\"\"Whether to encode the index.\"\"\"\n",
    "    column_wise: bool\n",
    "    r\"\"\"Whether to encode column-wise.\"\"\"\n",
    "    partitions: Optional[dict] = None\n",
    "    r\"\"\"Contains partitions if used column wise.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        column_encoders: Union[BaseEncoder, Mapping[Any, BaseEncoder]],\n",
    "        *,\n",
    "        index_encoders: Optional[Union[BaseEncoder, Mapping[Any, BaseEncoder]]] = None,\n",
    "    ):\n",
    "        r\"\"\"Set up the individual encoders.\n",
    "\n",
    "        Note: the same encoder instance can be used for multiple columns.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        column_encoders\n",
    "        index_encoders\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.column_encoders = column_encoders\n",
    "\n",
    "        if isinstance(index_encoders, Mapping):\n",
    "            raise NotImplementedError(\"Multi-Index encoders not yet supported\")\n",
    "\n",
    "        self.index_encoders = index_encoders\n",
    "        self.column_wise: bool = isinstance(self.column_encoders, Mapping)\n",
    "        self.encode_index: bool = index_encoders is not None\n",
    "\n",
    "        index_spec = DataFrame(\n",
    "            columns=[\"col\", \"encoder\"],\n",
    "            index=Index([], name=\"partition\"),\n",
    "        )\n",
    "\n",
    "        if self.encode_index:\n",
    "            if not isinstance(self.index_encoders, Mapping):\n",
    "                _idxenc_spec = Series(\n",
    "                    {\n",
    "                        \"col\": pd.NA,\n",
    "                        \"encoder\": self.index_encoders,\n",
    "                    },\n",
    "                    name=0,\n",
    "                )\n",
    "                # index_spec = index_spec.append(_idxenc_spec)\n",
    "                index_spec.loc[0] = _idxenc_spec\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    \"Multiple Index encoders are not supported yet.\"\n",
    "                )\n",
    "\n",
    "        if not isinstance(self.column_encoders, Mapping):\n",
    "            colenc_spec = DataFrame(\n",
    "                columns=[\"col\", \"encoder\"],\n",
    "                index=Index([], name=\"partition\"),\n",
    "            )\n",
    "\n",
    "            _colenc_spec = Series(\n",
    "                {\n",
    "                    \"col\": pd.NA,\n",
    "                    \"encoder\": self.column_encoders,\n",
    "                },\n",
    "                name=0,\n",
    "            )\n",
    "            # colenc_spec = colenc_spec.append(_colenc_spec)\n",
    "            # colenc_spec = pandas.concat([colenc_spec, _colenc_spec])\n",
    "            colenc_spec.loc[0] = _colenc_spec\n",
    "        else:\n",
    "            keys = self.column_encoders.keys()\n",
    "            assert len(set(keys)) == len(keys), \"Some index are duplicates!\"\n",
    "\n",
    "            _encoders = tuple(set(self.column_encoders.values()))\n",
    "            encoders = Series(_encoders, name=\"encoder\")\n",
    "            partitions = Series(range(len(_encoders)), name=\"partition\")\n",
    "\n",
    "            _columns = defaultdict(list)\n",
    "            for key, encoder in self.column_encoders.items():\n",
    "                _columns[encoder].append(key)\n",
    "\n",
    "            columns = Series(_columns, name=\"col\")\n",
    "\n",
    "            colenc_spec = DataFrame(encoders, index=partitions)\n",
    "            colenc_spec = colenc_spec.join(columns, on=\"encoder\")\n",
    "\n",
    "        self.spec = pandas.concat(\n",
    "            [index_spec, colenc_spec],\n",
    "            keys=[\"index\", \"columns\"],\n",
    "            names=[\"section\", \"partition\"],\n",
    "        ).astype({\"col\": object})\n",
    "\n",
    "        self.spec.name = self.__class__.__name__\n",
    "\n",
    "        # add extra repr options by cloning from spec.\n",
    "        # for x in [\n",
    "        #     \"_repr_data_resource_\",\n",
    "        #     \"_repr_fits_horizontal_\",\n",
    "        #     \"_repr_fits_vertical_\",\n",
    "        #     \"_repr_html_\",\n",
    "        #     \"_repr_latex_\",\n",
    "        # ]:\n",
    "        #     setattr(self, x, getattr(self.spec, x))\n",
    "\n",
    "    def fit(self, df: DataFrame, /) -> None:\n",
    "        r\"\"\"Fit to the data.\"\"\"\n",
    "        self.colspec = df.dtypes\n",
    "\n",
    "        if self.index_encoders is not None:\n",
    "            if isinstance(self.index_encoders, Mapping):\n",
    "                raise NotImplementedError(\"Multiple index encoders not yet supported\")\n",
    "            self.index_encoders.fit(df.index)\n",
    "\n",
    "        if isinstance(self.column_encoders, Mapping):\n",
    "            # check if cols are a proper partition.\n",
    "            keys = set(df.columns)\n",
    "            _keys = set(self.column_encoders.keys())\n",
    "            assert keys <= _keys, f\"Missing encoders for columns {keys - _keys}!\"\n",
    "            assert (\n",
    "                keys >= _keys\n",
    "            ), f\"Encoder given for non-existent columns {_keys - keys}!\"\n",
    "\n",
    "            for _, series in self.spec.loc[\"columns\"].iterrows():\n",
    "                encoder = series[\"encoder\"]\n",
    "                cols = series[\"col\"]\n",
    "                encoder.fit(df[cols])\n",
    "        else:\n",
    "            cols = list(df.columns)\n",
    "            self.spec.loc[\"columns\"].iloc[0][\"col\"] = cols\n",
    "            encoder = self.spec.loc[\"columns\", \"encoder\"].item()\n",
    "            encoder.fit(df)\n",
    "\n",
    "    def encode(self, df: DataFrame, /) -> DataFrame:\n",
    "        r\"\"\"Encode the input.\"\"\"\n",
    "        encoded_frames: dict[Any, DataFrame] = {}\n",
    "        for partition, (col_names, encoder) in self.spec.loc[\"columns\"].iterrows():\n",
    "            encoded_frame = encoder.encode(df[col_names])\n",
    "            encoded_frames[partition] = encoded_frame\n",
    "\n",
    "        if self.index_encoders is not None:\n",
    "            if isinstance(self.index_encoders, Mapping):\n",
    "                raise NotImplementedError(\"Multiple index encoders not yet supported\")\n",
    "            encoded_index = self.index_encoders.encode(df.index)\n",
    "        else:\n",
    "            encoded_index = df.index\n",
    "\n",
    "        encoded = pandas.concat(\n",
    "            encoded_frames, axis=\"columns\", names=[\"partition\", df.columns.name]\n",
    "        )\n",
    "        encoded = encoded.droplevel(\n",
    "            \"partition\", axis=\"columns\"\n",
    "        )  # remove partition index\n",
    "        encoded = encoded.set_index(encoded_index)\n",
    "        encoded = encoded[df.columns]  # fix column order\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, data: DataFrame, /) -> DataFrame:\n",
    "        r\"\"\"Decode the input.\"\"\"\n",
    "        if self.encode_index:\n",
    "            if isinstance(self.index_encoders, Mapping):\n",
    "                raise NotImplementedError(\"Multiple index encoders not yet supported\")\n",
    "            encoder = self.spec.loc[\"index\", \"encoder\"].item()\n",
    "            decoded_index = encoder.decode(data.index)\n",
    "        else:\n",
    "            decoded_index = None\n",
    "\n",
    "        decoded_frames: dict[Any, DataFrame] = {}\n",
    "        for partition, (col_names, encoder) in self.spec.loc[\"columns\"].iterrows():\n",
    "            # col_names += col_names\n",
    "            decoded_frame = encoder.decode(data[col_names])\n",
    "            decoded_frames[partition] = decoded_frame\n",
    "\n",
    "        # return decoded_frames\n",
    "        decoded = pandas.concat(\n",
    "            decoded_frames, axis=\"columns\", names=[\"partition\", data.columns.name]\n",
    "        )\n",
    "        decoded = decoded.droplevel(\n",
    "            \"partition\", axis=\"columns\"\n",
    "        )  # remove partition index\n",
    "        decoded = decoded.set_index(decoded_index)\n",
    "        decoded = decoded[self.colspec.index]  # fix column order\n",
    "        decoded = decoded.astype(self.colspec)  # fix data types\n",
    "        return decoded\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Pretty print.\"\"\"\n",
    "        return f\"{self.__class__.__name__}(\" + self.spec.__repr__() + \"\\n)\"\n",
    "\n",
    "    def _repr_html_(self) -> str:\n",
    "        \"\"\"HTML representation.\"\"\"\n",
    "        html_repr = self.spec._repr_html_()  # pylint: disable=protected-access\n",
    "        return f\"<h3>{self.__class__.__name__}</h3> {html_repr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dloader.sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dloader.sampler[439, 15325].intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
