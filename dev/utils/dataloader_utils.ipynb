{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, jit\n",
    "\n",
    "np.set_printoptions(precision=4, floatmode=\"fixed\", suppress=True)\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?torch.jit.annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit.script\n",
    "def aggregate_and(\n",
    "    x: Tensor,\n",
    "    dim: list[int],\n",
    "    keepdim: bool = False,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Compute logical ``AND`` across dim.\"\"\"\n",
    "    dims = torch.jit.annotate(List[int], [])\n",
    "\n",
    "    if dim is None:\n",
    "        dims = list(range(x.ndim))\n",
    "    elif isinstance(dim, int):\n",
    "        dims = [dim]\n",
    "    else:\n",
    "        dims = dim\n",
    "\n",
    "    if isinstance(dims, tuple):\n",
    "        if len(dims) == 0:\n",
    "            return x\n",
    "        if keepdim:\n",
    "            for d in dims:\n",
    "                x = torch.all(x, dim=d, keepdim=keepdim)\n",
    "        else:\n",
    "            for i, d in enumerate(dims):\n",
    "                x = torch.all(x, dim=d - i, keepdim=keepdim)\n",
    "        return x\n",
    "\n",
    "    if keepdim:\n",
    "        for d in dims:\n",
    "            x = torch.all(x, dim=d, keepdim=keepdim)\n",
    "    else:\n",
    "        for i, d in enumerate(dims):\n",
    "            x = torch.all(x, dim=d - i, keepdim=keepdim)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(3, 4, 5) > 0.1\n",
    "aggregate_and(m, dim=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import BoolTensor, Tensor, jit\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [torch.randn(n + 1) for n in range(3)]\n",
    "lengths = torch.tensor([len(t) for t in tensors])\n",
    "tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = pad_sequence(tensors, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsdm.utils.data import aggregate_and\n",
    "\n",
    "m = torch.randn(3, 4, 5) > 0.1\n",
    "aggregate_and(m, dim=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit.script\n",
    "def torch_being_dumb(im: Union[None, int, list[int], tuple] = None) -> int:\n",
    "    return len(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_being_dumb(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpad_sequence(batch, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[:l] for x, l in zip(batch_pad_packed, lengths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pad_packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?unpack_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [torch.randn(abs(n - 3), 3) for n in range(6)]\n",
    "\n",
    "for i, t in enumerate(tensors):\n",
    "    if len(t) > 0:\n",
    "        tensors[i][0] = float(\"nan\")\n",
    "tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_seq = pad_sequence(tensors, batch_first=True, padding_value=float(\"nan\"))\n",
    "padded_seq.swapaxes(-1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def unpad_sequence(\n",
    "    padded_seq: Tensor,\n",
    "    batch_first: bool = False,\n",
    "    lengths: Optional[Tensor] = None,\n",
    "    padding_value: float = 0.0,\n",
    ") -> list[Tensor]:\n",
    "    r\"\"\"Reverse operation of `torch.nn.utils.rnn.pad_sequence`.\"\"\"\n",
    "    padded_seq: Tensor = padded_seq.swapaxes(0, 1) if not batch_first else padded_seq\n",
    "    padding: Tensor = torch.tensor(\n",
    "        padding_value, dtype=padded_seq.dtype, device=padded_seq.device\n",
    "    )\n",
    "\n",
    "    if lengths is not None:\n",
    "        return [x[0:l] for x, l in zip(padded_seq, lengths)]\n",
    "\n",
    "    # infer lengths from mask\n",
    "    if torch.isnan(padding):\n",
    "        mask = torch.isnan(padded_seq)\n",
    "    else:\n",
    "        mask = padded_seq == padding_value\n",
    "\n",
    "    # all features are masked\n",
    "    dims = list(range(2, padded_seq.ndim))\n",
    "    agg = aggregate_and(mask, dim=dims)\n",
    "    # count, starting from the back, until the first observation occurs.\n",
    "    inferred_lengths = (~cumulative_and(agg.flip(dims=(1,)), dim=1)).sum(dim=1)\n",
    "\n",
    "    return [x[0:l] for x, l in zip(padded_seq, inferred_lengths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence, pad_sequence\n",
    "\n",
    "from tsdm.utils.data import unpack_sequence, unpad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [torch.randn(1 + abs(n - 3), 3) for n in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = pack_sequence(tensors, enforce_sorted=False)\n",
    "unpacked = unpack_sequence(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_sequence(batch: PackedSequence) -> list[Tensor]:\n",
    "    r\"\"\"Reverse operation of pack_sequence.\"\"\"\n",
    "    batch_pad_packed, lengths = pad_packed_sequence(batch, batch_first=True)\n",
    "    torch.swapaxes(batch_pad_packed, 1, 2)\n",
    "    return [x[:l] for x, l in zip(batch_pad_packed, lengths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpad_sequence(padded_seq, batch_first=True, padding_value=float(\"nan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(4, 5, 6) > 0.1\n",
    "b.all(dim=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "jnp.add.aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dir(jnp.add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import BoolTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit.script\n",
    "def get_longest(x: Tensor, value: Tensor, reverse: bool = False) -> Tensor:\n",
    "    \"\"\"take as long as equal to value\"\"\"\n",
    "\n",
    "    y = torch.flip(x, dims=(0,)) if reverse else x\n",
    "\n",
    "    if torch.isnan(value):\n",
    "        i = 0\n",
    "        for el in y:\n",
    "            if not torch.isnan(el).all():\n",
    "                break\n",
    "            i += 1\n",
    "        return x[:i]\n",
    "\n",
    "    i = 0\n",
    "    while (y[i] == value).all():\n",
    "        i += 1\n",
    "    return x[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit.script\n",
    "def aggregate_and(\n",
    "    x: BoolTensor,\n",
    "    dim: Union[None, int, list[int]] = None,\n",
    "    keepdim: bool = False,\n",
    ") -> BoolTensor:\n",
    "    r\"\"\"Compute logical ``AND`` across dim.\"\"\"\n",
    "\n",
    "    if dim is None:\n",
    "        dims = list(range(x.ndim))\n",
    "    elif isinstance(dim, int):\n",
    "        dims = [dim]\n",
    "    else:\n",
    "        dims = dim\n",
    "\n",
    "    if keepdim:\n",
    "        for d in dims:\n",
    "            x = torch.all(x, dim=d, keepdim=keepdim)\n",
    "    else:\n",
    "        for i, d in enumerate(dims):\n",
    "            x = torch.all(x, dim=d - i, keepdim=keepdim)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.isnan(padded_seq)\n",
    "aggregate_and(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
