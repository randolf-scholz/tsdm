{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some sample text, and a first block of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import importlib\n",
    "import pkgutil\n",
    "import re\n",
    "import sys\n",
    "from contextlib import redirect_stderr, redirect_stdout\n",
    "from pathlib import Path\n",
    "from types import ModuleType\n",
    "from typing import Any, NamedTuple\n",
    "\n",
    "if sys.version_info >= (3, 10):\n",
    "    import importlib.metadata as metadata\n",
    "else:\n",
    "    try:\n",
    "        metadata = importlib.import_module(\"importlib_metadata\")\n",
    "    except ImportError as E:\n",
    "        raise ImportError(\n",
    "            \"This pre-commit hook runs in the local interpreter and requires\"\n",
    "            \" the `importlib_metadata` package for python versions < 3.10.\"\n",
    "        ) from E\n",
    "\n",
    "if sys.version_info >= (3, 11):\n",
    "    import tomllib\n",
    "else:\n",
    "    try:\n",
    "        tomllib = importlib.import_module(\"tomlkit\")\n",
    "    except ImportError as E:\n",
    "        raise ImportError(\n",
    "            \"This pre-commit hook runs in the local interpreter and requires\"\n",
    "            \" the `tomlkit` package for python versions < 3.11.\"\n",
    "        ) from E\n",
    "\n",
    "PACKAGES: dict[\n",
    "    str, list[str]\n",
    "] = metadata.packages_distributions()  # type:ignore[assignment]\n",
    "\"\"\"A dictionary that maps module names to their pip-package names.\"\"\"\n",
    "\n",
    "# NOTE: illogical type hint in stdlib, maybe open issue.\n",
    "# https://github.com/python/cpython/blob/608927b01447b110de5094271fbc4d49c60130b0/Lib/importlib/metadata/__init__.py#L933-L947C29\n",
    "# https://github.com/python/typeshed/blob/d82a8325faf35aa0c9d03d9e9d4a39b7fcb78f8e/stdlib/importlib/metadata/__init__.pyi#L32\n",
    "\n",
    "\n",
    "def normalize_dep_name(dep: str, /) -> str:\n",
    "    \"\"\"Normalize a dependency name.\"\"\"\n",
    "    return dep.lower().replace(\"-\", \"_\")\n",
    "\n",
    "\n",
    "def get_deps_import(node: ast.Import, /) -> set[str]:\n",
    "    \"\"\"Extract dependencies from an `import ...` statement.\"\"\"\n",
    "    dependencies = set()\n",
    "    for alias in node.names:\n",
    "        module = alias.name.split(\".\")[0]\n",
    "        if not module.startswith(\"_\"):\n",
    "            dependencies.add(module)\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def get_deps_importfrom(node: ast.ImportFrom, /) -> set[str]:\n",
    "    \"\"\"Extract dependencies from an `from y import ...` statement.\"\"\"\n",
    "    assert node.module is not None\n",
    "    module_name = node.module.split(\".\")[0]\n",
    "    if module_name.startswith(\"_\"):  # ignore _private modules\n",
    "        return set()\n",
    "    return {module_name}\n",
    "\n",
    "\n",
    "def get_deps_tree(tree: ast.AST, /) -> set[str]:\n",
    "    \"\"\"Extract the set of dependencies from `ast.AST` object.\"\"\"\n",
    "    dependencies: set[str] = set()\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Import):\n",
    "            dependencies |= get_deps_import(node)\n",
    "        elif isinstance(node, ast.ImportFrom):\n",
    "            dependencies |= get_deps_importfrom(node)\n",
    "\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def get_deps_file(file_path: str | Path, /) -> set[str]:\n",
    "    \"\"\"Extract set of dependencies imported by a script.\"\"\"\n",
    "    path = Path(file_path)\n",
    "\n",
    "    if path.suffix != \".py\":\n",
    "        raise ValueError(f\"Invalid file extension: {path.suffix}\")\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf8\") as file:\n",
    "        tree = ast.parse(file.read())\n",
    "\n",
    "    return get_deps_tree(tree)\n",
    "\n",
    "\n",
    "def get_deps_module(module: str | ModuleType, /, *, silent: bool = True) -> set[str]:\n",
    "    \"\"\"Extract set of dependencies imported by a module.\"\"\"\n",
    "    # NOTE: Generally there is no correct way to do it without importing the module.\n",
    "    # This is because modules can be imported dynamically.\n",
    "\n",
    "    if isinstance(module, str):\n",
    "        with (  # load the submodule silently\n",
    "            redirect_stdout(None if silent else sys.stdout),\n",
    "            redirect_stderr(None if silent else sys.stderr),\n",
    "        ):\n",
    "            module = importlib.import_module(module)\n",
    "\n",
    "    # Visit the current module\n",
    "    assert module.__file__ is not None\n",
    "    dependencies = get_deps_file(module.__file__)\n",
    "\n",
    "    if not hasattr(module, \"__path__\"):\n",
    "        # note: can only recurse into packages.\n",
    "        return dependencies\n",
    "\n",
    "    # Visit the sub-packages/modules of the package\n",
    "    # TODO: add dynamically imported submodules using the `pkgutil` module.\n",
    "\n",
    "    for module_info in pkgutil.walk_packages(module.__path__):\n",
    "        submodule_name = f\"{module.__name__}.{module_info.name}\"\n",
    "\n",
    "        if submodule_name == \"tsdm.datasets.examples.experiment\":\n",
    "            print(\n",
    "                module,\n",
    "                module.__name__,\n",
    "                module_info.name,\n",
    "                module_info,\n",
    "                module.__path__,\n",
    "                list(pkgutil.walk_packages(module.__path__)),\n",
    "                sep=\"\\n\\n\",\n",
    "            )\n",
    "\n",
    "        dependencies |= get_deps_module(submodule_name, silent=silent)\n",
    "\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def get_deps_pyproject_section(config: dict[str, Any], /, *, section: str) -> set[str]:\n",
    "    \"\"\"Get the dependencies from a section of pyproject.toml.\n",
    "\n",
    "    Looking up the section must either result in a list of strings or a dict.\n",
    "    \"\"\"\n",
    "    try:  # recursively get the section\n",
    "        for key in section.split(\".\"):\n",
    "            config = config[key]\n",
    "    except KeyError:\n",
    "        return NotImplemented\n",
    "\n",
    "    match config:\n",
    "        case list() as lst:  # type: ignore[unreachable]\n",
    "            # assume format `\"package<comparator>version\"`\n",
    "            regex = re.compile(r\"[a-zA-Z0-9_-]*\")  # type: ignore[unreachable]\n",
    "            return {re.search(regex, dep).group() for dep in lst}\n",
    "        case dict() as dct:  # poetry\n",
    "            # assume format `package = \"<comparator>version\"`\n",
    "            return set(dct.keys()) - {\"python\"}\n",
    "        case _:\n",
    "            raise TypeError(f\"Unexpected type: {type(config)}\")\n",
    "\n",
    "\n",
    "def get_deps_pyproject(fname: str | Path = \"pyproject.toml\", /) -> set[str]:\n",
    "    \"\"\"Extract the dependencies from a pyproject.toml file.\n",
    "\n",
    "    There are 6 sections we check:\n",
    "    - pyproject.dependencies\n",
    "    - pyproject.optional-dependencies.test(s)\n",
    "    - tool.poetry.dependencies\n",
    "    - tool.poetry.group.test(s).dependencies\n",
    "\n",
    "    If dependencies are specified in multiple sections, it is validated that they are\n",
    "    the same.\n",
    "    \"\"\"\n",
    "    with open(fname, \"rb\") as file:\n",
    "        pyproject = tomllib.load(file)\n",
    "\n",
    "    dependencies = {\n",
    "        key: get_deps_pyproject_section(pyproject, section=key)\n",
    "        for key in (\n",
    "            \"project.dependencies\",\n",
    "            \"tool.poetry.dependencies\",\n",
    "        )\n",
    "    }\n",
    "\n",
    "    match (\n",
    "        dependencies[\"project.dependencies\"],\n",
    "        dependencies[\"tool.poetry.dependencies\"],\n",
    "    ):\n",
    "        case set() as a, set() as b:\n",
    "            if (left := a - b) | (right := b - a):\n",
    "                raise ValueError(\n",
    "                    \"Found different dependencies in [project] and [tool.poetry].\"\n",
    "                    f\"\\n [project]     is missing: {left}, \"\n",
    "                    f\"\\n [tool.poetry] is missing: {right}.\"\n",
    "                )\n",
    "            project_dependencies = a\n",
    "        case set() as a, _:\n",
    "            project_dependencies = a\n",
    "        case _, set() as b:\n",
    "            project_dependencies = b\n",
    "        case _:\n",
    "            project_dependencies = set()\n",
    "\n",
    "    return project_dependencies\n",
    "\n",
    "\n",
    "def get_deps_pyproject_test(fname: str | Path = \"pyproject.toml\", /) -> set[str]:\n",
    "    \"\"\"Extract the test dependencies from a pyproject.toml file.\"\"\"\n",
    "    with open(fname, \"rb\") as file:\n",
    "        pyproject = tomllib.load(file)\n",
    "\n",
    "    dependencies = {\n",
    "        key: get_deps_pyproject_section(pyproject, section=key)\n",
    "        for key in (\n",
    "            \"project.optional-dependencies.test\",\n",
    "            \"project.optional-dependencies.tests\",\n",
    "            \"tool.poetry.group.test.dependencies\",\n",
    "            \"tool.poetry.group.tests.dependencies\",\n",
    "        )\n",
    "    }\n",
    "\n",
    "    match (\n",
    "        dependencies[\"project.optional-dependencies.test\"],\n",
    "        dependencies[\"project.optional-dependencies.tests\"],\n",
    "    ):\n",
    "        case set(), set():\n",
    "            raise ValueError(\n",
    "                \"Found both [project.optional-dependencies.test]\"\n",
    "                \" and [project.optional-dependencies.tests].\"\n",
    "            )\n",
    "        case set() as a, _:\n",
    "            project_test_dependencies = a\n",
    "        case _, set() as b:\n",
    "            project_test_dependencies = b\n",
    "        case _:\n",
    "            project_test_dependencies = NotImplemented\n",
    "\n",
    "    match (\n",
    "        dependencies[\"tool.poetry.group.test.dependencies\"],\n",
    "        dependencies[\"tool.poetry.group.tests.dependencies\"],\n",
    "    ):\n",
    "        case set(), set():\n",
    "            raise ValueError(\n",
    "                \"Found both [tool.poetry.group.test.dependencies]\"\n",
    "                \" and [tool.poetry.group.tests.dependencies].\"\n",
    "            )\n",
    "        case set() as a, _:\n",
    "            poetry_test_dependencies = a\n",
    "        case _, set() as b:\n",
    "            poetry_test_dependencies = b\n",
    "        case _:\n",
    "            poetry_test_dependencies = NotImplemented\n",
    "\n",
    "    match (\n",
    "        project_test_dependencies,\n",
    "        poetry_test_dependencies,\n",
    "    ):\n",
    "        case set() as a, set() as b:\n",
    "            if (left := a - b) | (right := b - a):\n",
    "                raise ValueError(\n",
    "                    \"Found different test dependencies in [project] and [tool.poetry].\"\n",
    "                    f\"\\n [project]     is missing: {left}, \"\n",
    "                    f\"\\n [tool.poetry] is missing: {right}.\"\n",
    "                )\n",
    "            test_dependencies = a\n",
    "        case set() as a, _:\n",
    "            test_dependencies = a\n",
    "        case _, set() as b:\n",
    "            test_dependencies = b\n",
    "        case _:\n",
    "            test_dependencies = set()\n",
    "\n",
    "    return test_dependencies\n",
    "\n",
    "\n",
    "class GroupedDependencies(NamedTuple):\n",
    "    \"\"\"A named tuple containing the dependencies grouped by type.\"\"\"\n",
    "\n",
    "    imported_dependencies: set[str]\n",
    "    stdlib_dependencies: set[str]\n",
    "\n",
    "\n",
    "def group_dependencies(dependencies: set[str], /) -> GroupedDependencies:\n",
    "    \"\"\"Splits the dependencies into first-party and third-party.\"\"\"\n",
    "    imported_dependencies = set()\n",
    "    stdlib_dependencies = set()\n",
    "\n",
    "    for dependency in dependencies:\n",
    "        if dependency in sys.stdlib_module_names:\n",
    "            stdlib_dependencies.add(dependency)\n",
    "        else:\n",
    "            imported_dependencies.add(dependency)\n",
    "\n",
    "    return GroupedDependencies(\n",
    "        imported_dependencies=imported_dependencies,\n",
    "        stdlib_dependencies=stdlib_dependencies,\n",
    "    )\n",
    "\n",
    "\n",
    "def collect_dependencies(fname: str | Path, /, raise_notfound: bool = True) -> set[str]:\n",
    "    \"\"\"Collect the third-party dependencies from files in the given path.\"\"\"\n",
    "    path = Path(fname)\n",
    "    dependencies = set()\n",
    "\n",
    "    if path.is_file():  # Single file\n",
    "        dependencies |= get_deps_file(path)\n",
    "    elif path.is_dir():  # Directory\n",
    "        for file_path in path.rglob(\"*.py\"):\n",
    "            if file_path.is_file():\n",
    "                dependencies |= get_deps_file(file_path)\n",
    "    elif not path.exists():  # assume module\n",
    "        try:\n",
    "            dependencies |= get_deps_module(str(fname))\n",
    "        except ModuleNotFoundError as exc:\n",
    "            if raise_notfound:\n",
    "                raise exc\n",
    "    elif raise_notfound:\n",
    "        raise FileNotFoundError(f\"Invalid path: {path}\")\n",
    "\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def validate_dependencies(\n",
    "    *,\n",
    "    pyproject_dependencies: set[str],\n",
    "    imported_dependencies: set[str],\n",
    "    raise_unused_dependencies: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Validate the dependencies.\"\"\"\n",
    "    # extract 3rd party dependencies.\n",
    "    imported_dependencies = group_dependencies(\n",
    "        imported_dependencies\n",
    "    ).imported_dependencies\n",
    "\n",
    "    # map the dependencies to their pip-package names\n",
    "    imported_deps: set[str] = set()\n",
    "    unknown_deps: set[str] = set()\n",
    "    for dep in used_deps:\n",
    "        if dep not in PACKAGES:\n",
    "            unknown_deps.add(dep)\n",
    "            continue\n",
    "\n",
    "        # get the pypi-package name\n",
    "        values: list[str] = PACKAGES[dep]\n",
    "        if len(values) > 1:\n",
    "            raise ValueError(f\"Found multiple pip-packages for {dep!r}: {values}.\")\n",
    "        imported_deps.add(values[0])\n",
    "\n",
    "    # normalize the dependencies\n",
    "    pyproject_deps = {normalize_dep_name(dep) for dep in pyproject_dependencies}\n",
    "    imported_deps = {normalize_dep_name(dep) for dep in imported_deps}\n",
    "\n",
    "    # check if all imported dependencies are listed in pyproject.toml\n",
    "    missing_deps = imported_deps - pyproject_deps\n",
    "    unused_deps = pyproject_deps - imported_deps\n",
    "\n",
    "    if missing_deps or unknown_deps or (unused_deps and raise_unused_dependencies):\n",
    "        raise ValueError(\n",
    "            f\"Found discrepancy between imported dependencies and pyproject.toml!\"\n",
    "            f\"\\nImported dependencies not listed in pyproject.toml: {missing_deps}.\"\n",
    "            f\"\\nUnused dependencies listed in pyproject.toml: {unused_deps}.\"\n",
    "            f\"\\nUnknown dependencies: {unknown_deps}.\"\n",
    "            f\"\\n\"\n",
    "            f\"\\nOptional dependencies are currently not supported (PR welcome).\"\n",
    "            f\"\\nWorkaround: use `importlib.import_module('optional_dependency')`.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Print the third-party dependencies of a module.\"\"\"\n",
    "    # usage\n",
    "    modules_default = [\"src/\"]\n",
    "    tests_default = [\"tests/\"]\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Print the third-party dependencies of a module.\",\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"pyproject_file\",\n",
    "        nargs=\"?\",\n",
    "        default=\"pyproject.toml\",\n",
    "        type=str,\n",
    "        help=\"The path to the pyproject.toml file.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--modules\",\n",
    "        nargs=\"*\",\n",
    "        default=modules_default,\n",
    "        type=str,\n",
    "        help=\"The folder of the module to check.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--error_unused_project_deps\",\n",
    "        action=argparse.BooleanOptionalAction,\n",
    "        default=True,\n",
    "        help=\"Raise error if pyproject.toml lists unused project dependencies\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tests\",\n",
    "        nargs=\"*\",\n",
    "        default=tests_default,\n",
    "        type=str,\n",
    "        help=\"The path to the test directories.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--error_unused_test_deps\",\n",
    "        action=argparse.BooleanOptionalAction,\n",
    "        default=False,\n",
    "        help=\"Raise error if pyproject.toml lists unused test dependencies\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--verbose\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Disables silencing of import messages.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--debug\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Print debug information.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # compute the dependencies from the source files\n",
    "    modules_given = args.modules is not modules_default\n",
    "    imported_dependencies = set().union(\n",
    "        *(\n",
    "            collect_dependencies(fname, raise_notfound=modules_given)\n",
    "            for fname in args.modules\n",
    "        )\n",
    "    )\n",
    "    # get dependencies from pyproject.toml\n",
    "    pyproject_dependencies = get_deps_pyproject(args.pyproject_file)\n",
    "    # validate the dependencies\n",
    "    validate_dependencies(\n",
    "        pyproject_dependencies=pyproject_dependencies,\n",
    "        imported_dependencies=imported_dependencies,\n",
    "    )\n",
    "\n",
    "    # compute the test dependencies from the test files\n",
    "    tests_given = args.tests is not tests_default\n",
    "    imported_test_dependencies = set().union(\n",
    "        *(\n",
    "            collect_dependencies(fname, raise_notfound=tests_given)\n",
    "            for fname in args.tests\n",
    "        )\n",
    "    )\n",
    "    # get dependencies from pyproject.toml\n",
    "    pyproject_test_dependencies = get_deps_pyproject(args.pyproject_file)\n",
    "    # validate the dependencies\n",
    "    validate_dependencies(\n",
    "        pyproject_dependencies=pyproject_test_dependencies,\n",
    "        imported_dependencies=imported_test_dependencies,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fname = Path.cwd().parent / \"src\"\n",
    "imported_dependencies = group_dependencies(\n",
    "    collect_dependencies(fname)\n",
    ").imported_dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp = Path.cwd().parent / \"pyproject.toml\"\n",
    "pyproject_dependencies = get_deps_pyproject(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imported_deps: set[str] = set()\n",
    "unknown_deps: set[str] = set()\n",
    "for dep in imported_dependencies:\n",
    "    if dep not in PACKAGES:\n",
    "        unknown_deps.add(dep)\n",
    "        continue\n",
    "\n",
    "    # get the pypi-package name\n",
    "    values: list[str] = PACKAGES[dep]\n",
    "    if len(values) > 1:\n",
    "        raise ValueError(f\"Found multiple pip-packages for {dep!r}: {values}.\")\n",
    "    imported_deps.add(values[0])\n",
    "\n",
    "imported_deps, unknown_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize the dependencies\n",
    "pyproject_deps = {normalize_dep_name(dep) for dep in pyproject_dependencies}\n",
    "imported_deps = {normalize_dep_name(dep) for dep in imported_deps}\n",
    "\n",
    "imported_deps, pyproject_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if all imported dependencies are listed in pyproject.toml\n",
    "missing_deps = imported_deps - pyproject_deps\n",
    "unused_deps = pyproject_deps - imported_deps\n",
    "\n",
    "missing_deps, unused_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imported_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pkgutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mod = importlib.import_module(\"tsdm.datasets.examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(pkgutil.walk_packages(mod.__path__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fname = Path.cwd().parent / \"src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_deps_pyproject(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "group_dependencies(collect_dependencies(fname)).imported_dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "from collections.abc import Iterator\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../dev/tmp.py\", \"r\") as file:\n",
    "    tree = ast.parse(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_deps_import(node: ast.Import, /) -> set[str]:\n",
    "    \"\"\"Extract dependencies from an `import ...` statement.\"\"\"\n",
    "    dependencies = set()\n",
    "    for alias in node.names:\n",
    "        module = alias.name.split(\".\")[0]\n",
    "        if not module.startswith(\"_\"):\n",
    "            dependencies.add(module)\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def get_deps_importfrom(node: ast.ImportFrom, /) -> set[str]:\n",
    "    \"\"\"Extract dependencies from an `from y import ...` statement.\"\"\"\n",
    "    dependencies = set()\n",
    "    assert node.module is not None\n",
    "    module = node.module.split(\".\")[0]\n",
    "    if not module.startswith(\"_\"):  # ignore _private modules\n",
    "        dependencies.add(module)\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def get_deps_tree(tree: ast.AST, /) -> set[str]:\n",
    "    \"\"\"Extract the set of dependencies from `ast.AST` object.\"\"\"\n",
    "    dependencies: set[str] = set()\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Import):\n",
    "            dependencies |= get_deps_import(node)\n",
    "        elif isinstance(node, ast.ImportFrom):\n",
    "            dependencies |= get_deps_importfrom(node)\n",
    "\n",
    "    return dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for node in ast.walk(tree):\n",
    "    if isinstance(node, ast.Import):\n",
    "        break\n",
    "\n",
    "display(vars(node))\n",
    "display(vars(node.names[0]))\n",
    "\n",
    "{alias.name.split(\".\")[0] for alias in node.names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for node in ast.walk(tree):\n",
    "    if isinstance(node, ast.ImportFrom):\n",
    "        break\n",
    "\n",
    "display(vars(node))\n",
    "display(vars(node.names[0]))\n",
    "display(vars(node.names[1]))\n",
    "node.module.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_attributes(tree: ast.AST) -> Iterator[ast.Attribute]:\n",
    "    \"\"\"Get all attribute nodes.\"\"\"\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Attribute):\n",
    "            yield node\n",
    "\n",
    "\n",
    "def get_type_hints(tree: ast.AST) -> Iterator[ast.AST]:\n",
    "    \"\"\"Get all nodes that are type hints.\"\"\"\n",
    "    for node in ast.walk(tree):\n",
    "        ann = getattr(node, \"annotation\", None)\n",
    "        if ann is not None:\n",
    "            yield ann\n",
    "\n",
    "\n",
    "def get_imported_symbols(tree: ast.AST) -> dict[str, str]:\n",
    "    \"\"\"Get all imported symbols.\"\"\"\n",
    "    imported_symbols = {}\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Import):\n",
    "            for alias in node.names:\n",
    "                imported_symbols[alias.asname or alias.name] = alias.name\n",
    "        elif isinstance(node, ast.ImportFrom):\n",
    "            module_name = node.module\n",
    "            if module_name is not None:\n",
    "                for alias in node.names:\n",
    "                    full_name = f\"{module_name}.{alias.name}\"\n",
    "                    imported_symbols[alias.asname or alias.name] = full_name\n",
    "\n",
    "    return imported_symbols\n",
    "\n",
    "\n",
    "def get_attrs_shadow_imported(tree: ast.AST) -> Iterator[ast.Attribute]:\n",
    "    \"\"\"Get attribute nodes that shadow directly imported symbols.\"\"\"\n",
    "    imported_symbols = get_imported_symbols(tree)\n",
    "\n",
    "    for node in get_type_hints(tree):\n",
    "        for attr in get_attributes(node):\n",
    "            if attr.attr in imported_symbols:\n",
    "                yield attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes = list(get_attrs_shadow_imported(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vars(nodes[0].value.value.value.ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vars(nodes[0].value.value.attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Concatenate\n",
    "\n",
    "\n",
    "def with_request(f: Callable[Concatenate[Request, P], R]) -> Callable[P, R]:\n",
    "    def inner(*args: P.args, **kwargs: P.kwargs) -> R:\n",
    "        return f(Request(), *args, **kwargs)\n",
    "\n",
    "    return inner\n",
    "\n",
    "\n",
    "@with_request\n",
    "def takes_int_str(request: Request, x: int, y: str) -> int:\n",
    "    # use request\n",
    "    return x + 7\n",
    "\n",
    "\n",
    "takes_int_str(1, \"A\")  # Accepted\n",
    "takes_int_str(\"B\", 2)  # Correctly rejected by the type checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for node in ast.walk(tree):\n",
    "    if isinstance(node, ast.ImportFrom):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in ast.walk(tree):\n",
    "    if isinstance(node, ast.AnnAssign) and isinstance(node.annotation, ast.Name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_hints(node: ast.FunctionDef):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_type_hints(tree):\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.AnnAssign):\n",
    "            return node\n",
    "            for arg in node.args.args:\n",
    "                if arg.annotation:\n",
    "                    lineno, col_offset = (\n",
    "                        arg.annotation.lineno,\n",
    "                        arg.annotation.col_offset,\n",
    "                    )\n",
    "                    type_hint = ast.dump(arg.annotation)\n",
    "                    print(\n",
    "                        f\"Function: {node.name}, Argument: {arg.arg}, Type Hint: {type_hint}, Line: {lineno}, Column: {col_offset}\"\n",
    "                    )\n",
    "\n",
    "\n",
    "node = extract_type_hints(tree)\n",
    "ARG = node.args.posonlyargs[0]\n",
    "ANN = ARG.annotation\n",
    "display(type(ANN), ANN)\n",
    "display(vars(ANN))\n",
    "display(vars(ANN.value))\n",
    "display(ANN.attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_type_hints(tree):\n",
    "    for node in ast.walk(tree):\n",
    "        ann = getattr(node, \"annotation\", None)\n",
    "        if ann is not None:\n",
    "            yield ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_type_hint(arg: ast.arg) -> ast.AST:\n",
    "    if arg.annotation is not None:\n",
    "        yield arg.annotation\n",
    "\n",
    "\n",
    "def get_type_hints(args: ast.arguments) -> Iterator[ast.AST]:\n",
    "    for arg in args.posonlyargs:\n",
    "        yield from get_type_hint(arg)\n",
    "    for arg in args.args:\n",
    "        yield from get_type_hint(arg)\n",
    "    if args.vararg is not None:\n",
    "        yield from get_type_hint(args.vararg)\n",
    "    for arg in args.kwonlyargs:\n",
    "        yield from get_type_hint(arg)\n",
    "    if args.kwarg is not None:\n",
    "        yield from get_type_hint(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(get_type_hints(node.args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_type_hints(tree):\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            return node\n",
    "            for arg in node.args.args:\n",
    "                if arg.annotation:\n",
    "                    lineno, col_offset = (\n",
    "                        arg.annotation.lineno,\n",
    "                        arg.annotation.col_offset,\n",
    "                    )\n",
    "                    type_hint = ast.dump(arg.annotation)\n",
    "                    print(\n",
    "                        f\"Function: {node.name}, Argument: {arg.arg}, Type Hint: {type_hint}, Line: {lineno}, Column: {col_offset}\"\n",
    "                    )\n",
    "\n",
    "\n",
    "node = extract_type_hints(tree)\n",
    "ARG = node.args.posonlyargs[0]\n",
    "display(vars(ARG))\n",
    "ANN = ARG.annotation\n",
    "display(type(ANN), ANN)\n",
    "display(vars(ANN))\n",
    "display(vars(ANN.value))\n",
    "display(ANN.attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ARG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(node.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(node.args.posonlyargs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ast.dump(node.args.posonlyargs[0].annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"posonlyargs\",\n",
    "\"args\",\n",
    "\"vararg\"\n",
    "\"kwonlyargs\",\n",
    "\"kwarg\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_hints(node: ast.FunctionDef):\n",
    "    args = node.args\n",
    "    if args.posonlyargs:\n",
    "        ...\n",
    "    if args.args:\n",
    "        ...\n",
    "    if args.vararg is not None:\n",
    "        ...\n",
    "    if args.kwonlyargs:\n",
    "        ...\n",
    "    if args.kwarg is not None:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vars(node.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir(node.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast.arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vars(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Protocol, runtime_checkable\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class Top(Protocol):\n",
    "    \"\"\"Represents the top type.\"\"\"\n",
    "\n",
    "\n",
    "assert issubclass(object, Top)\n",
    "assert issubclass(Top, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "issubclass(Any, Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isinstance(..., object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tsdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tsdm.datasets.PhysioNet2019()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "pa_dict = pa.dictionary(pa.int32(), pa.string())\n",
    "pd_dict = pd.ArrowDtype(pa_dict)\n",
    "\n",
    "# create table in pyarrow\n",
    "data = {\"foo\": [1, 2], \"bar\": [\"a\", \"b\"]}\n",
    "schema = {\"foo\": int, \"bar\": \"category\"}\n",
    "\n",
    "df = pd.DataFrame(data).astype(schema)\n",
    "df.to_parquet(\"foo\")\n",
    "df_loaded = pd.read_parquet(\"foo\", dtype_backend=\"pyarrow\")\n",
    "\n",
    "\n",
    "assert df_loaded.bar.dtype == pd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded.bar.dtype == df.bar.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "pa_dict = pa.dictionary(pa.int32(), pa.string())\n",
    "pd_dict = pd.ArrowDtype(pa_dict)\n",
    "\n",
    "# create table in pyarrow\n",
    "data = {\"foo\": [1, 2], \"bar\": [\"a\", \"b\"]}\n",
    "schema = pa.schema({\"foo\": pa.int32(), \"bar\": pa_dict})\n",
    "table = pa.table(data, schema=schema)\n",
    "\n",
    "# serialize in arrow and loading to pandas works\n",
    "pa.parquet.write_table(table, \"demo.parquet\")\n",
    "df = pd.read_parquet(\"demo.parquet\", dtype_backend=\"pyarrow\")\n",
    "\n",
    "assert df.bar.dtype == pd_dict  # ✔ the dtype is dictionary[int32,string]\n",
    "\n",
    "# # saving and re-loading doesn't\n",
    "# df.to_parquet(\"demo2.parquet\")\n",
    "# pd.read_parquet(\"demo2.parquet\", dtype_backend=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.timeseries.hist(figsize=(20, 12), density=True, log=True, bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ds.timeseries.astype({\"wd\": pd.ArrowDtype(pa.dictionary(pa.int32(), pa.string()))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "pa_dict = pa.dictionary(pa.int32(), pa.string())\n",
    "pd_dict = pd.ArrowDtype(pa_dict)\n",
    "\n",
    "# create table in pyarrow\n",
    "data = {\"foo\": [1, 2], \"bar\": [\"a\", \"b\"]}\n",
    "schema = pa.schema({\"foo\": pa.int32(), \"bar\": pa_dict})\n",
    "table = pa.table(data, schema=schema)\n",
    "\n",
    "# serliaze in arrow and loading to pandas works\n",
    "pa.parquet.write_table(table, \"demo.parquet\")\n",
    "df = pd.read_parquet(\"demo.parquet\", dtype_backend=\"pyarrow\")\n",
    "\n",
    "assert df.bar.dtype == pd_dict  # ✔ the dtype is dictionary[int32,string]\n",
    "\n",
    "# saving and re-loading doesn't\n",
    "df.to_parquet(\"demo2.parquet\")\n",
    "pd.read_parquet(\"demo2.parquet\", dtype_backend=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
