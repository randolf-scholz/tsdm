{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Torch Implementation of Set function for Time Series\n",
    "\n",
    "\n",
    "Base Model: Deep-Set Architecture\n",
    "\n",
    "$$ f(\\mathcal{S})=g\\left(\\frac{1}{|\\mathcal{S}|} \\sum_{s_{j} \\in \\mathcal{S}} h\\left(s_{j}\\right)\\right) $$\n",
    "\n",
    "Modification: Scaled-Dot-Product\n",
    "Paper:\n",
    "\n",
    "$$ K_{j, i}=\\left[f^{\\prime}(\\mathcal{S}), s_{j}\\right]^{T} W_{i}$$\n",
    "\n",
    "where $f'$ is another deep-set model. \n",
    "\n",
    "$$ e_{j, i}=\\frac{K_{j, i} \\cdot Q_{i}}{\\sqrt{d}} \\quad \\text { and } \\quad a_{j, i}=\\frac{\\exp \\left(e_{j, i}\\right)}{\\sum_{j} \\exp \\left(e_{j, i}\\right)}$$\n",
    "\n",
    "> For each head, we multiply\n",
    "the set element embeddings computed via the function h\n",
    "with the attentions derived for the individual instances, i.e.\n",
    "\n",
    "$$ r_{i}=\\sum_{j} a_{j, i} h\\left(s_{j}\\right)$$\n",
    "\n",
    "The final prediction is made by\n",
    "\n",
    "$$  \\hat{y} = g\\Big(\\sum_{s∈S} a(S, s) h(s)\\Big) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes\n",
    "\n",
    "- $g$ and $h$ are usually just MLPs, $f'$ is a DeepSet\n",
    "- $m$ is the number of heads\n",
    "- $W$, $Q$ are learnable. $Q$ is initialized with zeros\n",
    "- $W_i$ has shape $(\\dim(f')+\\dim(s), d)$\n",
    "- $Q$ has shape $(m, d)$\n",
    "- $K$ has shape $(|S|, d)$\n",
    "- $E$ has shape $(|S|, m)$\n",
    "- $e_i$ is a vector of size $|S|$\n",
    "- $a_i$ is a vector of size $|S|$\n",
    "- $a(S, s)$ is $(|S|, m)$\n",
    "- $h(s)$ is $(d,)$\n",
    "- $r= [r_1, …, r_m] = \\sum_{s∈S} a(S, s) h(s)$ is of shape $(m,d)$\n",
    "- The authors do not seem to include latent dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Equations\n",
    "\n",
    "Rename: $h = ϕ$, $f' = ρ∘∑∘ψ$\n",
    "\n",
    "$$ a_{j,i} = \\operatorname{softmax}(e_i) = \\sigma(e_i)$$\n",
    "\n",
    "$$ e_{j,i} = \\frac{1}{\\sqrt{d}}K_{j, i}\\cdot Q_{i} = \\frac{1}{\\sqrt{d}}\\left[ψ(\\mathcal{S}), s_{j}\\right]^{T} W_{i}\\cdot Q_{i} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isqrt, sqrt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, jit, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import *\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Tensors for demo purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, Lmax, D = 32, 50, 7\n",
    "m, d = 6, 5\n",
    "\n",
    "batch = [torch.randn(np.random.randint(1, Lmax), D) for _ in range(B)]\n",
    "s = pad_sequence(batch, padding_value=float(\"nan\"), batch_first=True)\n",
    "mask = torch.isnan(s[..., 0])\n",
    "L = int(s.shape[1])\n",
    "s.shape, mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, input_size: int, output_size: int, num_layers: int = 2):\n",
    "        layers = []\n",
    "        for k in range(num_layers):\n",
    "            layer = nn.Linear(input_size, input_size)\n",
    "            nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "            nn.init.kaiming_normal_(layer.bias[None], nonlinearity=\"relu\")\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.ReLU())\n",
    "        else:\n",
    "            layer = nn.Linear(input_size, output_size)\n",
    "            nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "            nn.init.kaiming_normal_(layer.bias[None], nonlinearity=\"relu\")\n",
    "            layers.append(layer)\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "summary(MLP(3, 4, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSet Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "    \"\"\"Signature: `[... V, K] -> [... D]`\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        latent_size: Optional[int] = None,\n",
    "        encoder_layers: int = 2,\n",
    "        decoder_layers: int = 2,\n",
    "        # aggregation: Literal[\"min\", \"max\", \"sum\", \"mean\", \"prod\"] = \"sum\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        latent_size = input_size if latent_size is None else latent_size\n",
    "        self.encoder = MLP(input_size, latent_size, encoder_layers)\n",
    "        self.decoder = MLP(latent_size, output_size, decoder_layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Signature: [..., <Var>, D] -> [..., F]\n",
    "\n",
    "        Components:\n",
    "\n",
    "          - Encoder: [..., D] -> [..., E]\n",
    "          - Aggregation: [..., V, E] -> [..., E]\n",
    "          - Decoder: [..., E] -> [..., F]\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = torch.nanmean(x, dim=-2)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "summary(DeepSet(3, 4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = jit.script(DeepSet(7, 4))\n",
    "f(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(s.shape[1])\n",
    "assert torch.allclose(f(s[..., p, :]), f(s), atol=1e-06), torch.linalg.norm(\n",
    "    f(s[..., p, :]) - f(s)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention\n",
    "\n",
    "Keys: $K_{ji} = [f(S), s_j]^T W_i$\n",
    "- $K: |S|×d$. If we want to include batch-size, we need to pad things or operate on lists. \n",
    "    - let's do lists and hope torchscript takes care of it.\n",
    "        - ⟹ But then we need to apply components in \"listified\" manner\n",
    "        - Maybe we can write a decorator that automatically takes care of list inputs?\n",
    "            - Would that work well with torchscript?\n",
    "    - so use padding, but make sure to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randn(m, d)\n",
    "K = torch.randn(D, m, d)\n",
    "V = torch.randn(D, m, 17)\n",
    "V = torch.einsum(\"...D, DMF -> ...MF \", s, V)\n",
    "print(f\"{Q.shape=}\")\n",
    "K = torch.einsum(\"...f, fmd -> ...md\", s, K)\n",
    "print(f\"{K.shape=}\")\n",
    "QK = torch.einsum(\"...md, md -> ...m\", K, Q) / np.sqrt(d)\n",
    "QK[mask] = float(\"-inf\")\n",
    "print(f\"{QK.shape=}\")\n",
    "σ = nn.functional.softmax(QK, dim=1)\n",
    "print(f\"{σ.shape=}\")\n",
    "print(f\"{V.shape=}\")\n",
    "r = torch.nanmean(σ[..., None] * V, dim=1)\n",
    "print(f\"{r.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_k: int,\n",
    "        dim_v: int,\n",
    "        output_size: int,\n",
    "        num_heads: int = 5,\n",
    "        dim_k_latent: Optional[int] = None,\n",
    "        dim_v_latent: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        dim_q = dim_k\n",
    "\n",
    "        dim_k_latent = max(1, isqrt(dim_k)) if dim_k_latent is None else dim_k_latent\n",
    "        dim_v_latent = dim_v if dim_v_latent is None else dim_v_latent\n",
    "\n",
    "        Wq = torch.zeros((num_heads, dim_k_latent))\n",
    "        Wk = torch.randn((dim_k, num_heads, dim_k_latent)) / sqrt(dim_k)\n",
    "        Wv = torch.randn((dim_v, num_heads, dim_v_latent)) / sqrt(dim_v)\n",
    "        Wo = torch.randn((num_heads, dim_v_latent, output_size)) / sqrt(\n",
    "            num_heads * dim_v_latent\n",
    "        )\n",
    "\n",
    "        self.Wq = nn.Parameter(Wq)\n",
    "        self.Wk = nn.Parameter(Wk)\n",
    "        self.Wv = nn.Parameter(Wv)\n",
    "        self.Wo = nn.Parameter(Wo)\n",
    "        # self.softmax = nn.Softmax(dim=-2)\n",
    "        self.register_buffer(\"scale\", torch.tensor(1 / sqrt(dim_q)))\n",
    "        self.register_buffer(\"attention_weights\", torch.tensor([]))\n",
    "\n",
    "    def forward(self, K: Tensor, V: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Q : (h, q)\n",
    "        K : (..., L, d)\n",
    "        V : (..., L, d)\n",
    "        \"\"\"\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.isnan(K[..., 0])\n",
    "\n",
    "        K = torch.einsum(\"...d, dhk -> ...hk\", K, self.Wk)\n",
    "        V = torch.einsum(\"...d, dhv -> ...hv\", V, self.Wv)\n",
    "        QK = torch.einsum(\"hd, ...hd -> ...h\", self.Wq, K)\n",
    "        QK[mask] = float(\"-inf\")\n",
    "        w = F.softmax(self.scale * QK, dim=-2)\n",
    "        # w = self.softmax(self.scale * QK)\n",
    "        self.attention_weights = w\n",
    "        QKV = torch.nanmean(w[..., None] * V, dim=-3)  #  ...h,...Lhv -> ...hv\n",
    "        return torch.einsum(\"...hv, hvr -> ...r\", QKV, self.Wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = jit.script(ScaledDotProductAttention(7, 7, 2))\n",
    "model(s, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.shape, f(s).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(s).repeat(1, L, 1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = torch.tile(f(s).unsqueeze(-2), (L, 1))\n",
    "fs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([fs, s], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsdm.encoders.torch import PositionalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetFuncTS(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        latent_size: Optional[int] = None,\n",
    "        dim_keys: Optional[int] = None,\n",
    "        dim_vals: Optional[int] = None,\n",
    "        dim_time: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        dim_keys = input_size if dim_keys is None else dim_keys\n",
    "        dim_vals = input_size if dim_vals is None else dim_vals\n",
    "        dim_time = 10 if dim_time is None else dim_time\n",
    "        latent_size = input_size if latent_size is None else latent_size\n",
    "        # time_encoder\n",
    "        # feature_encoder -> CNN?\n",
    "        self.time_encoder = PositionalEncoder(dim_time, scale=1.0)\n",
    "        self.key_encoder = DeepSet(input_size + dim_time - 1, dim_keys)\n",
    "        self.value_encoder = MLP(input_size + dim_time - 1, dim_vals)\n",
    "        self.attn = ScaledDotProductAttention(\n",
    "            dim_keys + input_size + dim_time - 1, dim_vals, latent_size\n",
    "        )\n",
    "        self.head = MLP(latent_size, output_size)\n",
    "\n",
    "    def forward(self, s: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        s must be a tensor of the shape L×(2+C), sᵢ = [tᵢ, zᵢ, mᵢ], where\n",
    "        - tᵢ is timestamp\n",
    "        - zᵢ is observed value\n",
    "        - mᵢ is indentifier\n",
    "\n",
    "        C is the number of classes (one-hot encoded identifier)\n",
    "        \"\"\"\n",
    "\n",
    "        t = s[..., 0]\n",
    "        v = s[..., 1:2]\n",
    "        m = s[..., 2:]\n",
    "        time_features = self.time_encoder(t)\n",
    "        s = torch.cat([time_features, v, m], dim=-1)\n",
    "        fs = self.key_encoder(s)\n",
    "        fs = torch.tile(fs.unsqueeze(-2), (s.shape[-2], 1))\n",
    "        K = torch.cat([fs, s], dim=-1)\n",
    "        V = self.value_encoder(s)\n",
    "        mask = torch.isnan(s[..., 0])\n",
    "        z = self.attn(K, V, mask=mask)\n",
    "        y = self.head(z)\n",
    "        return y\n",
    "\n",
    "    @jit.export\n",
    "    def batch_forward(self, s: list[Tensor]) -> Tensor:\n",
    "        return torch.stack([self.forward(x) for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = PositionalEncoder(10, 0.9)\n",
    "g.scales\n",
    "g(s[:, :, 0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = jit.script(SetFuncTS(7, 8))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [torch.randn(16, 7), torch.randn(3, 7), torch.randn(7, 7)]\n",
    "model.batch_forward(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A second heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and some more text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetFuncTS(nn.Module):\n",
    "    def __init__(self, num_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder\n",
    "        self.decoder\n",
    "        self.aggregator\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Signature: `[..., <var>, ]`.\n",
    "\n",
    "        Takes list of triplet-encoded data and applies.\n",
    "        \"\"\"\n",
    "        t = torch.stack(x, dim=-1)\n",
    "        return torch.sum(t, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit.script(SetFuncTS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>>>>> input_shapes:                    [(16, 8), (16, 15009, 1), (16, 15009, 1), (16, 15009), (16,)]\n",
    ">>>>>> lengths:                         (16,)\n",
    ">>>>>> max length |S|:                  15009\n",
    ">>>>>> sum lengths ∑|S|:                238416\n",
    ">>>>>> transformed_times:               (16, 15009, 4)\n",
    ">>>>>> transformed_measurements:        (16, 15009, 24)\n",
    ">>>>>> combined_values:                 (16, 15009, 29)\n",
    ">>>>>> demo_encoded:                    (16, 29)\n",
    ">>>>>> combined_with_demo:              (16, 15010, 29)\n",
    ">>>>>> mask:                            (16, 15010)\n",
    ">>>>>> collected_values S:              (238432, 29)\n",
    ">>>>>> encoded ϕ = h(s):                (238432, 256)\n",
    ">>>>>> encoded ψ = f'(S):               (238432, 128)\n",
    ">>>>>> agg ψ:                           (16, 128)\n",
    ">>>>>> agg ρ:                           (16, 128)\n",
    ">>>>>> combined [f(S),s]:               (238432, 157)\n",
    ">>>>>> keys [f(S),s]ᵀW:                 (238432, 4, 1, 128)\n",
    ">>>>>> preattn eᵢⱼ= KQ/√d:              (238432, 4, 1, 128)\n",
    ">>>>>> attentions a(S):                 (4, 238432, 1)\n",
    ">>>>>> weighted_values:                 (4, 238432, 256)\n",
    ">>>>>> weighted_values a(S,s)h(s):      (238432, 1024)\n",
    ">>>>>> aggregated_values ∑a(S,s)h(s):   (16, 1024)\n",
    ">>>>>> output_values g(∑a(S,s)h(s)):    (16, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
