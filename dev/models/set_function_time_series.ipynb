{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Torch Implementation of Set function for Time Series\n",
    "\n",
    "\n",
    "Base Model: Deep-Set Architecture\n",
    "\n",
    "$$ f(\\mathcal{S})=g\\left(\\frac{1}{|\\mathcal{S}|} \\sum_{s_{j} \\in \\mathcal{S}} h\\left(s_{j}\\right)\\right) $$\n",
    "\n",
    "Modification: Scaled-Dot-Product\n",
    "Paper:\n",
    "\n",
    "$$ K_{j, i}=\\left[f^{\\prime}(\\mathcal{S}), s_{j}\\right]^{T} W_{i}$$\n",
    "\n",
    "where $f'$ is another deep-set model. \n",
    "\n",
    "$$ e_{j, i}=\\frac{K_{j, i} \\cdot Q_{i}}{\\sqrt{d}} \\quad \\text { and } \\quad a_{j, i}=\\frac{\\exp \\left(e_{j, i}\\right)}{\\sum_{j} \\exp \\left(e_{j, i}\\right)}$$\n",
    "\n",
    "> For each head, we multiply\n",
    "the set element embeddings computed via the function h\n",
    "with the attentions derived for the individual instances, i.e.\n",
    "\n",
    "$$ r_{i}=\\sum_{j} a_{j, i} h\\left(s_{j}\\right)$$\n",
    "\n",
    "The final prediction is made by\n",
    "\n",
    "$$  \\hat{y} = g\\Big(\\sum_{s∈S} a(S, s) h(s)\\Big) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes\n",
    "\n",
    "- $g$ and $h$ are usually just MLPs, $f'$ is a DeepSet\n",
    "- $m$ is the number of heads\n",
    "- $W$, $Q$ are learnable. $Q$ is initialized with zeros\n",
    "- $W_i$ has shape $(\\dim(f')+\\dim(s), d)$\n",
    "- $Q$ has shape $(m, d)$\n",
    "- $K$ has shape $(|S|, d)$\n",
    "- $E$ has shape $(|S|, m)$\n",
    "- $e_i$ is a vector of size $|S|$\n",
    "- $a_i$ is a vector of size $|S|$\n",
    "- $a(S, s)$ is $(|S|, m)$\n",
    "- $h(s)$ is $(d,)$\n",
    "- $r= [r_1, …, r_m] = \\sum_{s∈S} a(S, s) h(s)$ is of shape $(m,d)$\n",
    "- The authors do not seem to include latent dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Equations\n",
    "\n",
    "Rename: $h = ϕ$, $f' = ρ∘∑∘ψ$\n",
    "\n",
    "$$ a_{j,i} = \\operatorname{softmax}(e_i) = \\sigma(e_i)$$\n",
    "\n",
    "$$ e_{j,i} = \\frac{1}{\\sqrt{d}}K_{j, i}\\cdot Q_{i} = \\frac{1}{\\sqrt{d}}\\left[ψ(\\mathcal{S}), s_{j}\\right]^{T} W_{i}\\cdot Q_{i} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, jit, nn\n",
    "from typing import Final, Literal\n",
    "\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Repeat(nn.Sequential):\n",
    "    \"\"\"An copies of a module multiple times.\"\"\"\n",
    "\n",
    "    DEFAULT_HP: dict = {\n",
    "        \"__name__\": __qualname__,  # type: ignore[name-defined]\n",
    "        \"__module__\": __module__,  # type: ignore[name-defined]\n",
    "        \"module\": None,\n",
    "        \"copies\": 1,\n",
    "        \"independent\": True,\n",
    "    }\n",
    "\n",
    "    HP: Dict[str, Any]\n",
    "    \"\"\"The HP\"\"\"\n",
    "\n",
    "    def __init__(self, **HP: Any) -> None:\n",
    "        self.HP = self.DEFAULT_HP | HP\n",
    "        HP = self.HP\n",
    "        copies: list[nn.Module] = []\n",
    "\n",
    "        for k in range(HP[\"copies\"]):\n",
    "            if isinstance(HP[\"module\"], nn.Module):\n",
    "                module = HP[\"module\"]\n",
    "            else:\n",
    "                module = initialize_from_config(HP[\"module\"])\n",
    "\n",
    "            if HP[\"independent\"]:\n",
    "                copies.append(module)\n",
    "            else:\n",
    "                copies = [module] * HP[\"copies\"]\n",
    "                break\n",
    "\n",
    "        HP[\"module\"] = str(HP[\"module\"])\n",
    "        super().__init__(*copies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, input_size: int, output_size: int, num_layers: int):\n",
    "\n",
    "        layers = []\n",
    "        for k in range(num_layers):\n",
    "            layer = nn.Linear(input_size, input_size)\n",
    "            nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "            nn.init.kaiming_normal_(layer.bias[None], nonlinearity=\"relu\")\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.ReLU())\n",
    "        else:\n",
    "            layer = nn.Linear(input_size, output_size)\n",
    "            nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "            nn.init.kaiming_normal_(layer.bias[None], nonlinearity=\"relu\")\n",
    "            layers.append(layer)\n",
    "        super().__init__(*layers)\n",
    "        \n",
    "summary(MLP(3,4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "    \"\"\"Signature: `[... V, K] -> [... D]`\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        latent_size: int,\n",
    "        output_size: int,\n",
    "        encoder_layers: int = 2,\n",
    "        decoder_layers: int = 2,\n",
    "        aggregation: Literal[\"min\", \"max\", \"sum\", \"mean\", \"prod\"] = \"sum\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = MLP(input_size, latent_size, encoder_layers)\n",
    "        self.aggregation = Reduce(\"... j k -> ... k\", aggregation)\n",
    "        self.decoder = MLP(latent_size, output_size, decoder_layers)\n",
    "        \n",
    "summary(DeepSet(3,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepSet().forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4, 5)\n",
    "torch.einsum(\"ijk -> ij\", x)\n",
    "\n",
    "reduce(x, \"... (j k) -> ... k\", \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = torch.tensor(1.23)\n",
    "num_dim = 10\n",
    "1.23 ** -torch.arange(0, num_dim + 2, 2) / num_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0, 2, 4, 6, 8]\n",
    "b = [1, 3, 5, 7, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sum(zip(a, b), ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(c, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A second heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and some more text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, jit, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetFuncTS(nn.Module):\n",
    "    def __init__(self, num_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder\n",
    "        self.decoder\n",
    "        self.aggregator\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Signature: `[..., <var>, ]`.\n",
    "\n",
    "        Takes list of triplet-encoded data and applies.\n",
    "        \"\"\"\n",
    "        t = torch.stack(x, dim=-1)\n",
    "        return torch.sum(t, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit.script(SetFuncTS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs, segment_ids, lengths, training=None):\n",
    "    if training is None:\n",
    "        training = tf.keras.backend.learning_phase()\n",
    "\n",
    "    def dropout_attn(input_tensor):\n",
    "        if self.attn_dropout > 0:\n",
    "            mask = tf.random.uniform(tf.shape(input_tensor)[:-1]) < self.attn_dropout\n",
    "            return input_tensor + tf.expand_dims(tf.cast(mask, tf.float32), -1) * -1e9\n",
    "        else:\n",
    "            return tf.identity(input_tensor)\n",
    "\n",
    "    encoded = self.psi(inputs)\n",
    "    agg = self.psi_aggregation(encoded, segment_ids)\n",
    "    agg = self.rho(agg)\n",
    "    agg_scattered = tf.gather_nd(agg, tf.expand_dims(segment_ids, -1))\n",
    "    combined = tf.concat([inputs, agg_scattered], axis=-1)\n",
    "    keys = tf.matmul(combined, self.W_k)\n",
    "    keys = tf.stack(tf.split(keys, self.n_heads, -1), 1)\n",
    "    keys = tf.expand_dims(keys, axis=2)\n",
    "    # should have shape (el, heads, 1, dot_prod_dim)\n",
    "    queries = tf.expand_dims(tf.expand_dims(self.W_q, -1), 0)\n",
    "    # should have shape (1, heads, dot_prod_dim, 1)\n",
    "    preattn = tf.matmul(keys, queries) / tf.sqrt(float(self.dot_prod_dim))\n",
    "    preattn = tf.squeeze(preattn, -1)\n",
    "    preattn = smart_cond(\n",
    "        training, lambda: dropout_attn(preattn), lambda: tf.identity(preattn)\n",
    "    )\n",
    "\n",
    "    per_head_preattn = tf.unstack(preattn, axis=1)\n",
    "    attentions = []\n",
    "    for pre_attn in per_head_preattn:\n",
    "        attentions.append(segment_softmax(pre_attn, segment_ids))\n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>>>>> input_shapes:                    [(16, 8), (16, 15009, 1), (16, 15009, 1), (16, 15009), (16,)]\n",
    ">>>>>> lengths:                         (16,)\n",
    ">>>>>> max length |S|:                  15009\n",
    ">>>>>> sum lengths ∑|S|:                238416\n",
    ">>>>>> transformed_times:               (16, 15009, 4)\n",
    ">>>>>> transformed_measurements:        (16, 15009, 24)\n",
    ">>>>>> combined_values:                 (16, 15009, 29)\n",
    ">>>>>> demo_encoded:                    (16, 29)\n",
    ">>>>>> combined_with_demo:              (16, 15010, 29)\n",
    ">>>>>> mask:                            (16, 15010)\n",
    ">>>>>> collected_values S:              (238432, 29)\n",
    ">>>>>> encoded ϕ = h(s):                (238432, 256)\n",
    ">>>>>> encoded ψ = f'(S):               (238432, 128)\n",
    ">>>>>> agg ψ:                           (16, 128)\n",
    ">>>>>> agg ρ:                           (16, 128)\n",
    ">>>>>> combined [f(S),s]:               (238432, 157)\n",
    ">>>>>> keys [f(S),s]ᵀW:                 (238432, 4, 1, 128)\n",
    ">>>>>> preattn eᵢⱼ= KQ/√d:              (238432, 4, 1, 128)\n",
    ">>>>>> attentions a(S):                 (4, 238432, 1)\n",
    ">>>>>> weighted_values:                 (4, 238432, 256)\n",
    ">>>>>> weighted_values a(S,s)h(s):      (238432, 1024)\n",
    ">>>>>> aggregated_values ∑a(S,s)h(s):   (16, 1024)\n",
    ">>>>>> output_values g(∑a(S,s)h(s)):    (16, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
