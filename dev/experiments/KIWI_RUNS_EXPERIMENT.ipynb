{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc22d517-c76d-4da2-ac57-ab077be3396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b711e2-eb3a-4657-bf46-a52304d1add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = \"skew-init\"  # | input(\"enter name for run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880c03d7-324c-4bc8-ae50-5392d4ff4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# enable JIT compilation - must be done before loading torch!\n",
    "os.environ[\"PYTORCH_JIT\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20e1c36b-b316-486d-bf09-724a98b0f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torch\n",
    "import torchinfo\n",
    "from linodenet.models import LinODE, LinODECell, LinODEnet\n",
    "from linodenet.projections.functional import skew_symmetric, symmetric\n",
    "from pandas import DataFrame, Index, Series, Timedelta, Timestamp\n",
    "from torch import Tensor, jit, tensor\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.utils.data import BatchSampler, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import tsdm\n",
    "from tsdm.datasets import DATASETS\n",
    "from tsdm.encoders.functional import time2float\n",
    "from tsdm.logutils import (\n",
    "    log_kernel_information,\n",
    "    log_metrics,\n",
    "    log_model_state,\n",
    "    log_optimizer_state,\n",
    ")\n",
    "from tsdm.losses import LOSSES\n",
    "from tsdm.tasks import KIWI_RUNS_TASK\n",
    "from tsdm.util import grad_norm, multi_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c73e6-8f71-4570-ae16-0f01889052b3",
   "metadata": {},
   "source": [
    "# Initialize Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ccb53e-8b33-45be-902f-575f1ed5afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float32\n",
    "NAN = tensor(float(\"nan\"), dtype=DTYPE, device=DEVICE)\n",
    "BATCH_SIZE = 128\n",
    "PRD_HORIZON = 30\n",
    "OBS_HORIZON = 90\n",
    "HORIZON = SEQLEN = OBS_HORIZON + PRD_HORIZON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0cfe050-72d6-46c0-b062-e1dbf7ebc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = KIWI_RUNS_TASK(\n",
    "    forecasting_horizon=PRD_HORIZON,\n",
    "    observation_horizon=OBS_HORIZON,\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    eval_batch_size=2048,\n",
    ")\n",
    "\n",
    "DATASET = TASK.dataset\n",
    "ts = TASK.timeseries\n",
    "md = TASK.metadata\n",
    "NUM_PTS, NUM_DIM = ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ad313-0bca-47ae-a23d-05c1430fb739",
   "metadata": {},
   "source": [
    "## Initialize Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89a3b59-6ef4-4b41-b53e-9b23ef5bbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS = TASK.test_metric.to(device=DEVICE)\n",
    "\n",
    "TASK.loss_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde25ec-dea8-462e-9164-45dff7fd7859",
   "metadata": {},
   "source": [
    "## Initialize DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f62fa2-dc36-455e-91cf-6b46b8b0e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINLOADER = TASK.batchloader\n",
    "EVALLOADERS = TASK.dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1996de-7202-46f1-8c42-70d7622db779",
   "metadata": {},
   "source": [
    "## Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeaef85b-c8a3-4cb5-a6ad-bf6312f6f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dicts(d: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Recursively join dict by composing keys with '/'.\"\"\"\n",
    "    result = {}\n",
    "    for key, val in d.items():\n",
    "        if isinstance(val, dict):\n",
    "            result |= join_dicts(\n",
    "                {f\"{key}/{subkey}\": item for subkey, item in val.items()}\n",
    "            )\n",
    "        else:\n",
    "            result[key] = val\n",
    "    return result\n",
    "\n",
    "\n",
    "def add_prefix(d: dict[str, Any], /, prefix: str) -> dict[str, Any]:\n",
    "    return {f\"{prefix}/{key}\": item for key, item in d.items()}\n",
    "\n",
    "\n",
    "# OPTIMIZER_CONIFG = {\n",
    "#     \"__name__\": \"SGD\",\n",
    "#     \"lr\": 0.001,\n",
    "#     \"momentum\": 0,\n",
    "#     \"dampening\": 0,\n",
    "#     \"weight_decay\": 0,\n",
    "#     \"nesterov\": False,\n",
    "# }\n",
    "\n",
    "OPTIMIZER_CONIFG = {\n",
    "    \"__name__\": \"Adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"eps\": 1e-08,\n",
    "    \"weight_decay\": 0,\n",
    "    \"amsgrad\": False,\n",
    "}\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"__name__\": \"LinODEnet\",\n",
    "    \"input_size\": NUM_DIM,\n",
    "    \"hidden_size\": 128,\n",
    "    \"embedding_type\": \"concat\",\n",
    "    \"Encoder_cfg\": {\"nblocks\": 10},\n",
    "    \"Decoder_cfg\": {\"nblocks\": 10},\n",
    "    \"System_cfg\": {\n",
    "        \"kernel_initialization\": \"skew-symmetric\",\n",
    "        \"kernel_parametrization\": \"skew_symmetric\",\n",
    "        \"scale\": 0.1,\n",
    "    },\n",
    "}\n",
    "\n",
    "HPARAMS = join_dicts(\n",
    "    {\n",
    "        \"Optimizer\": OPTIMIZER_CONIFG,\n",
    "        \"Model\": MODEL_CONFIG,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32262eb-5d33-49fa-b68d-e2688ac618fe",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e09543-0692-4f23-af83-912d14a18ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = LinODEnet\n",
    "model = MODEL(**MODEL_CONFIG)\n",
    "model.to(device=DEVICE, dtype=DTYPE)\n",
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e71ba-af79-4c01-a587-3785154e4036",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initalize Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c34d87a-a015-46c7-8448-19731bbf7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsdm.optimizers import OPTIMIZERS\n",
    "from tsdm.util import initialize_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbabd0e-473c-4900-ae00-1819f116c96a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OPTIMIZER_CONIFG |= {\"params\": model.parameters()}\n",
    "optimizer = initialize_from(OPTIMIZERS, **OPTIMIZER_CONIFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e8ab3-a104-4492-b68a-d7f67e739921",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2de2a66d-5536-40d9-a209-11ba8d133a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(TRAINLOADER[0]))\n",
    "T, X = batch\n",
    "targets = X[..., OBS_HORIZON:, TASK.targets.index].clone()\n",
    "# assert targets.shape == (BATCH_SIZE, PRD_HORIZON, len(TASK.targets))\n",
    "\n",
    "inputs = X.clone()\n",
    "inputs[:, OBS_HORIZON:, TASK.targets.index] = NAN\n",
    "inputs[:, OBS_HORIZON:, TASK.observables.index] = NAN\n",
    "# assert inputs.shape == (BATCH_SIZE, HORIZON, NUM_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88ea547d-d10e-420e-8f93-5a32069d99fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets = X[..., OBS_HORIZON:, TASK.targets.index].clone()\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20388705-69a4-4765-bde4-d27527def516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_batch(batch: tuple[Tensor, Tensor]):\n",
    "    \"\"\"Get batch and create model inputs and targets\"\"\"\n",
    "    T, X = batch\n",
    "    targets = X[..., OBS_HORIZON:, TASK.targets.index].clone()\n",
    "    # assert targets.shape == (BATCH_SIZE, PRD_HORIZON, len(TASK.targets))\n",
    "\n",
    "    inputs = X.clone()\n",
    "    inputs[:, OBS_HORIZON:, TASK.targets.index] = NAN\n",
    "    inputs[:, OBS_HORIZON:, TASK.observables.index] = NAN\n",
    "    # assert inputs.shape == (BATCH_SIZE, HORIZON, NUM_DIM)\n",
    "    return T, inputs, targets\n",
    "\n",
    "\n",
    "def get_all_preds(model, dataloader):\n",
    "    Y, Ŷ = [], []\n",
    "    for batch in (pbar := tqdm(dataloader, leave=False)):\n",
    "        with torch.no_grad():\n",
    "            model.zero_grad()\n",
    "            times, inputs, targets = prep_batch(batch)\n",
    "            outputs = model(times, inputs)\n",
    "            predics = outputs[:, OBS_HORIZON:, TASK.targets.index]\n",
    "            loss = LOSS(targets, predics)\n",
    "            Y.append(targets)\n",
    "            Ŷ.append(predics)\n",
    "        if pbar.n == 5:\n",
    "            break\n",
    "\n",
    "    targets, predics = torch.cat(Y, dim=0), torch.cat(Ŷ, dim=0)\n",
    "    mask = torch.isnan(targets)\n",
    "    targets[mask] = torch.tensor(0.0)\n",
    "    predics[mask] = torch.tensor(0.0)\n",
    "    # scale = 1/torch.mean(mask.to(dtype=torch.float32))\n",
    "    # targets *= scale\n",
    "    # predics *= scale\n",
    "    return targets, predics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5892b9-c3ad-4b3b-992e-e082b600227c",
   "metadata": {},
   "source": [
    "## Logging Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2919d5f-1d1f-4147-aacf-3e50d09230ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsdm.logutils import compute_metrics\n",
    "\n",
    "\n",
    "def log_all(i, model, writer, optimizer):\n",
    "    kernel = model.system.kernel.clone().detach().cpu()\n",
    "    log_kernel_information(i, writer, kernel, histograms=True)\n",
    "    log_optimizer_state(i, writer, optimizer, histograms=True)\n",
    "\n",
    "\n",
    "def log_hparams(i, writer, *, metric_dict, hparam_dict):\n",
    "    hparam_dict |= {\"epoch\": i}\n",
    "    metric_dict = add_prefix(metric_dict, \"hparam\")\n",
    "    writer.add_hparams(hparam_dict=hparam_dict, metric_dict=metric_dict)\n",
    "\n",
    "\n",
    "metrics = {key: LOSSES[key] for key in (\"ND\", \"NRMSE\", \"MSE\", \"MAE\")}\n",
    "# assert any(isinstance(TASK.test_metric, metric) for metric in metrics.values())\n",
    "metrics = {key: LOSSES[key]() for key in (\"ND\", \"NRMSE\", \"MSE\", \"MAE\")} | {\n",
    "    \"WRMSE\": LOSS\n",
    "}\n",
    "\n",
    "print(\"WARMUP\")\n",
    "t = torch.randn(NUM_DIM).to(DEVICE)\n",
    "x = torch.randn(1, NUM_DIM).to(device=DEVICE)\n",
    "y = model(t, x)\n",
    "torch.linalg.norm(y).backward()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d902635f-ae08-43aa-b60b-d21805d83f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_START = tsdm.util.now()\n",
    "CHECKPOINTDIR = Path(\n",
    "    f\"checkpoints/{MODEL.__name__}/{DATASET.__name__}/{RUN_NAME}/{RUN_START}\"\n",
    ")\n",
    "CHECKPOINTDIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGGING_DIR = f\"runs/{MODEL.__name__}/{DATASET.__name__}/{RUN_NAME}/{RUN_START}\"\n",
    "writer = SummaryWriter(LOGGING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62361bf2-16b2-46b0-bc69-71bee7045422",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e9b5c3-8da4-4e5b-b4a5-f0dfac9ff450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "epoch = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    # log optimizer state first !!!\n",
    "    # log_optimizer_state(epoch, writer, optimizer, histograms=True)\n",
    "    log_kernel_information(epoch, writer, model.system.kernel, histograms=True)\n",
    "\n",
    "    for key in ((0, \"train\"), (0, \"test\")):\n",
    "        dataloader = EVALLOADERS[key]\n",
    "        y, ŷ = get_all_preds(model, dataloader)\n",
    "        assert torch.isfinite( y).all()\n",
    "        log_metrics(\n",
    "            epoch, writer, metrics=metrics, targets=y, predics=ŷ, prefix=key[1]\n",
    "        )\n",
    "\n",
    "\n",
    "for _ in (epochs := trange(100)):\n",
    "    epoch += 1\n",
    "    for batch in (batches := tqdm(TRAINLOADER[0])):\n",
    "        i += 1\n",
    "        # Optimization step\n",
    "        model.zero_grad()\n",
    "        times, inputs, targets = prep_batch(batch)\n",
    "\n",
    "        forward_time = time()\n",
    "        outputs = model(times, inputs)\n",
    "        forward_time = time() - forward_time\n",
    "\n",
    "        predics = outputs[:, OBS_HORIZON:, TASK.targets.index]\n",
    "\n",
    "        # get rid of nan-values in teh targets.\n",
    "        mask = torch.isnan(targets)\n",
    "        targets[mask] = torch.tensor(0.0)\n",
    "        predics[mask] = torch.tensor(0.0)\n",
    "\n",
    "        # # compensate NaN-Value with upscaling\n",
    "        # scale = 1/torch.mean(mask.to(dtype=torch.float32))\n",
    "        # targets *= scale\n",
    "        # predics *= scale\n",
    "\n",
    "        loss = LOSS(targets, predics)\n",
    "\n",
    "        backward_time = time()\n",
    "        loss.backward()\n",
    "        backward_time = time() - backward_time\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # batch logging\n",
    "        logging_time = time()\n",
    "        with torch.no_grad():\n",
    "            log_metrics(\n",
    "                i,\n",
    "                writer,\n",
    "                metrics=metrics,\n",
    "                targets=targets,\n",
    "                predics=predics,\n",
    "                prefix=\"batch\",\n",
    "            )\n",
    "            log_optimizer_state(i, writer, optimizer, prefix=\"batch\")\n",
    "\n",
    "            lval = loss.clone().detach().cpu().numpy()\n",
    "            gval = grad_norm(list(model.parameters())).clone().detach().cpu().numpy()\n",
    "            if torch.any(torch.isnan(loss)):\n",
    "                raise RuntimeError(\"NaN-value encountered!!\")\n",
    "        logging_time = time() - logging_time\n",
    "\n",
    "        batches.set_postfix(\n",
    "            loss=f\"{lval:.2e}\",\n",
    "            gnorm=f\"{gval:.2e}\",\n",
    "            Δt_forward=f\"{forward_time:.1f}\",\n",
    "            Δt_backward=f\"{backward_time:.1f}\",\n",
    "            Δt_logging=f\"{logging_time:.1f}\",\n",
    "        )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # log optimizer state first !!!\n",
    "        log_optimizer_state(epoch, writer, optimizer, histograms=True)\n",
    "        log_kernel_information(epoch, writer, model.system.kernel, histograms=True)\n",
    "\n",
    "        for key in ((0, \"train\"), (0, \"test\")):\n",
    "            dataloader = EVALLOADERS[key]\n",
    "            y, ŷ = get_all_preds(model, dataloader)\n",
    "            metric_values = compute_metrics(metrics, targets=y, predics=ŷ)\n",
    "            log_metrics(\n",
    "                epoch, writer, metrics=metrics, values=metric_values, prefix=key[1]\n",
    "            )\n",
    "            # log_hparams(epoch, writer, metric_dict=metric_values, hparam_dict=HPARAMS)\n",
    "\n",
    "        # Model Checkpoint\n",
    "        torch.jit.save(model, CHECKPOINTDIR.joinpath(f\"{MODEL.__name__}-{epochs.n}\"))\n",
    "        torch.save(\n",
    "            {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": i,\n",
    "            },\n",
    "            CHECKPOINTDIR.joinpath(f\"{optimizer.__class__.__name__}-{epochs.n}\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505db793-d81a-442e-85ec-997fb5056333",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffers = dict(model.named_buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d915d3d-f7ef-4fc0-afaa-0300d240ceb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b9f1c-4a0f-4b2e-9a66-fa9a4f4589e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83ec26-f1de-42d0-aaa1-77febc770d21",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "zhat_pre = buffers['zhat_pre']\n",
    "zhat_post = buffers['zhat_post']\n",
    "xhat_pre = buffers['xhat_pre']\n",
    "xhat_post = buffers['xhat_post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5de7a-45b2-4e60-9606-fd05a57adb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhat_post.shape, zhat_pre.shape, xhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7ee84-7786-4de7-8347-52d21aff84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhat_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c5f33-598b-4384-b10a-950de208093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xhat_post - xhat_pre).abs().amax(dim=(0,2)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675025e-308c-4f12-821a-9e6406b80d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "(zhat_pre - zhat_post).abs().amax(dim=(0,2)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e18e4-7d01-458b-b318-f04b38a7fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[buffers['xhat_pre'][:, i, :].max() for i in range(120)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a236bf5f-7664-461a-b838-c30308f8a2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc4dcd-4ead-4316-8db7-b3aa0ec5686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffers['xhat_post'][:, -1, :].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4460473-200c-44e9-9abe-c0b6999d316c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
