{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4, floatmode='fixed', suppress=True)\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "from typing import (\n",
    "    TYPE_CHECKING,\n",
    "    cast,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas._libs.reshape as libreshape\n",
    "from pandas._libs.sparse import IntIndex\n",
    "from pandas._typing import Dtype\n",
    "from pandas.util._decorators import cache_readonly\n",
    "\n",
    "from pandas.core.dtypes.cast import maybe_promote\n",
    "from pandas.core.dtypes.common import (\n",
    "    ensure_platform_int,\n",
    "    is_1d_only_ea_dtype,\n",
    "    is_bool_dtype,\n",
    "    is_extension_array_dtype,\n",
    "    is_integer,\n",
    "    is_integer_dtype,\n",
    "    is_list_like,\n",
    "    is_object_dtype,\n",
    "    needs_i8_conversion,\n",
    ")\n",
    "from pandas.core.dtypes.dtypes import ExtensionDtype\n",
    "from pandas.core.dtypes.missing import notna\n",
    "\n",
    "import pandas.core.algorithms as algos\n",
    "from pandas.core.arrays import SparseArray\n",
    "from pandas.core.arrays.categorical import factorize_from_iterable\n",
    "from pandas.core.construction import ensure_wrapped_if_datetimelike\n",
    "from pandas.core.frame import DataFrame\n",
    "from pandas.core.indexes.api import (\n",
    "    Index,\n",
    "    MultiIndex,\n",
    ")\n",
    "from pandas.core.series import Series\n",
    "from pandas.core.sorting import (\n",
    "    compress_group_index,\n",
    "    decons_obs_group_ids,\n",
    "    get_compressed_ids,\n",
    "    get_group_index,\n",
    "    get_group_index_sorter,\n",
    ")\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from pandas.core.arrays import ExtensionArray\n",
    "\n",
    "\n",
    "class _Unstacker:\n",
    "    \"\"\"\n",
    "    Helper class to unstack data / pivot with multi-level index\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index : MultiIndex\n",
    "    level : int or str, default last level\n",
    "        Level to \"unstack\". Accepts a name for the level.\n",
    "    fill_value : scalar, optional\n",
    "        Default value to fill in missing values if subgroups do not have the\n",
    "        same set of labels. By default, missing values will be replaced with\n",
    "        the default fill value for that data type, NaN for float, NaT for\n",
    "        datetimelike, etc. For integer types, by default data will converted to\n",
    "        float and missing values will be set to NaN.\n",
    "    constructor : object\n",
    "        Pandas ``DataFrame`` or subclass used to create unstacked\n",
    "        response.  If None, DataFrame will be used.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\n",
    "    ...                                    ('two', 'a'), ('two', 'b')])\n",
    "    >>> s = pd.Series(np.arange(1, 5, dtype=np.int64), index=index)\n",
    "    >>> s\n",
    "    one  a    1\n",
    "         b    2\n",
    "    two  a    3\n",
    "         b    4\n",
    "    dtype: int64\n",
    "\n",
    "    >>> s.unstack(level=-1)\n",
    "         a  b\n",
    "    one  1  2\n",
    "    two  3  4\n",
    "\n",
    "    >>> s.unstack(level=0)\n",
    "       one  two\n",
    "    a    1    3\n",
    "    b    2    4\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    unstacked : DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index: MultiIndex, level=-1, constructor=None):\n",
    "\n",
    "        if constructor is None:\n",
    "            constructor = DataFrame\n",
    "        self.constructor = constructor\n",
    "\n",
    "        self.index = index.remove_unused_levels()\n",
    "\n",
    "        self.level = self.index._get_level_number(level)\n",
    "\n",
    "        # when index includes `nan`, need to lift levels/strides by 1\n",
    "        self.lift = 1 if -1 in self.index.codes[self.level] else 0\n",
    "\n",
    "        # Note: the \"pop\" below alters these in-place.\n",
    "        self.new_index_levels = list(self.index.levels)\n",
    "        self.new_index_names = list(self.index.names)\n",
    "\n",
    "        self.removed_name = self.new_index_names.pop(self.level)\n",
    "        self.removed_level = self.new_index_levels.pop(self.level)\n",
    "        self.removed_level_full = index.levels[self.level]\n",
    "\n",
    "        # Bug fix GH 20601\n",
    "        # If the data frame is too big, the number of unique index combination\n",
    "        # will cause int32 overflow on windows environments.\n",
    "        # We want to check and raise an error before this happens\n",
    "        num_rows = np.max([index_level.size for index_level in self.new_index_levels])\n",
    "        num_columns = self.removed_level.size\n",
    "\n",
    "        # GH20601: This forces an overflow if the number of cells is too high.\n",
    "        num_cells = np.multiply(num_rows, num_columns, dtype=np.int32)\n",
    "\n",
    "        if num_rows > 0 and num_columns > 0 and num_cells <= 0:\n",
    "            raise ValueError(\"Unstacked DataFrame is too big, causing int32 overflow\")\n",
    "\n",
    "        self._make_selectors()\n",
    "\n",
    "    @cache_readonly\n",
    "    def _indexer_and_to_sort(\n",
    "        self,\n",
    "    ) -> tuple[\n",
    "        np.ndarray,  # np.ndarray[np.intp]\n",
    "        list[np.ndarray],  # each has _some_ signed integer dtype\n",
    "    ]:\n",
    "        v = self.level\n",
    "\n",
    "        codes = list(self.index.codes)\n",
    "        levs = list(self.index.levels)\n",
    "        to_sort = codes[:v] + codes[v + 1 :] + [codes[v]]\n",
    "        sizes = tuple(len(x) for x in levs[:v] + levs[v + 1 :] + [levs[v]])\n",
    "\n",
    "        comp_index, obs_ids = get_compressed_ids(to_sort, sizes)\n",
    "        ngroups = len(obs_ids)\n",
    "\n",
    "        indexer = get_group_index_sorter(comp_index, ngroups)\n",
    "        return indexer, to_sort\n",
    "\n",
    "    @cache_readonly\n",
    "    def sorted_labels(self):\n",
    "        indexer, to_sort = self._indexer_and_to_sort\n",
    "        return [line.take(indexer) for line in to_sort]\n",
    "\n",
    "    def _make_sorted_values(self, values: np.ndarray) -> np.ndarray:\n",
    "        indexer, _ = self._indexer_and_to_sort\n",
    "\n",
    "        sorted_values = algos.take_nd(values, indexer, axis=0)\n",
    "        return sorted_values\n",
    "\n",
    "    def _make_selectors(self):\n",
    "        new_levels = self.new_index_levels\n",
    "\n",
    "        # make the mask\n",
    "        remaining_labels = self.sorted_labels[:-1]\n",
    "        level_sizes = tuple(len(x) for x in new_levels)\n",
    "\n",
    "        comp_index, obs_ids = get_compressed_ids(remaining_labels, level_sizes)\n",
    "        ngroups = len(obs_ids)\n",
    "\n",
    "        comp_index = ensure_platform_int(comp_index)\n",
    "        stride = self.index.levshape[self.level] + self.lift\n",
    "        self.full_shape = ngroups, stride\n",
    "\n",
    "        selector = self.sorted_labels[-1] + stride * comp_index + self.lift\n",
    "        mask = np.zeros(np.prod(self.full_shape), dtype=bool)\n",
    "        mask.put(selector, True)\n",
    "\n",
    "        if mask.sum() < len(self.index):\n",
    "            raise ValueError(\"Index contains duplicate entries, cannot reshape\")\n",
    "\n",
    "        self.group_index = comp_index\n",
    "        self.mask = mask\n",
    "        self.unique_groups = obs_ids\n",
    "        self.compressor = comp_index.searchsorted(np.arange(ngroups))\n",
    "\n",
    "    def get_result(self, values, value_columns, fill_value):\n",
    "\n",
    "        if values.ndim == 1:\n",
    "            values = values[:, np.newaxis]\n",
    "\n",
    "        if value_columns is None and values.shape[1] != 1:  # pragma: no cover\n",
    "            raise ValueError(\"must pass column labels for multi-column data\")\n",
    "\n",
    "        values, _ = self.get_new_values(values, fill_value)\n",
    "        columns = self.get_new_columns(value_columns)\n",
    "        index = self.new_index\n",
    "\n",
    "        return self.constructor(values, index=index, columns=columns)\n",
    "\n",
    "    def get_new_values(self, values, fill_value=None):\n",
    "\n",
    "        if values.ndim == 1:\n",
    "            values = values[:, np.newaxis]\n",
    "\n",
    "        sorted_values = self._make_sorted_values(values)\n",
    "\n",
    "        # place the values\n",
    "        length, width = self.full_shape\n",
    "        stride = values.shape[1]\n",
    "        result_width = width * stride\n",
    "        result_shape = (length, result_width)\n",
    "        mask = self.mask\n",
    "        mask_all = mask.all()\n",
    "\n",
    "        # we can simply reshape if we don't have a mask\n",
    "        if mask_all and len(values):\n",
    "            # TODO: Under what circumstances can we rely on sorted_values\n",
    "            #  matching values?  When that holds, we can slice instead\n",
    "            #  of take (in particular for EAs)\n",
    "            new_values = (\n",
    "                sorted_values.reshape(length, width, stride)\n",
    "                .swapaxes(1, 2)\n",
    "                .reshape(result_shape)\n",
    "            )\n",
    "            new_mask = np.ones(result_shape, dtype=bool)\n",
    "            return new_values, new_mask\n",
    "\n",
    "        # if our mask is all True, then we can use our existing dtype\n",
    "        if mask_all:\n",
    "            dtype = values.dtype\n",
    "            new_values = np.empty(result_shape, dtype=dtype)\n",
    "            name = np.dtype(dtype).name\n",
    "        else:\n",
    "            dtype, fill_value = maybe_promote(values.dtype, fill_value)\n",
    "            if isinstance(dtype, ExtensionDtype):\n",
    "                # GH#41875\n",
    "                cls = dtype.construct_array_type()\n",
    "                new_values = cls._empty(result_shape, dtype=dtype)\n",
    "                new_values[:] = fill_value\n",
    "                name = dtype.name\n",
    "            else:\n",
    "                new_values = np.empty(result_shape, dtype=dtype)\n",
    "                new_values.fill(fill_value)\n",
    "                name = np.dtype(dtype).name\n",
    "\n",
    "        new_mask = np.zeros(result_shape, dtype=bool)\n",
    "\n",
    "        # we need to convert to a basic dtype\n",
    "        # and possibly coerce an input to our output dtype\n",
    "        # e.g. ints -> floats\n",
    "        if needs_i8_conversion(values.dtype):\n",
    "            sorted_values = sorted_values.view(\"i8\")\n",
    "            new_values = new_values.view(\"i8\")\n",
    "        elif is_bool_dtype(values.dtype):\n",
    "            sorted_values = sorted_values.astype(\"object\")\n",
    "            new_values = new_values.astype(\"object\")\n",
    "        else:\n",
    "            sorted_values = sorted_values.astype(name, copy=False)\n",
    "\n",
    "        # fill in our values & mask\n",
    "        libreshape.unstack(\n",
    "            sorted_values,\n",
    "            mask.view(\"u1\"),\n",
    "            stride,\n",
    "            length,\n",
    "            width,\n",
    "            new_values,\n",
    "            new_mask.view(\"u1\"),\n",
    "        )\n",
    "\n",
    "        # reconstruct dtype if needed\n",
    "        if needs_i8_conversion(values.dtype):\n",
    "            # view as datetime64 so we can wrap in DatetimeArray and use\n",
    "            #  DTA's view method\n",
    "            new_values = new_values.view(\"M8[ns]\")\n",
    "            new_values = ensure_wrapped_if_datetimelike(new_values)\n",
    "            new_values = new_values.view(values.dtype)\n",
    "\n",
    "        return new_values, new_mask\n",
    "\n",
    "    def get_new_columns(self, value_columns):\n",
    "        if value_columns is None:\n",
    "            if self.lift == 0:\n",
    "                return self.removed_level._rename(name=self.removed_name)\n",
    "\n",
    "            lev = self.removed_level.insert(0, item=self.removed_level._na_value)\n",
    "            return lev.rename(self.removed_name)\n",
    "\n",
    "        stride = len(self.removed_level) + self.lift\n",
    "        width = len(value_columns)\n",
    "        propagator = np.repeat(np.arange(width), stride)\n",
    "        if isinstance(value_columns, MultiIndex):\n",
    "            new_levels = value_columns.levels + (self.removed_level_full,)\n",
    "            new_names = value_columns.names + (self.removed_name,)\n",
    "\n",
    "            new_codes = [lab.take(propagator) for lab in value_columns.codes]\n",
    "        else:\n",
    "            new_levels = [value_columns, self.removed_level_full]\n",
    "            new_names = [value_columns.name, self.removed_name]\n",
    "            new_codes = [propagator]\n",
    "\n",
    "        # The two indices differ only if the unstacked level had unused items:\n",
    "        if len(self.removed_level_full) != len(self.removed_level):\n",
    "            # In this case, we remap the new codes to the original level:\n",
    "            repeater = self.removed_level_full.get_indexer(self.removed_level)\n",
    "            if self.lift:\n",
    "                repeater = np.insert(repeater, 0, -1)\n",
    "        else:\n",
    "            # Otherwise, we just use each level item exactly once:\n",
    "            repeater = np.arange(stride) - self.lift\n",
    "\n",
    "        # The entire level is then just a repetition of the single chunk:\n",
    "        new_codes.append(np.tile(repeater, width))\n",
    "        return MultiIndex(\n",
    "            levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\n",
    "        )\n",
    "\n",
    "    @cache_readonly\n",
    "    def new_index(self):\n",
    "        # Does not depend on values or value_columns\n",
    "        result_codes = [lab.take(self.compressor) for lab in self.sorted_labels[:-1]]\n",
    "\n",
    "        # construct the new index\n",
    "        if len(self.new_index_levels) == 1:\n",
    "            level, level_codes = self.new_index_levels[0], result_codes[0]\n",
    "            if (level_codes == -1).any():\n",
    "                level = level.insert(len(level), level._na_value)\n",
    "            return level.take(level_codes).rename(self.new_index_names[0])\n",
    "\n",
    "        return MultiIndex(\n",
    "            levels=self.new_index_levels,\n",
    "            codes=result_codes,\n",
    "            names=self.new_index_names,\n",
    "            verify_integrity=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def _unstack_multiple(data, clocs, fill_value=None):\n",
    "    if len(clocs) == 0:\n",
    "        return data\n",
    "\n",
    "    # NOTE: This doesn't deal with hierarchical columns yet\n",
    "\n",
    "    index = data.index\n",
    "\n",
    "    # GH 19966 Make sure if MultiIndexed index has tuple name, they will be\n",
    "    # recognised as a whole\n",
    "    if clocs in index.names:\n",
    "        clocs = [clocs]\n",
    "    clocs = [index._get_level_number(i) for i in clocs]\n",
    "\n",
    "    rlocs = [i for i in range(index.nlevels) if i not in clocs]\n",
    "\n",
    "    clevels = [index.levels[i] for i in clocs]\n",
    "    ccodes = [index.codes[i] for i in clocs]\n",
    "    cnames = [index.names[i] for i in clocs]\n",
    "    rlevels = [index.levels[i] for i in rlocs]\n",
    "    rcodes = [index.codes[i] for i in rlocs]\n",
    "    rnames = [index.names[i] for i in rlocs]\n",
    "\n",
    "    shape = tuple(len(x) for x in clevels)\n",
    "    group_index = get_group_index(ccodes, shape, sort=False, xnull=False)\n",
    "\n",
    "    comp_ids, obs_ids = compress_group_index(group_index, sort=False)\n",
    "    recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\n",
    "\n",
    "    if not rlocs:\n",
    "        # Everything is in clocs, so the dummy df has a regular index\n",
    "        dummy_index = Index(obs_ids, name=\"__placeholder__\")\n",
    "    else:\n",
    "        dummy_index = MultiIndex(\n",
    "            levels=rlevels + [obs_ids],\n",
    "            codes=rcodes + [comp_ids],\n",
    "            names=rnames + [\"__placeholder__\"],\n",
    "            verify_integrity=False,\n",
    "        )\n",
    "\n",
    "    if isinstance(data, Series):\n",
    "        dummy = data.copy()\n",
    "        dummy.index = dummy_index\n",
    "\n",
    "        unstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\n",
    "        new_levels = clevels\n",
    "        new_names = cnames\n",
    "        new_codes = recons_codes\n",
    "    else:\n",
    "        if isinstance(data.columns, MultiIndex):\n",
    "            result = data\n",
    "            for i in range(len(clocs)):\n",
    "                val = clocs[i]\n",
    "                result = result.unstack(val, fill_value=fill_value)\n",
    "                clocs = [v if v < val else v - 1 for v in clocs]\n",
    "\n",
    "            return result\n",
    "\n",
    "        # GH#42579 deep=False to avoid consolidating\n",
    "        dummy = data.copy(deep=False)\n",
    "        dummy.index = dummy_index\n",
    "\n",
    "        unstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\n",
    "        if isinstance(unstacked, Series):\n",
    "            unstcols = unstacked.index\n",
    "        else:\n",
    "            unstcols = unstacked.columns\n",
    "        assert isinstance(unstcols, MultiIndex)  # for mypy\n",
    "        new_levels = [unstcols.levels[0]] + clevels\n",
    "        new_names = [data.columns.name] + cnames\n",
    "\n",
    "        new_codes = [unstcols.codes[0]]\n",
    "        for rec in recons_codes:\n",
    "            new_codes.append(rec.take(unstcols.codes[-1]))\n",
    "\n",
    "    new_columns = MultiIndex(\n",
    "        levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\n",
    "    )\n",
    "\n",
    "    if isinstance(unstacked, Series):\n",
    "        unstacked.index = new_columns\n",
    "    else:\n",
    "        unstacked.columns = new_columns\n",
    "\n",
    "    return unstacked\n",
    "\n",
    "\n",
    "def unstack(obj, level, fill_value=None):\n",
    "\n",
    "    if isinstance(level, (tuple, list)):\n",
    "        if len(level) != 1:\n",
    "            # _unstack_multiple only handles MultiIndexes,\n",
    "            # and isn't needed for a single level\n",
    "            return _unstack_multiple(obj, level, fill_value=fill_value)\n",
    "        else:\n",
    "            level = level[0]\n",
    "\n",
    "    # Prioritize integer interpretation (GH #21677):\n",
    "    if not is_integer(level) and not level == \"__placeholder__\":\n",
    "        level = obj.index._get_level_number(level)\n",
    "\n",
    "    if isinstance(obj, DataFrame):\n",
    "        if isinstance(obj.index, MultiIndex):\n",
    "            return _unstack_frame(obj, level, fill_value=fill_value)\n",
    "        else:\n",
    "            return obj.T.stack(dropna=False)\n",
    "    elif not isinstance(obj.index, MultiIndex):\n",
    "        # GH 36113\n",
    "        # Give nicer error messages when unstack a Series whose\n",
    "        # Index is not a MultiIndex.\n",
    "        raise ValueError(\n",
    "            f\"index must be a MultiIndex to unstack, {type(obj.index)} was passed\"\n",
    "        )\n",
    "    else:\n",
    "        if is_1d_only_ea_dtype(obj.dtype):\n",
    "            return _unstack_extension_series(obj, level, fill_value)\n",
    "        unstacker = _Unstacker(\n",
    "            obj.index, level=level, constructor=obj._constructor_expanddim\n",
    "        )\n",
    "        return unstacker.get_result(\n",
    "            obj._values, value_columns=None, fill_value=fill_value\n",
    "        )\n",
    "\n",
    "\n",
    "def _unstack_frame(obj, level, fill_value=None):\n",
    "    if not obj._can_fast_transpose:\n",
    "        unstacker = _Unstacker(obj.index, level=level)\n",
    "        mgr = obj._mgr.unstack(unstacker, fill_value=fill_value)\n",
    "        return obj._constructor(mgr)\n",
    "    else:\n",
    "        unstacker = _Unstacker(obj.index, level=level, constructor=obj._constructor)\n",
    "        return unstacker.get_result(\n",
    "            obj._values, value_columns=obj.columns, fill_value=fill_value\n",
    "        )\n",
    "\n",
    "\n",
    "def _unstack_extension_series(series, level, fill_value):\n",
    "    \"\"\"\n",
    "    Unstack an ExtensionArray-backed Series.\n",
    "\n",
    "    The ExtensionDtype is preserved.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : Series\n",
    "        A Series with an ExtensionArray for values\n",
    "    level : Any\n",
    "        The level name or number.\n",
    "    fill_value : Any\n",
    "        The user-level (not physical storage) fill value to use for\n",
    "        missing values introduced by the reshape. Passed to\n",
    "        ``series.values.take``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Each column of the DataFrame will have the same dtype as\n",
    "        the input Series.\n",
    "    \"\"\"\n",
    "    # Defer to the logic in ExtensionBlock._unstack\n",
    "    df = series.to_frame()\n",
    "    result = df.unstack(level=level, fill_value=fill_value)\n",
    "    return result.droplevel(level=0, axis=1)\n",
    "\n",
    "\n",
    "def stack(frame, level=-1, dropna=True):\n",
    "    \"\"\"\n",
    "    Convert DataFrame to Series with multi-level Index. Columns become the\n",
    "    second level of the resulting hierarchical index\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stacked : Series\n",
    "    \"\"\"\n",
    "\n",
    "    def factorize(index):\n",
    "        if index.is_unique:\n",
    "            return index, np.arange(len(index))\n",
    "        codes, categories = factorize_from_iterable(index)\n",
    "        return categories, codes\n",
    "\n",
    "    N, K = frame.shape\n",
    "\n",
    "    # Will also convert negative level numbers and check if out of bounds.\n",
    "    level_num = frame.columns._get_level_number(level)\n",
    "\n",
    "    if isinstance(frame.columns, MultiIndex):\n",
    "        return _stack_multi_columns(frame, level_num=level_num, dropna=dropna)\n",
    "    elif isinstance(frame.index, MultiIndex):\n",
    "        new_levels = list(frame.index.levels)\n",
    "        new_codes = [lab.repeat(K) for lab in frame.index.codes]\n",
    "\n",
    "        clev, clab = factorize(frame.columns)\n",
    "        new_levels.append(clev)\n",
    "        new_codes.append(np.tile(clab, N).ravel())\n",
    "\n",
    "        new_names = list(frame.index.names)\n",
    "        new_names.append(frame.columns.name)\n",
    "        new_index = MultiIndex(\n",
    "            levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\n",
    "        )\n",
    "    else:\n",
    "        levels, (ilab, clab) = zip(*map(factorize, (frame.index, frame.columns)))\n",
    "        codes = ilab.repeat(K), np.tile(clab, N).ravel()\n",
    "        new_index = MultiIndex(\n",
    "            levels=levels,\n",
    "            codes=codes,\n",
    "            names=[frame.index.name, frame.columns.name],\n",
    "            verify_integrity=False,\n",
    "        )\n",
    "\n",
    "    if not frame.empty and frame._is_homogeneous_type:\n",
    "        # For homogeneous EAs, frame._values will coerce to object. So\n",
    "        # we concatenate instead.\n",
    "        dtypes = list(frame.dtypes._values)\n",
    "        dtype = dtypes[0]\n",
    "\n",
    "        if is_extension_array_dtype(dtype):\n",
    "            arr = dtype.construct_array_type()\n",
    "            new_values = arr._concat_same_type(\n",
    "                [col._values for _, col in frame.items()]\n",
    "            )\n",
    "            new_values = _reorder_for_extension_array_stack(new_values, N, K)\n",
    "        else:\n",
    "            # homogeneous, non-EA\n",
    "            new_values = frame._values.ravel()\n",
    "\n",
    "    else:\n",
    "        # non-homogeneous\n",
    "        new_values = frame._values.ravel()\n",
    "\n",
    "    if dropna:\n",
    "        mask = notna(new_values)\n",
    "        new_values = new_values[mask]\n",
    "        new_index = new_index[mask]\n",
    "\n",
    "    return frame._constructor_sliced(new_values, index=new_index)\n",
    "\n",
    "\n",
    "def stack_multiple(frame, level, dropna=True):\n",
    "    # If all passed levels match up to column names, no\n",
    "    # ambiguity about what to do\n",
    "    if all(lev in frame.columns.names for lev in level):\n",
    "        result = frame\n",
    "        for lev in level:\n",
    "            result = stack(result, lev, dropna=dropna)\n",
    "\n",
    "    # Otherwise, level numbers may change as each successive level is stacked\n",
    "    elif all(isinstance(lev, int) for lev in level):\n",
    "        # As each stack is done, the level numbers decrease, so we need\n",
    "        #  to account for that when level is a sequence of ints\n",
    "        result = frame\n",
    "        # _get_level_number() checks level numbers are in range and converts\n",
    "        # negative numbers to positive\n",
    "        level = [frame.columns._get_level_number(lev) for lev in level]\n",
    "\n",
    "        # Can't iterate directly through level as we might need to change\n",
    "        # values as we go\n",
    "        for index in range(len(level)):\n",
    "            lev = level[index]\n",
    "            result = stack(result, lev, dropna=dropna)\n",
    "            # Decrement all level numbers greater than current, as these\n",
    "            # have now shifted down by one\n",
    "            updated_level = []\n",
    "            for other in level:\n",
    "                if other > lev:\n",
    "                    updated_level.append(other - 1)\n",
    "                else:\n",
    "                    updated_level.append(other)\n",
    "            level = updated_level\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"level should contain all level names or all level \"\n",
    "            \"numbers, not a mixture of the two.\"\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def _stack_multi_column_index(columns: MultiIndex) -> MultiIndex:\n",
    "    \"\"\"Creates a MultiIndex from the first N-1 levels of this MultiIndex.\"\"\"\n",
    "    if len(columns.levels) <= 2:\n",
    "        return columns.levels[0]._rename(name=columns.names[0])\n",
    "\n",
    "    levs = [\n",
    "        [lev[c] if c >= 0 else None for c in codes]\n",
    "        for lev, codes in zip(columns.levels[:-1], columns.codes[:-1])\n",
    "    ]\n",
    "\n",
    "    # Remove duplicate tuples in the MultiIndex.\n",
    "    tuples = zip(*levs)\n",
    "    unique_tuples = (key for key, _ in itertools.groupby(tuples))\n",
    "    new_levs = zip(*unique_tuples)\n",
    "\n",
    "    # The dtype of each level must be explicitly set to avoid inferring the wrong type.\n",
    "    # See GH-36991.\n",
    "    return MultiIndex.from_arrays(\n",
    "        [\n",
    "            # Not all indices can accept None values.\n",
    "            Index(new_lev, dtype=lev.dtype) if None not in new_lev else new_lev\n",
    "            for new_lev, lev in zip(new_levs, columns.levels)\n",
    "        ],\n",
    "        names=columns.names[:-1],\n",
    "    )\n",
    "\n",
    "\n",
    "def _stack_multi_columns(frame, level_num=-1, dropna=True):\n",
    "    def _convert_level_number(level_num, columns):\n",
    "        \"\"\"\n",
    "        Logic for converting the level number to something we can safely pass\n",
    "        to swaplevel.\n",
    "\n",
    "        If `level_num` matches a column name return the name from\n",
    "        position `level_num`, otherwise return `level_num`.\n",
    "        \"\"\"\n",
    "        if level_num in columns.names:\n",
    "            return columns.names[level_num]\n",
    "\n",
    "        return level_num\n",
    "\n",
    "    this = frame.copy()\n",
    "\n",
    "    # this makes life much simpler\n",
    "    if level_num != frame.columns.nlevels - 1:\n",
    "        # roll levels to put selected level at end\n",
    "        roll_columns = this.columns\n",
    "        for i in range(level_num, frame.columns.nlevels - 1):\n",
    "            # Need to check if the ints conflict with level names\n",
    "            lev1 = _convert_level_number(i, roll_columns)\n",
    "            lev2 = _convert_level_number(i + 1, roll_columns)\n",
    "            roll_columns = roll_columns.swaplevel(lev1, lev2)\n",
    "        this.columns = roll_columns\n",
    "\n",
    "    if not this.columns._is_lexsorted():\n",
    "        # Workaround the edge case where 0 is one of the column names,\n",
    "        # which interferes with trying to sort based on the first\n",
    "        # level\n",
    "        level_to_sort = _convert_level_number(0, this.columns)\n",
    "        this = this.sort_index(level=level_to_sort, axis=1)\n",
    "\n",
    "    new_columns = _stack_multi_column_index(this.columns)\n",
    "\n",
    "    # time to ravel the values\n",
    "    new_data = {}\n",
    "    level_vals = this.columns.levels[-1]\n",
    "    level_codes = sorted(set(this.columns.codes[-1]))\n",
    "    level_vals_nan = level_vals.insert(len(level_vals), None)\n",
    "\n",
    "    level_vals_used = np.take(level_vals_nan, level_codes)\n",
    "    levsize = len(level_codes)\n",
    "    drop_cols = []\n",
    "    for key in new_columns:\n",
    "        try:\n",
    "            loc = this.columns.get_loc(key)\n",
    "        except KeyError:\n",
    "            drop_cols.append(key)\n",
    "            continue\n",
    "\n",
    "        # can make more efficient?\n",
    "        # we almost always return a slice\n",
    "        # but if unsorted can get a boolean\n",
    "        # indexer\n",
    "        if not isinstance(loc, slice):\n",
    "            slice_len = len(loc)\n",
    "        else:\n",
    "            slice_len = loc.stop - loc.start\n",
    "\n",
    "        if slice_len != levsize:\n",
    "            chunk = this.loc[:, this.columns[loc]]\n",
    "            chunk.columns = level_vals_nan.take(chunk.columns.codes[-1])\n",
    "            value_slice = chunk.reindex(columns=level_vals_used).values\n",
    "        else:\n",
    "            if frame._is_homogeneous_type and is_extension_array_dtype(\n",
    "                frame.dtypes.iloc[0]\n",
    "            ):\n",
    "                dtype = this[this.columns[loc]].dtypes.iloc[0]\n",
    "                subset = this[this.columns[loc]]\n",
    "\n",
    "                value_slice = dtype.construct_array_type()._concat_same_type(\n",
    "                    [x._values for _, x in subset.items()]\n",
    "                )\n",
    "                N, K = this.shape\n",
    "                idx = np.arange(N * K).reshape(K, N).T.ravel()\n",
    "                value_slice = value_slice.take(idx)\n",
    "\n",
    "            elif frame._is_mixed_type:\n",
    "                value_slice = this[this.columns[loc]].values\n",
    "            else:\n",
    "                value_slice = this.values[:, loc]\n",
    "\n",
    "        if value_slice.ndim > 1:\n",
    "            # i.e. not extension\n",
    "            value_slice = value_slice.ravel()\n",
    "\n",
    "        new_data[key] = value_slice\n",
    "\n",
    "    if len(drop_cols) > 0:\n",
    "        new_columns = new_columns.difference(drop_cols)\n",
    "\n",
    "    N = len(this)\n",
    "\n",
    "    if isinstance(this.index, MultiIndex):\n",
    "        new_levels = list(this.index.levels)\n",
    "        new_names = list(this.index.names)\n",
    "        new_codes = [lab.repeat(levsize) for lab in this.index.codes]\n",
    "    else:\n",
    "        old_codes, old_levels = factorize_from_iterable(this.index)\n",
    "        new_levels = [old_levels]\n",
    "        new_codes = [old_codes.repeat(levsize)]\n",
    "        new_names = [this.index.name]  # something better?\n",
    "\n",
    "    new_levels.append(level_vals)\n",
    "    new_codes.append(np.tile(level_codes, N))\n",
    "    new_names.append(frame.columns.names[level_num])\n",
    "\n",
    "    new_index = MultiIndex(\n",
    "        levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\n",
    "    )\n",
    "\n",
    "    result = frame._constructor(new_data, index=new_index, columns=new_columns)\n",
    "\n",
    "    # more efficient way to go about this? can do the whole masking biz but\n",
    "    # will only save a small amount of time...\n",
    "    if dropna:\n",
    "        result = result.dropna(axis=0, how=\"all\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_dummies(\n",
    "    data,\n",
    "    prefix=None,\n",
    "    prefix_sep=\"_\",\n",
    "    dummy_na: bool = False,\n",
    "    columns=None,\n",
    "    sparse: bool = False,\n",
    "    drop_first: bool = False,\n",
    "    dtype: Dtype | None = None,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert categorical variable into dummy/indicator variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array-like, Series, or DataFrame\n",
    "        Data of which to get dummy indicators.\n",
    "    prefix : str, list of str, or dict of str, default None\n",
    "        String to append DataFrame column names.\n",
    "        Pass a list with length equal to the number of columns\n",
    "        when calling get_dummies on a DataFrame. Alternatively, `prefix`\n",
    "        can be a dictionary mapping column names to prefixes.\n",
    "    prefix_sep : str, default '_'\n",
    "        If appending prefix, separator/delimiter to use. Or pass a\n",
    "        list or dictionary as with `prefix`.\n",
    "    dummy_na : bool, default False\n",
    "        Add a column to indicate NaNs, if False NaNs are ignored.\n",
    "    columns : list-like, default None\n",
    "        Column names in the DataFrame to be encoded.\n",
    "        If `columns` is None then all the columns with\n",
    "        `object` or `category` dtype will be converted.\n",
    "    sparse : bool, default False\n",
    "        Whether the dummy-encoded columns should be backed by\n",
    "        a :class:`SparseArray` (True) or a regular NumPy array (False).\n",
    "    drop_first : bool, default False\n",
    "        Whether to get k-1 dummies out of k categorical levels by removing the\n",
    "        first level.\n",
    "    dtype : dtype, default np.uint8\n",
    "        Data type for new columns. Only a single dtype is allowed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Dummy-coded data.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    Series.str.get_dummies : Convert Series to dummy codes.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> s = pd.Series(list('abca'))\n",
    "\n",
    "    >>> pd.get_dummies(s)\n",
    "       a  b  c\n",
    "    0  1  0  0\n",
    "    1  0  1  0\n",
    "    2  0  0  1\n",
    "    3  1  0  0\n",
    "\n",
    "    >>> s1 = ['a', 'b', np.nan]\n",
    "\n",
    "    >>> pd.get_dummies(s1)\n",
    "       a  b\n",
    "    0  1  0\n",
    "    1  0  1\n",
    "    2  0  0\n",
    "\n",
    "    >>> pd.get_dummies(s1, dummy_na=True)\n",
    "       a  b  NaN\n",
    "    0  1  0    0\n",
    "    1  0  1    0\n",
    "    2  0  0    1\n",
    "\n",
    "    >>> df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],\n",
    "    ...                    'C': [1, 2, 3]})\n",
    "\n",
    "    >>> pd.get_dummies(df, prefix=['col1', 'col2'])\n",
    "       C  col1_a  col1_b  col2_a  col2_b  col2_c\n",
    "    0  1       1       0       0       1       0\n",
    "    1  2       0       1       1       0       0\n",
    "    2  3       1       0       0       0       1\n",
    "\n",
    "    >>> pd.get_dummies(pd.Series(list('abcaa')))\n",
    "       a  b  c\n",
    "    0  1  0  0\n",
    "    1  0  1  0\n",
    "    2  0  0  1\n",
    "    3  1  0  0\n",
    "    4  1  0  0\n",
    "\n",
    "    >>> pd.get_dummies(pd.Series(list('abcaa')), drop_first=True)\n",
    "       b  c\n",
    "    0  0  0\n",
    "    1  1  0\n",
    "    2  0  1\n",
    "    3  0  0\n",
    "    4  0  0\n",
    "\n",
    "    >>> pd.get_dummies(pd.Series(list('abc')), dtype=float)\n",
    "         a    b    c\n",
    "    0  1.0  0.0  0.0\n",
    "    1  0.0  1.0  0.0\n",
    "    2  0.0  0.0  1.0\n",
    "    \"\"\"\n",
    "    from pandas.core.reshape.concat import concat\n",
    "\n",
    "    dtypes_to_encode = [\"object\", \"category\"]\n",
    "\n",
    "    if isinstance(data, DataFrame):\n",
    "        # determine columns being encoded\n",
    "        if columns is None:\n",
    "            data_to_encode = data.select_dtypes(include=dtypes_to_encode)\n",
    "        elif not is_list_like(columns):\n",
    "            raise TypeError(\"Input must be a list-like for parameter `columns`\")\n",
    "        else:\n",
    "            data_to_encode = data[columns]\n",
    "\n",
    "        # validate prefixes and separator to avoid silently dropping cols\n",
    "        def check_len(item, name):\n",
    "\n",
    "            if is_list_like(item):\n",
    "                if not len(item) == data_to_encode.shape[1]:\n",
    "                    len_msg = (\n",
    "                        f\"Length of '{name}' ({len(item)}) did not match the \"\n",
    "                        \"length of the columns being encoded \"\n",
    "                        f\"({data_to_encode.shape[1]}).\"\n",
    "                    )\n",
    "                    raise ValueError(len_msg)\n",
    "\n",
    "        check_len(prefix, \"prefix\")\n",
    "        check_len(prefix_sep, \"prefix_sep\")\n",
    "\n",
    "        if isinstance(prefix, str):\n",
    "            prefix = itertools.cycle([prefix])\n",
    "        if isinstance(prefix, dict):\n",
    "            prefix = [prefix[col] for col in data_to_encode.columns]\n",
    "\n",
    "        if prefix is None:\n",
    "            prefix = data_to_encode.columns\n",
    "\n",
    "        # validate separators\n",
    "        if isinstance(prefix_sep, str):\n",
    "            prefix_sep = itertools.cycle([prefix_sep])\n",
    "        elif isinstance(prefix_sep, dict):\n",
    "            prefix_sep = [prefix_sep[col] for col in data_to_encode.columns]\n",
    "\n",
    "        with_dummies: list[DataFrame]\n",
    "        if data_to_encode.shape == data.shape:\n",
    "            # Encoding the entire df, do not prepend any dropped columns\n",
    "            with_dummies = []\n",
    "        elif columns is not None:\n",
    "            # Encoding only cols specified in columns. Get all cols not in\n",
    "            # columns to prepend to result.\n",
    "            with_dummies = [data.drop(columns, axis=1)]\n",
    "        else:\n",
    "            # Encoding only object and category dtype columns. Get remaining\n",
    "            # columns to prepend to result.\n",
    "            with_dummies = [data.select_dtypes(exclude=dtypes_to_encode)]\n",
    "\n",
    "        for (col, pre, sep) in zip(data_to_encode.items(), prefix, prefix_sep):\n",
    "            # col is (column_name, column), use just column data here\n",
    "            dummy = _get_dummies_1d(\n",
    "                col[1],\n",
    "                prefix=pre,\n",
    "                prefix_sep=sep,\n",
    "                dummy_na=dummy_na,\n",
    "                sparse=sparse,\n",
    "                drop_first=drop_first,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            with_dummies.append(dummy)\n",
    "        result = concat(with_dummies, axis=1)\n",
    "    else:\n",
    "        result = _get_dummies_1d(\n",
    "            data,\n",
    "            prefix,\n",
    "            prefix_sep,\n",
    "            dummy_na,\n",
    "            sparse=sparse,\n",
    "            drop_first=drop_first,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "    return result\n",
    "\n",
    "\n",
    "def _get_dummies_1d(\n",
    "    data,\n",
    "    prefix,\n",
    "    prefix_sep=\"_\",\n",
    "    dummy_na: bool = False,\n",
    "    sparse: bool = False,\n",
    "    drop_first: bool = False,\n",
    "    dtype: Dtype | None = None,\n",
    ") -> DataFrame:\n",
    "    from pandas.core.reshape.concat import concat\n",
    "\n",
    "    # Series avoids inconsistent NaN handling\n",
    "    codes, levels = factorize_from_iterable(Series(data))\n",
    "\n",
    "    if dtype is None:\n",
    "        dtype = np.uint8\n",
    "    # error: Argument 1 to \"dtype\" has incompatible type \"Union[ExtensionDtype, str,\n",
    "    # dtype[Any], Type[object]]\"; expected \"Type[Any]\"\n",
    "    dtype = np.dtype(dtype)  # type: ignore[arg-type]\n",
    "\n",
    "    if is_object_dtype(dtype):\n",
    "        raise ValueError(\"dtype=object is not a valid dtype for get_dummies\")\n",
    "\n",
    "    def get_empty_frame(data) -> DataFrame:\n",
    "        if isinstance(data, Series):\n",
    "            index = data.index\n",
    "        else:\n",
    "            index = np.arange(len(data))\n",
    "        return DataFrame(index=index)\n",
    "\n",
    "    # if all NaN\n",
    "    if not dummy_na and len(levels) == 0:\n",
    "        return get_empty_frame(data)\n",
    "\n",
    "    codes = codes.copy()\n",
    "    if dummy_na:\n",
    "        codes[codes == -1] = len(levels)\n",
    "        levels = np.append(levels, np.nan)\n",
    "\n",
    "    # if dummy_na, we just fake a nan level. drop_first will drop it again\n",
    "    if drop_first and len(levels) == 1:\n",
    "        return get_empty_frame(data)\n",
    "\n",
    "    number_of_cols = len(levels)\n",
    "\n",
    "    if prefix is None:\n",
    "        dummy_cols = levels\n",
    "    else:\n",
    "        dummy_cols = Index([f\"{prefix}{prefix_sep}{level}\" for level in levels])\n",
    "\n",
    "    index: Index | None\n",
    "    if isinstance(data, Series):\n",
    "        index = data.index\n",
    "    else:\n",
    "        index = None\n",
    "\n",
    "    if sparse:\n",
    "\n",
    "        fill_value: bool | float | int\n",
    "        if is_integer_dtype(dtype):\n",
    "            fill_value = 0\n",
    "        # error: Non-overlapping equality check (left operand type: \"dtype[Any]\", right\n",
    "        # operand type: \"Type[bool]\")\n",
    "        elif dtype == bool:  # type: ignore[comparison-overlap]\n",
    "            fill_value = False\n",
    "        else:\n",
    "            fill_value = 0.0\n",
    "\n",
    "        sparse_series = []\n",
    "        N = len(data)\n",
    "        sp_indices: list[list] = [[] for _ in range(len(dummy_cols))]\n",
    "        mask = codes != -1\n",
    "        codes = codes[mask]\n",
    "        n_idx = np.arange(N)[mask]\n",
    "\n",
    "        for ndx, code in zip(n_idx, codes):\n",
    "            sp_indices[code].append(ndx)\n",
    "\n",
    "        if drop_first:\n",
    "            # remove first categorical level to avoid perfect collinearity\n",
    "            # GH12042\n",
    "            sp_indices = sp_indices[1:]\n",
    "            dummy_cols = dummy_cols[1:]\n",
    "        for col, ixs in zip(dummy_cols, sp_indices):\n",
    "            sarr = SparseArray(\n",
    "                np.ones(len(ixs), dtype=dtype),\n",
    "                sparse_index=IntIndex(N, ixs),\n",
    "                fill_value=fill_value,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            sparse_series.append(Series(data=sarr, index=index, name=col))\n",
    "\n",
    "        out = concat(sparse_series, axis=1, copy=False)\n",
    "        # TODO: overload concat with Literal for axis\n",
    "        out = cast(DataFrame, out)\n",
    "        return out\n",
    "\n",
    "    else:\n",
    "        # take on axis=1 + transpose to ensure ndarray layout is column-major\n",
    "        dummy_mat = np.eye(number_of_cols, dtype=dtype).take(codes, axis=1).T\n",
    "\n",
    "        if not dummy_na:\n",
    "            # reset NaN GH4446\n",
    "            dummy_mat[codes == -1] = 0\n",
    "\n",
    "        if drop_first:\n",
    "            # remove first GH12042\n",
    "            dummy_mat = dummy_mat[:, 1:]\n",
    "            dummy_cols = dummy_cols[1:]\n",
    "        return DataFrame(dummy_mat, index=index, columns=dummy_cols)\n",
    "\n",
    "\n",
    "def _reorder_for_extension_array_stack(\n",
    "    arr: ExtensionArray, n_rows: int, n_columns: int\n",
    ") -> ExtensionArray:\n",
    "    \"\"\"\n",
    "    Re-orders the values when stacking multiple extension-arrays.\n",
    "\n",
    "    The indirect stacking method used for EAs requires a followup\n",
    "    take to get the order correct.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : ExtensionArray\n",
    "    n_rows, n_columns : int\n",
    "        The number of rows and columns in the original DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    taken : ExtensionArray\n",
    "        The original `arr` with elements re-ordered appropriately\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> arr = np.array(['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "    >>> _reorder_for_extension_array_stack(arr, 2, 3)\n",
    "    array(['a', 'c', 'e', 'b', 'd', 'f'], dtype='<U1')\n",
    "\n",
    "    >>> _reorder_for_extension_array_stack(arr, 3, 2)\n",
    "    array(['a', 'd', 'b', 'e', 'c', 'f'], dtype='<U1')\n",
    "    \"\"\"\n",
    "    # final take to get the order correct.\n",
    "    # idx is an indexer like\n",
    "    # [c0r0, c1r0, c2r0, ...,\n",
    "    #  c0r1, c1r1, c2r1, ...]\n",
    "    idx = np.arange(n_rows * n_columns).reshape(n_columns, n_rows).T.ravel()\n",
    "    return arr.take(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.core.arrays.categorical.factorize_from_iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
