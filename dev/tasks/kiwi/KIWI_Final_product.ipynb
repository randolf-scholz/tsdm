{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiwi Final Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4, floatmode=\"fixed\", suppress=True)\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cached_property\n",
    "from types import SimpleNamespace\n",
    "from typing import Any, Literal, Union\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Interval, Series, Timedelta, Timestamp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from tsdm.datasets import KIWI_RUNS\n",
    "from tsdm.tasks import KIWI_RUNS_TASK, BaseTask\n",
    "\n",
    "dataset = KIWI_RUNS()\n",
    "ts = dataset.timeseries.drop([355, 445, 482])\n",
    "md = dataset.timeseries.drop([355, 445, 482])\n",
    "task = KIWI_RUNS_TASK()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = task.split_idx_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the induction time and time/value of final product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = SimpleNamespace()\n",
    "\n",
    "target: Literal[\"OD600\", \"Fluo_GFP\"] = \"Fluo_GFP\"\n",
    "t_min: Union[str, Timedelta] = \"0.6h\"\n",
    "delta_t: Union[str, Timedelta] = \"5m\"\n",
    "eval_batch_size: int = 128\n",
    "train_batch_size: int = 32\n",
    "\n",
    "self.target = target\n",
    "self.delta_t = Timedelta(delta_t)\n",
    "self.t_min = Timedelta(t_min)\n",
    "self.eval_batch_size = eval_batch_size\n",
    "self.train_batch_size = train_batch_size\n",
    "\n",
    "# setup dataset\n",
    "self.dataset = KIWI_RUNS()\n",
    "self.dataset.timeseries = self.dataset.timeseries.drop([355, 445, 482])\n",
    "self.dataset.metadata = self.dataset.metadata.drop([355, 445, 482])\n",
    "self.units: DataFrame = self.dataset.units\n",
    "self.metadata: DataFrame = self.dataset.metadata\n",
    "self.timeseries: DataFrame = self.dataset.timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with empty table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_induction_time(s: Series) -> Timestamp:\n",
    "    # Compute the induction time\n",
    "    # s = ts.loc[run_id, experiment_id]\n",
    "    inducer = s[\"InducerConcentration\"]\n",
    "    total_induction = inducer[-1] - inducer[0]\n",
    "\n",
    "    if pd.isna(total_induction) or total_induction == 0:\n",
    "        return pd.NA\n",
    "\n",
    "    inductions = inducer[inducer.diff() != 0.0]\n",
    "    assert len(inductions) == 1, \"Multiple Inductions occur!\"\n",
    "    return inductions.first_valid_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_product(s: Series) -> Timestamp:\n",
    "    # Final and target times\n",
    "    targets = s[target]\n",
    "    mask = pd.notna(targets)\n",
    "    targets = targets[mask]\n",
    "    assert len(targets) >= 1, f\"not enough target observations {targets}\"\n",
    "    return targets.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_table(ts: DataFrame):\n",
    "    columns = [\n",
    "        \"slice\",\n",
    "        \"delta_t\",\n",
    "        \"t_min\",\n",
    "        \"t_induction\",\n",
    "        \"t_max\",\n",
    "        \"t_final\",\n",
    "        \"y_final\",\n",
    "    ]\n",
    "    index = ts.reset_index(level=[2]).index.unique()\n",
    "    df = DataFrame(index=index, columns=columns)\n",
    "\n",
    "    df[\"t_min\"] = self.t_min\n",
    "    df[\"delta_t\"] = self.delta_t\n",
    "\n",
    "    for idx, slc in tqdm(ts.groupby(level=[0, 1])):\n",
    "        slc = slc.reset_index(level=[0, 1], drop=True)\n",
    "        # display(slc)\n",
    "        t_induction = get_induction_time(slc)\n",
    "        t_final = get_final_product(slc)\n",
    "        if pd.isna(t_induction):\n",
    "            print(f\"{idx}: no t_induction!\")\n",
    "            t_max = get_final_product(slc.loc[slc.index < t_final])\n",
    "            assert t_max < t_final\n",
    "        else:\n",
    "            assert t_induction < t_final, f\"{t_induction=} after {t_final}!\"\n",
    "            t_max = t_induction\n",
    "        df.loc[idx, \"t_max\"] = t_max\n",
    "        df.loc[idx, \"t_min\"] = t_min = slc.index[0] + self.t_min\n",
    "        df.loc[idx, \"t_induction\"] = t_induction\n",
    "        df.loc[idx, \"t_final\"] = t_final\n",
    "        df.loc[idx, \"y_final\"] = slc.loc[t_final, target]\n",
    "        df.loc[idx, \"slice\"] = slice(t_min, t_max)\n",
    "        # = t_final\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_product_times = get_time_table(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slc = final_product_times.slice.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(439, 15325, slc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = KIWI_RUNS_TASK()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sampling\n",
    "\n",
    "for each time-series, we create a sampler that creates timeslices.\n",
    "\n",
    "\n",
    "\n",
    "### IntervalSampler\n",
    "\n",
    "Returns all intervals `[a, b]` such that\n",
    "\n",
    "- `a = t₀ + i⋅sₖ`\n",
    "- `b = t₀ + i⋅sₖ + Δtₖ`\n",
    "- `i, k ∈ ℤ`\n",
    "- `a ≥ t_min`\n",
    "- `b ≤ t_max`\n",
    "- `sₖ` is the stride corresponding to intervals of size `Δtₖ`\n",
    "- interval sizes can be provided by one of:\n",
    "   - single value -> `Δtₖ` will be integer multiples of it\n",
    "   - `Sequence[type]`\n",
    "   - `Mapping[int, type]`\n",
    "   - `Callable[[int], type]`\n",
    "- stride sizes can be provided via one of:\n",
    "   - single value -> `sₖ` will be integer multiples of it\n",
    "   - `Sequence[type]`\n",
    "   - `Mapping[int, type]`\n",
    "   - `Callable[[int], type]`\n",
    "\n",
    "**Mandatory Inputs**\n",
    "\n",
    "- `t_min: Timestamp`\n",
    "- `t_max: Timestamp`\n",
    "\n",
    "\n",
    "**Optional: Exactly one of the following**\n",
    "- `num_slices: int`\n",
    "- `delta_t: TimeDelta` \n",
    "- `grid: Sequence[Timestamp]`\n",
    "\n",
    "**Optional Inputs**\n",
    "- `t_offset: Timestamp = t_min` The basepoint for the grid. Can also be randomly generated, if required.\n",
    "- `min_length: TimeDelta | int = 0`\n",
    "  If int, the minimum multiple of `Δt` allowed.\n",
    "  If TimeDelta, then the lower bound for multiples of `Δt`\n",
    "- `max_length: TimeDelta | int = t_max-t_min`\n",
    "  If int, the maximum multiple of `Δt` allowed.\n",
    "  If TimeDelta, then the upper bound for multiples of `Δt`\n",
    "- `shuffle: bool = True` Whether to randomly order the generated slices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function\n",
    "\n",
    "Return all integers `k` for which\n",
    "\n",
    "`t_min ≤ t_0 + k⋅Δt ≤ t_max`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterator, Mapping, Sequence\n",
    "from itertools import count\n",
    "from typing import Optional, TypeVar\n",
    "\n",
    "TimedeltaLike = TypeVar(\"TimedeltaLike\", int, float, Timedelta)\n",
    "TimestampLike = TypeVar(\"TimestampLike\", int, float, Timestamp)\n",
    "\n",
    "\n",
    "def grid(\n",
    "    xmin: TimestampLike,\n",
    "    xmax: TimestampLike,\n",
    "    delta: TimedeltaLike,\n",
    "    xoffset: Optional[TimestampLike] = None,\n",
    ") -> list[int]:\n",
    "    \"\"\"Computes `\\{k∈ℤ∣ xₘᵢₙ ≤ x₀+k⋅Δ ≤ xₘₐₓ\\}`.\n",
    "\n",
    "    Special case: if Δ=0, returns [0]\n",
    "    \"\"\"\n",
    "\n",
    "    xo = xmin if xoffset is None else xoffset\n",
    "    zero = type(delta)(0)\n",
    "\n",
    "    if delta == zero:\n",
    "        return [0]\n",
    "\n",
    "    assert delta > zero, \"Assumption delta>0 violated!\"\n",
    "    assert xmin <= xoffset <= xmax, \"Assumption: xmin≤xoffset≤xmax violated!\"\n",
    "\n",
    "    a = xmin - xoffset\n",
    "    b = xmax - xoffset\n",
    "    kmax = b // delta\n",
    "    kmin = a // delta\n",
    "\n",
    "    assert xmin <= xo + kmin * delta\n",
    "    assert xmin > xo + (kmin - 1) * delta\n",
    "    assert xmax >= xo + kmax * delta\n",
    "    assert xmax < xo + (kmax + 1) * delta\n",
    "\n",
    "    return list(range(kmin, kmax + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.notna(task.timeseries).mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.loc[439, 15325]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = pandas.CategoricalDtype(categories=list(\"abcd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(list(\"abca\")).astype(cats)\n",
    "pd.get_dummies(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = TypeVar(\"V\")\n",
    "\n",
    "Boxed = Union[\n",
    "    Sequence[V],\n",
    "    Mapping[int, V],\n",
    "    Callable[[int], V],\n",
    "]\n",
    "\n",
    "dt_type = Union[\n",
    "    TimedeltaLike,\n",
    "    Sequence[TimedeltaLike],\n",
    "    Mapping[int, TimedeltaLike],\n",
    "    Callable[[int], TimedeltaLike],\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "\n",
    "class IntervalSampler(\n",
    "    Sampler,\n",
    "):\n",
    "    \"\"\"Returns all intervals `[a, b]` such that:\n",
    "\n",
    "    - `a = t₀ + i⋅sₖ`\n",
    "    - `b = t₀ + i⋅sₖ + Δtₖ`\n",
    "    - `i, k ∈ ℤ`\n",
    "    - `a ≥ t_min`\n",
    "    - `b ≤ t_max`\n",
    "    - `sₖ` is the stride corresponding to intervals of size `Δtₖ`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xmin,\n",
    "        xmax,\n",
    "        deltax: dt_type,\n",
    "        stride: Optional[dt_type] = None,\n",
    "        levels: Optional[Sequence[int]] = None,\n",
    "        offset: Optional[dt_type] = None,\n",
    "        multiples: bool = True,\n",
    "        shuffle: bool = True,\n",
    "    ) -> None:\n",
    "        # set stride and offset\n",
    "        zero = 0 * (xmax - xmin)\n",
    "        stride = zero if stride is None else stride\n",
    "        offset = xmin if offset is None else offset\n",
    "\n",
    "        # validate bounds\n",
    "        assert xmin <= offset <= xmax, \"Assumption: xmin≤xoffset≤xmax violated!\"\n",
    "\n",
    "        # determine delta_max\n",
    "        delta_max = max(offset - xmin, xmax - offset)\n",
    "\n",
    "        # determine levels\n",
    "        if levels is None:\n",
    "            if isinstance(deltax, Mapping):\n",
    "                levels = [k for k in deltax.keys() if deltax[l] <= delta_max]\n",
    "            elif isinstance(deltax, Sequence):\n",
    "                levels = [k for k in range(len(deltax)) if deltax[k] <= delta_max]\n",
    "            elif isinstance(deltax, Callable):\n",
    "                levels = []\n",
    "                for k in count():\n",
    "                    dt = self._get_value(deltax, k)\n",
    "                    if dt == zero:\n",
    "                        continue\n",
    "                    if dt > delta_max:\n",
    "                        break\n",
    "                    levels.append(k)\n",
    "            else:\n",
    "                levels = [0]\n",
    "        else:\n",
    "            levels = [k for k in levels if self._get_value(deltax, k) <= delta_max]\n",
    "\n",
    "        # validate levels\n",
    "        assert all(self._get_value(deltax, k) <= delta_max for k in levels)\n",
    "\n",
    "        # compute valid intervals\n",
    "        intervals: list[Interval] = []\n",
    "\n",
    "        # for each level, get all intervals\n",
    "        for k in levels:\n",
    "            dt = self._get_value(deltax, k)\n",
    "            st = self._get_value(stride, k)\n",
    "            x0 = self._get_value(offset, k)\n",
    "\n",
    "            # get valid interval bounds, probably there is an easier way to do it...\n",
    "            stridesa = grid(xmin, xmax, st, x0)\n",
    "            stridesb = grid(xmin, xmax, st, x0 + dt)\n",
    "            valid_strides = set.intersection(set(stridesa), set(stridesb))\n",
    "\n",
    "            if not valid_strides:\n",
    "                break\n",
    "\n",
    "            intervals.extend([\n",
    "                (x0 + i * st, x0 + i * st + dt, dt, st) for i in valid_strides\n",
    "            ])\n",
    "\n",
    "        # set variables\n",
    "        self.offset = offset\n",
    "        self.deltax = deltax\n",
    "        self.stride = stride\n",
    "        self.shuffle = shuffle\n",
    "        self.intervals = DataFrame(\n",
    "            intervals, columns=[\"left\", \"right\", \"delta\", \"stride\"]\n",
    "        )\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        if self.shuffle:\n",
    "            perm = np.random.permutation(len(self))\n",
    "        else:\n",
    "            perm = np.arange(len(self))\n",
    "\n",
    "        for k in perm:\n",
    "            yield self.loc[k, \"left\"], self.loc[k, \"right\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.intervals)\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        return self.intervals.__getattr__(key)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.intervals[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_value(obj: Union[V, Boxed[V]], k: int) -> V:\n",
    "        if isinstance(obj, Callable):\n",
    "            return obj(k)\n",
    "        if isinstance(obj, Sequence):\n",
    "            return obj[k]\n",
    "        # Fallback: multiple!\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (439, 15325)\n",
    "s = ts.loc[idx]\n",
    "t_min = s.index[0]\n",
    "t_max = s.index[-1]\n",
    "t_0 = t_min\n",
    "delta_t = Timedelta(\"5m\")\n",
    "stride = Timedelta(\"5m\")\n",
    "t_0, t_min, t_max, delta_t, stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = IntervalSampler(\n",
    "    xmin=t_min,\n",
    "    xmax=t_max,\n",
    "    # offset=t_0,\n",
    "    deltax=lambda k: k * delta_t,\n",
    "    stride=None,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_product = get_time_table(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_product.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "subsamplers = {}\n",
    "\n",
    "for idx, slc in tqdm(ts.groupby([\"run_id\", \"experiment_id\"])):\n",
    "    # T, X = self.preprocessor.encode(slc.reset_index(level=[0, 1], drop=True))\n",
    "    delta_t, t_min, t_induction, t_max, t_final, *_ = final_product.loc[idx]\n",
    "    subsamplers[idx] = IntervalSampler(\n",
    "        xmin=t_min,\n",
    "        xmax=t_max,\n",
    "        # offset=t_0,\n",
    "        deltax=lambda k: k * delta_t,\n",
    "        stride=None,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    datasets[idx] = slc.reset_index(level=[0, 1], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?CollectionSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.__next__ = obj.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(iter(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(thing, \"__next__\"):\n",
    "    return str(thing)\n",
    "elif isinstance(thing, dict) and _nest_lvl < get_option(\"display.pprint_nest_depth\"):\n",
    "    result = _pprint_dict(\n",
    "        thing, _nest_lvl, quote_strings=True, max_seq_items=max_seq_items\n",
    "    )\n",
    "elif is_sequence(thing) and _nest_lvl < get_option(\"display.pprint_nest_depth\"):\n",
    "    result = _pprint_seq(\n",
    "        thing,\n",
    "        _nest_lvl,\n",
    "        escape_chars=escape_chars,\n",
    "        quote_strings=quote_strings,\n",
    "        max_seq_items=max_seq_items,\n",
    "    )\n",
    "elif isinstance(thing, str) and quote_strings:\n",
    "    result = f\"'{as_escaped_string(thing)}'\"\n",
    "else:\n",
    "    result = as_escaped_string(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.dtypes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implementing the SliceSampler\n",
    "\n",
    "TODOS:\n",
    "\n",
    "- Create a **`DataLoader`**\n",
    "\n",
    "- Need a working **`TimeSeriesCollection`** object (tuple[TimeTensor] + tuple[MetaData])\n",
    "    - Need a working **`TimeTensor`** object that indexes a `torch.tensor` with a `pandas.Index` or `pandas.MultiIndex`\n",
    "- Need a working **`CollectionSampler`** object (⇝ currently does not return idx or metadata!)\n",
    "    - Return `NamedTuple` object (timeseries: list[tensor], metadata: list[tensor], index: list[tensor])\n",
    "- Implement custom **`collate_fn`** functions \n",
    "    - Just return `list[Tensor]`: Pro: simplest thing, Con: \n",
    "    - padded tensor: Pro: Simple, model must not specifically support it. Con: really bad when things have highly varying length\n",
    "        - Batch by size! (Issue: could lead to varying `batchsize`, )\n",
    "    - `PackedSequence`: Pro: Fastest code likely, Con: Model must explicitly support it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceSampler(Sampler):\n",
    "    r\"Sample slices from data\"\n",
    "    \n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        interval_sampler: Sampler,\n",
    "    ):\n",
    "    \n",
    "    self.dataset = dataset\n",
    "    self.interval_sampler = interval_sampler\n",
    "    \n",
    "    \n",
    "    def __iter__(self) -> Iterator:\n",
    "        for left, right in self.interval_sampler:\n",
    "            yield self.dataset[left:right]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Task Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KIWI_FINAL_PRODUCT(BaseTask):\n",
    "    \"\"\"Predict the final Biomass.\n",
    "\n",
    "    The goal ist to forecast the final product/biomass value only.\n",
    "    This means the problem can both be viewed as a time-series forecasting,\n",
    "    and as a time-series regression task if one ignores the final time stamp.\n",
    "\n",
    "    The evluation protocol consists of considering initial segments of the time-series `TS[t≤k*Δt]`\n",
    "    where `k` ranges over all integers satisfying `t_{min} ≤ k*Δt ≤ t_{max}`.\n",
    "\n",
    "    Here, `t_{min}` is a global constant (0.6h by defaut), `t_{max}` is chosen on a per-time-series basis\n",
    "\n",
    "    - If there was induction, `t_{max} = t_{induction}`.\n",
    "    - Else, `t_{max} = \\max\\{ t < t_{final}\\}`.\n",
    "\n",
    "    Thus, for each time-series one obtains a set of admissible slices\n",
    "\n",
    "    .. math::\n",
    "        J_i = \\{ k∈ℤ ∣ t_{min}(TS_i) ≤ k*Δt ≤ t_{max}(TS_i) \\}\n",
    "        S_i = \\{ TS_i[t≤k*Δt] ∣ k∈J_i \\}\n",
    "\n",
    "    The target metric is averaged over these slices, and each time-series weight is normalized by the number of slices.\n",
    "\n",
    "    .. math::\n",
    "        ℒ(θ) = 𝔼_i 𝔼_{S∈S_i} ℓ( ̂y(S, θ), y(S) )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: Literal[\"OD600\", \"Fluo_GFP\"] = \"Fluo_GFP\",\n",
    "        t_min: Union[str, Timedelta] = \"0.6h\",\n",
    "        delta_t: Union[str, Timedelta] = \"5m\",\n",
    "        eval_batch_size: int = 128,\n",
    "        train_batch_size: int = 32,\n",
    "    ) -> None:\n",
    "        self.target = target\n",
    "        self.delta_t = Timedelta(delta_t)\n",
    "        self.t_min = Timedelta(t_min)\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.train_batch_size = train_batch_size\n",
    "\n",
    "        # setup dataset\n",
    "        self.dataset: Dataset = KIWI_RUNS()\n",
    "        self.units: DataFrame = self.dataset.units\n",
    "        self.metadata: DataFrame = self.dataset.metadata.drop([355, 482])\n",
    "        self.timeseries: DataFrame = self.dataset.timeseries.drop([355, 482])\n",
    "\n",
    "        # compute t_max, t_induction and t_final for each time series\n",
    "\n",
    "    @cached_property\n",
    "    def index(self) -> None: ...\n",
    "\n",
    "    @cached_property\n",
    "    def split_idx(self) -> DataFrame:\n",
    "        splitter = ShuffleSplit(n_splits=5, random_state=0, test_size=0.25)\n",
    "        groups = self.metadata.groupby([\"color\", \"run_id\"])\n",
    "        group_idx = groups.ngroup()\n",
    "\n",
    "        splits = DataFrame(index=self.metadata.index)\n",
    "        for i, (train, _) in enumerate(splitter.split(groups)):\n",
    "            splits[i] = group_idx.isin(train).map({False: \"test\", True: \"train\"})\n",
    "\n",
    "        splits.columns.name = \"split\"\n",
    "        return splits.astype(\"string\").astype(\"category\")\n",
    "\n",
    "    @cached_property\n",
    "    def splits(self) -> dict[Any, tuple[DataFrame, DataFrame]]: ...\n",
    "\n",
    "    def get_dataloader(): ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
