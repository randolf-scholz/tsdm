{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107befe5-e4c4-43f4-bb1a-b9522beb41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c1437-f1ab-4cdd-b384-9f8d0fb25663",
   "metadata": {},
   "outputs": [],
   "source": [
    "_LR_SCHEDULERS = {\n",
    "    \"LambdaLR\": lr_scheduler.LambdaLR,\n",
    "    \"MultiplicativeLR\": lr_scheduler.MultiplicativeLR,\n",
    "    \"StepLR\": lr_scheduler.StepLR,\n",
    "    \"MultiStepLR\": lr_scheduler.MultiStepLR,\n",
    "    \"ExponentialLR\": lr_scheduler.ExponentialLR,\n",
    "    \"CosineAnnealingLR\": lr_scheduler.CosineAnnealingLR,\n",
    "    \"ReduceLROnPlateau\": lr_scheduler.ReduceLROnPlateau,\n",
    "    \"CyclicLR\": lr_scheduler.CyclicLR,\n",
    "    \"OneCycleLR\": lr_scheduler.OneCycleLR,\n",
    "    \"CosineAnnealingWarmRestarts\": lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93c5ef5-b53e-41f1-9ac2-7a9b09da6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Final, Type\n",
    "\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a7f527-c878-425d-bcde-260af0bf7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "OPTIMIZERS: Final[dict[str, Type[Optimizer]]] = {\n",
    "    \"Adadelta\": torch.optim.Adadelta,\n",
    "    # Implements Adadelta algorithm.\n",
    "    \"Adagrad\": torch.optim.Adagrad,\n",
    "    # Implements Adagrad algorithm.\n",
    "    \"Adam\": torch.optim.Adam,\n",
    "    # Implements Adam algorithm.\n",
    "    \"AdamW\": torch.optim.AdamW,\n",
    "    # Implements AdamW algorithm.\n",
    "    \"SparseAdam\": torch.optim.SparseAdam,\n",
    "    # Implements lazy version of Adam algorithm suitable for sparse tensors.\n",
    "    \"Adamax\": torch.optim.Adamax,\n",
    "    # Implements Adamax algorithm (a variant of Adam based on infinity norm).\n",
    "    \"ASGD\": torch.optim.ASGD,\n",
    "    # Implements Averaged Stochastic Gradient Descent.\n",
    "    \"LBFGS\": torch.optim.LBFGS,\n",
    "    # Implements L-BFGS algorithm, heavily inspired by minFunc.\n",
    "    \"RMSprop\": torch.optim.RMSprop,\n",
    "    # Implements RMSprop algorithm.\n",
    "    \"Rprop\": torch.optim.Rprop,\n",
    "    # Implements the resilient backpropagation algorithm.\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    # Implements stochastic gradient descent (optionally with momentum).\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960577a7-7b6f-422e-abaa-a0ab0958b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "def _opts():\n",
    "    _OPTIMIZERS: Final[dict[str, Type[Optimizer]]] = {\n",
    "    \"Adadelta\": torch.optim.Adadelta,\n",
    "    # Implements Adadelta algorithm.\n",
    "    \"Adagrad\": torch.optim.Adagrad,\n",
    "    # Implements Adagrad algorithm.\n",
    "    \"Adam\": torch.optim.Adam,\n",
    "    # Implements Adam algorithm.\n",
    "    \"AdamW\": torch.optim.AdamW,\n",
    "    # Implements AdamW algorithm.\n",
    "    \"SparseAdam\": torch.optim.SparseAdam,\n",
    "    # Implements lazy version of Adam algorithm suitable for sparse tensors.\n",
    "    \"Adamax\": torch.optim.Adamax,\n",
    "    # Implements Adamax algorithm (a variant of Adam based on infinity norm).\n",
    "    \"ASGD\": torch.optim.ASGD,\n",
    "    # Implements Averaged Stochastic Gradient Descent.\n",
    "    \"LBFGS\": torch.optim.LBFGS,\n",
    "    # Implements L-BFGS algorithm, heavily inspired by minFunc.\n",
    "    \"RMSprop\": torch.optim.RMSprop,\n",
    "    # Implements RMSprop algorithm.\n",
    "    \"Rprop\": torch.optim.Rprop,\n",
    "    # Implements the resilient backpropagation algorithm.\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    # Implements stochastic gradient descent (optionally with momentum).\n",
    "    }\n",
    "    return _optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29411d-30c8-4902-80f8-91f48e137bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
