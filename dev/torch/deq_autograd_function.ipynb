{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of the possibility of inputting a nn.Module as input into a custom autograd function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:01.180097Z",
     "iopub.status.busy": "2023-05-09T16:50:01.179971Z",
     "iopub.status.idle": "2023-05-09T16:50:02.091147Z",
     "shell.execute_reply": "2023-05-09T16:50:02.090572Z",
     "shell.execute_reply.started": "2023-05-09T16:50:01.180083Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "\n",
    "import warnings\n",
    "from typing import Any, Callable, NamedTuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, dot, nn\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup - Implement a Linear Solver (CGS - Conjugate Gradients Squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:02.092336Z",
     "iopub.status.busy": "2023-05-09T16:50:02.092148Z",
     "iopub.status.idle": "2023-05-09T16:50:02.099665Z",
     "shell.execute_reply": "2023-05-09T16:50:02.099208Z",
     "shell.execute_reply.started": "2023-05-09T16:50:02.092325Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CGS_STATE(NamedTuple):\n",
    "    \"\"\"State of the conjugate gradient squared solver.\"\"\"\n",
    "\n",
    "    L: Callable[[Tensor], Tensor]\n",
    "    \"\"\"The linear function.\"\"\"\n",
    "    x: Tensor\n",
    "    \"\"\"Vector: Current iterate.\"\"\"\n",
    "    r: Tensor\n",
    "    \"\"\"Vector: Residual vector.\"\"\"\n",
    "    p: Tensor\n",
    "    \"\"\"Vector: Search direction.\"\"\"\n",
    "    q: Tensor\n",
    "    \"\"\"Vector: \"\"\"\n",
    "    r0star: Tensor\n",
    "    \"\"\"Vector: Initial dual residual vector.\"\"\"\n",
    "    rho: Tensor\n",
    "    \"\"\"Scalar: Inner Product between r and r0star.\"\"\"\n",
    "\n",
    "\n",
    "def cgs_step(state: CGS_STATE) -> CGS_STATE:\n",
    "    \"\"\"Perform a single step of the conjugate gradient squared method.\"\"\"\n",
    "    # unpack state\n",
    "    L = state.L\n",
    "    x = state.x\n",
    "    r = state.r\n",
    "    p = state.p\n",
    "    q = state.q\n",
    "    r0star = state.r0star\n",
    "    rho_old = state.rho\n",
    "\n",
    "    # perform iteration\n",
    "    rho = dot(r, r0star)\n",
    "    beta = rho / rho_old\n",
    "    u = r + beta * q\n",
    "    p = u + beta * (q + beta * p)\n",
    "    v = L(p)\n",
    "    sigma = dot(v, r0star)\n",
    "    alpha = rho / sigma\n",
    "    q = u - alpha * v\n",
    "    r = r - alpha * L(u + q)\n",
    "    x = x + alpha * (u + q)\n",
    "    return CGS_STATE(L=L, x=x, r=r, p=p, q=q, r0star=r0star, rho=rho)\n",
    "\n",
    "\n",
    "def cgs(\n",
    "    L: nn.Module,\n",
    "    y: Tensor,\n",
    "    x0: Optional[Tensor] = None,\n",
    "    r0star: Optional[Tensor] = None,\n",
    "    maxiter: int = 100,\n",
    "    atol: float = 10**-8,\n",
    "    rtol: float = 10**-5,\n",
    ") -> CGS_STATE:\n",
    "    \"\"\"Solves linear equation L(x)=y.\"\"\"\n",
    "    tol = max(atol, rtol * y.norm())\n",
    "    x0 = torch.zeros_like(y) if x0 is None else x0\n",
    "    r0 = y - L(x0)\n",
    "    r0star = r0.clone() if r0star is None else r0star\n",
    "    p0 = torch.zeros_like(r0)\n",
    "    q0 = torch.zeros_like(r0)\n",
    "    rho0 = 1.0  # dot(r0, r0star)\n",
    "    state = CGS_STATE(L=L, x=x0, r=r0, p=p0, q=q0, r0star=r0star, rho=rho0)\n",
    "\n",
    "    for it in range(maxiter):\n",
    "        state = cgs_step(state)\n",
    "\n",
    "        if state.r.norm() <= tol:\n",
    "            print(f\"Converged after {it} iterations.\")\n",
    "            break\n",
    "    else:\n",
    "        warnings.warn(f\"No convergence after {maxiter} iterations.\")\n",
    "\n",
    "    residual = (y - L(state.x)).norm().item()\n",
    "    print(f\"Final {residual=:.4}  (r={state.r.norm().item():.4})\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test whether CGS works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:02.100926Z",
     "iopub.status.busy": "2023-05-09T16:50:02.100432Z",
     "iopub.status.idle": "2023-05-09T16:50:02.109920Z",
     "shell.execute_reply": "2023-05-09T16:50:02.109597Z",
     "shell.execute_reply.started": "2023-05-09T16:50:02.100895Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 8\n",
    "L = nn.Linear(N, N, bias=False)\n",
    "# L.weight = nn.Parameter(torch.eye(N) + torch.randn(N, N) / np.sqrt(N))\n",
    "y = torch.randn(N)\n",
    "x0 = torch.zeros_like(y)\n",
    "x_cgs = cgs(L, y).x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify against scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:02.110605Z",
     "iopub.status.busy": "2023-05-09T16:50:02.110356Z",
     "iopub.status.idle": "2023-05-09T16:50:02.221846Z",
     "shell.execute_reply": "2023-05-09T16:50:02.221323Z",
     "shell.execute_reply.started": "2023-05-09T16:50:02.110593Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import cgs as cgs_scipy\n",
    "\n",
    "A = L.weight.detach().numpy()\n",
    "b = y.numpy()\n",
    "x_ref, r = cgs_scipy(A, b, x0=np.zeros_like(b))\n",
    "print(f\"Final residual: {np.linalg.norm(A @ x_ref - b)}\")\n",
    "\n",
    "diff = np.mean((x_cgs.detach().numpy() - x_ref) ** 2)\n",
    "print(f\"MSE between custom and reference solution: {diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on a model\n",
    "\n",
    "We compute gradients for $‚Äñ\\text{deq-layer}(x)‚Äñ^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:02.223446Z",
     "iopub.status.busy": "2023-05-09T16:50:02.222916Z",
     "iopub.status.idle": "2023-05-09T16:50:02.233059Z",
     "shell.execute_reply": "2023-05-09T16:50:02.232526Z",
     "shell.execute_reply.started": "2023-05-09T16:50:02.223423Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fixed_point_iteration(\n",
    "    f: nn.Module,\n",
    "    x: Tensor,\n",
    "    maxiter: int = 100,\n",
    "    atol: float = 10**-8,\n",
    "    rtol: float = 10**-5,\n",
    ") -> Tensor:\n",
    "    \"\"\"Solves $z‚Åé=f(xÔºåz‚Åé)$ via FP iteration.\"\"\"\n",
    "    z = torch.zeros(f.hidden_size)\n",
    "    for it in range(maxiter):\n",
    "        z_new = f(x, z)\n",
    "        converged = (z_new - z).norm() <= rtol * z.norm() + atol\n",
    "        z = z_new\n",
    "        if converged:\n",
    "            print(f\"Converged after {it} iterations.\")\n",
    "            break\n",
    "    else:\n",
    "        warnings.warn(f\"No convergence after {maxiter} iterations.\")\n",
    "    return z\n",
    "\n",
    "\n",
    "input_size, hidden_size = 4, 3\n",
    "model = nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
    "x = torch.randn(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Automatic differentiation (through the fixed-point iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:02.234289Z",
     "iopub.status.busy": "2023-05-09T16:50:02.233918Z",
     "iopub.status.idle": "2023-05-09T16:50:07.486760Z",
     "shell.execute_reply": "2023-05-09T16:50:07.485480Z",
     "shell.execute_reply.started": "2023-05-09T16:50:02.234267Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.zero_grad(set_to_none=True)\n",
    "\n",
    "# forward: fixed point iteration\n",
    "z = fixed_point_iteration(model, x)\n",
    "\n",
    "# backward\n",
    "z.norm().pow(2).backward()\n",
    "reference_gradients = [w.grad for w in model.parameters()]\n",
    "print(reference_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:07.490814Z",
     "iopub.status.busy": "2023-05-09T16:50:07.490450Z",
     "iopub.status.idle": "2023-05-09T16:50:07.505919Z",
     "shell.execute_reply": "2023-05-09T16:50:07.504727Z",
     "shell.execute_reply.started": "2023-05-09T16:50:07.490783Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.zero_grad(set_to_none=True)\n",
    "\n",
    "# forward: fixed point iteration\n",
    "with torch.no_grad():\n",
    "    z = fixed_point_iteration(model, x)\n",
    "\n",
    "# backward setup.\n",
    "outer_grad = 2 * z  # ‚àÇ‚Äñz‚Äñ¬≤/‚àÇz = 2z\n",
    "zstar = z.requires_grad_()  # must enable grad to compute ‚àÇf/‚àÇz‚Åé\n",
    "fstar = model(x, zstar)\n",
    "\n",
    "# backward step 1: solve for $g ‚âî \\Bigl(ùïÄ - \\frac{‚àÇf}{‚àÇz‚Åé}\\Bigr)^{-‚ä§} y$\n",
    "L = lambda g: g - grad(fstar, zstar, g, retain_graph=True)[0]\n",
    "gstar = cgs(L, outer_grad).x\n",
    "\n",
    "# compute the outer grad\n",
    "manual_gradients = grad(fstar, model.parameters(), gstar)\n",
    "\n",
    "print(\"MSE between automatic gradients to manual gradients:\")\n",
    "for g1, g2 in zip(reference_gradients, manual_gradients):\n",
    "    print((g1 - g2).pow(2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using `register_hook` (https://implicit-layers-tutorial.org/deep_equilibrium_models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:07.507034Z",
     "iopub.status.busy": "2023-05-09T16:50:07.506836Z",
     "iopub.status.idle": "2023-05-09T16:50:07.511568Z",
     "shell.execute_reply": "2023-05-09T16:50:07.510675Z",
     "shell.execute_reply.started": "2023-05-09T16:50:07.507019Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DEQFixedPoint(nn.Module):\n",
    "    def __init__(self, f, solver):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.solver = solver\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute forward pass and re-engage autograd tape\n",
    "        with torch.no_grad():\n",
    "            z = fixed_point_iteration(self.f, x)\n",
    "        z = self.f(x, z)\n",
    "\n",
    "        # set up Jacobian vector product (without additional forward calls)\n",
    "        zstar = z.clone().detach().requires_grad_()\n",
    "        fstar = self.f(x, zstar)\n",
    "\n",
    "        def backward_hook(outer_grad):\n",
    "            L = lambda g: g - grad(fstar, zstar, g, retain_graph=True)[0]\n",
    "            gstar = self.solver(L, outer_grad).x\n",
    "            return gstar\n",
    "\n",
    "        z.register_hook(backward_hook)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:07.512487Z",
     "iopub.status.busy": "2023-05-09T16:50:07.512376Z",
     "iopub.status.idle": "2023-05-09T16:50:07.520269Z",
     "shell.execute_reply": "2023-05-09T16:50:07.519311Z",
     "shell.execute_reply.started": "2023-05-09T16:50:07.512476Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEQ = DEQFixedPoint(model, cgs)\n",
    "DEQ.zero_grad(set_to_none=True)\n",
    "\n",
    "print([w.grad for w in DEQ.parameters()])\n",
    "\n",
    "# forward\n",
    "y = DEQ(x)\n",
    "\n",
    "# backward\n",
    "y.norm().pow(2).backward()\n",
    "gradients_deq = [w.grad for w in DEQ.parameters()]\n",
    "\n",
    "print(\"MSE between automatic gradients to manual gradients:\")\n",
    "for g1, g2 in zip(reference_gradients, gradients_deq):\n",
    "    print((g1 - g2).pow(2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Using a custom autograd function (doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:07.521153Z",
     "iopub.status.busy": "2023-05-09T16:50:07.520973Z",
     "iopub.status.idle": "2023-05-09T16:50:07.527312Z",
     "shell.execute_reply": "2023-05-09T16:50:07.526886Z",
     "shell.execute_reply.started": "2023-05-09T16:50:07.521138Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DEQ_Layer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(f: nn.Module, x: Tensor, **kwargs: Any) -> Tensor:\n",
    "        \"\"\"Compute the fixed point $z‚Åé = f(x, z‚Åé)$.\"\"\"\n",
    "        zstar = fixed_point_iteration(f, x, **kwargs)\n",
    "        return zstar.requires_grad_()\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output, **_):\n",
    "        f, x = inputs\n",
    "        zstar = output\n",
    "        ctx.save_for_backward(f, x, zstar)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, ctx, grad_output):\n",
    "        f, x, zstar = ctx.saved_tensors\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            fstar = f(x, zstar)\n",
    "\n",
    "        # solve the linear system $(ùïÄ - ‚àÇf(xÔºåz‚Åé)/‚àÇz‚Åé)·µÄg = y$\n",
    "        L = lambda g: g - grad(fstar, zstar, g, retain_graph=True)[0]\n",
    "        gstar = cgs(L, grad_output).x\n",
    "\n",
    "        # compute the outer grads\n",
    "        grad_f = [\n",
    "            (grad(fstar, w, gstar) if w.requires_grad else None) for w in f.parameters()\n",
    "        ]\n",
    "        grad_x = grad(fstar, x, gstar) if x.requires_grad else None\n",
    "        return grad_f, grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:07.528049Z",
     "iopub.status.busy": "2023-05-09T16:50:07.527912Z",
     "iopub.status.idle": "2023-05-09T16:50:07.534048Z",
     "shell.execute_reply": "2023-05-09T16:50:07.533639Z",
     "shell.execute_reply.started": "2023-05-09T16:50:07.528037Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.zero_grad(set_to_none=True)\n",
    "\n",
    "# forward\n",
    "y = DEQ_Layer.apply(model, x).norm().pow(2)\n",
    "\n",
    "# backward\n",
    "y.backward()\n",
    "print([w.grad for w in model.parameters()])  # ‚úò no gradients...\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Proposed Solution via 'nn.Module.backward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:50:07.534826Z",
     "iopub.status.busy": "2023-05-09T16:50:07.534608Z",
     "iopub.status.idle": "2023-05-09T16:50:07.540264Z",
     "shell.execute_reply": "2023-05-09T16:50:07.539781Z",
     "shell.execute_reply.started": "2023-05-09T16:50:07.534811Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DEQFixedPoint(nn.Module):\n",
    "    def __init__(self, f, solver):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.solver = solver\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return fixed_point_iteration(self.f, x)\n",
    "\n",
    "    def backward(self, outer_grad):\n",
    "        \"\"\"Computes gradients for inputs of forward.\n",
    "\n",
    "        Since this a nn.Module, we ought to compute gradients both for 'self' and 'x'\n",
    "        \"\"\"\n",
    "        # let's just assume these are automatically captured somehow\n",
    "        # alternatively, one could make use of buffers.\n",
    "        x = self.forward.inputs\n",
    "        z = self.forward.outputs\n",
    "\n",
    "        # backward step 1: solve for $g ‚âî \\Bigl(ùïÄ - \\frac{‚àÇf}{‚àÇz‚Åé}\\Bigr)^{-‚ä§} y$\n",
    "        zstar = z.requires_grad_()\n",
    "        fstar = self.f(x, zstar)\n",
    "        L = lambda g: g - grad(fstar, zstar, g, retain_graph=True)[0]\n",
    "        gstar = self.solver(L, outer_grad)\n",
    "\n",
    "        # compute the outer grads\n",
    "        # grad_self = grad(fstar, self, gstar)  # <- would be nice to be able to do this.\n",
    "        grad_self = [\n",
    "            (grad(fstar, w, gstar) if w.requires_grad else None)\n",
    "            for w in self.parameters()\n",
    "        ]\n",
    "        grad_x = grad(fstar, x, gstar) if x.requires_grad else None\n",
    "        return grad_self, grad_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T11:10:32.389664Z",
     "iopub.status.busy": "2023-05-09T11:10:32.389438Z",
     "iopub.status.idle": "2023-05-09T11:10:32.395269Z",
     "shell.execute_reply": "2023-05-09T11:10:32.394724Z",
     "shell.execute_reply.started": "2023-05-09T11:10:32.389651Z"
    },
    "tags": []
   },
   "source": [
    "## Background: DEQ models\n",
    "\n",
    "In a Deep Equilibrium model, given input $x$, we use out model $f$, parameterized by $Œ∏$ to compute the fixed point\n",
    "\n",
    "\n",
    "$$ z‚Åé = f(z‚ÅéÔºåxÔºåŒ∏) $$\n",
    "\n",
    "Now, we need the gradients $\\frac{‚àÇz‚Åé}{‚àÇŒ∏}$. Computing the derivative on both sides yields\n",
    "\n",
    "$$ \\frac{‚àÇz‚Åé}{‚àÇŒ∏} \n",
    "= \\frac{‚àÇf}{‚àÇz‚Åé}\\frac{‚àÇz‚Åé}{‚àÇŒ∏} + \\frac{‚àÇf}{‚àÇx}\\frac{‚àÇx}{‚àÇŒ∏} + \\frac{‚àÇf}{‚àÇŒ∏}\\frac{‚àÇŒ∏}{‚àÇŒ∏}\n",
    "= \\frac{‚àÇf}{‚àÇz‚Åé}\\frac{‚àÇz‚Åé}{‚àÇŒ∏} + \\frac{‚àÇf}{‚àÇŒ∏}\n",
    "‚üπ \\Bigl(ùïÄ - \\frac{‚àÇf}{‚àÇz‚Åé}\\Bigr)\\frac{‚àÇz‚Åé}{‚àÇŒ∏} = \\frac{‚àÇf}{‚àÇŒ∏}\n",
    "$$\n",
    "\n",
    "In particular, the VJP given outer gradient $g$ is\n",
    "\n",
    "$$ \\frac{‚àÇz‚Åé}{‚àÇŒ∏}^‚ä§ y = \\frac{‚àÇf}{‚àÇŒ∏}^‚ä§ \\Bigl(ùïÄ - \\frac{‚àÇf}{‚àÇz‚Åé}\\Bigr)^{-‚ä§} y $$\n",
    "\n",
    "so, as an intermediate we need to compute \n",
    "\n",
    "$$ g ‚âî \\Bigl(ùïÄ - \\frac{‚àÇf}{‚àÇz‚Åé}\\Bigr)^{-‚ä§} y ‚ü∫ \\Bigl(ùïÄ - \\frac{‚àÇf}{‚àÇz‚Åé}\\Bigr)^‚ä§ g = y ‚ü∫ g + \\text{VJP}(f, z‚Åé, g) = y$$\n",
    "\n",
    "Once we have $g$ we can compute \n",
    "\n",
    "$$\\text{VJP(z‚ÅéÔºåŒ∏Ôºåy)} = \\text{VJP}(fÔºåŒ∏Ôºåg)$$\n",
    "\n",
    "In summary, the steps for computing the gradients are:\n",
    "\n",
    "1. **Forward:** given input $x$, return solution  $z‚Åé$  of the Fixed Point equation $z = f(zÔºåxÔºåŒ∏)$.\n",
    "2. **Backward:** Given outer gradients y, we need  to compute the gradients $\\frac{‚àÇz‚Åé}{‚àÇŒ∏}$.\n",
    "   1. Compute solution $g‚Åé$ of the linear system $g+\\text{VJP}(f, z‚ÅéÔºåg) = y$.\n",
    "   2. Compute $\\text{VJP(z‚ÅéÔºåŒ∏Ôºåy)} = \\text{VJP}(fÔºåŒ∏Ôºåg‚Åé)$.In summary, the steps for computing the gradients are:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
