{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of the possibility of inputting a nn.Module as input into a custom autograd function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Callable, NamedTuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, dot, nn\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Setup - Implement a Linear Solver (CGS - Conjugate Gradients Squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CGS_STATE(NamedTuple):\n",
    "    \"\"\"State of the conjugate gradient squared solver.\"\"\"\n",
    "\n",
    "    L: Callable[[Tensor], Tensor]\n",
    "    \"\"\"The linear function.\"\"\"\n",
    "    x: Tensor\n",
    "    \"\"\"Vector: Current iterate.\"\"\"\n",
    "    r: Tensor\n",
    "    \"\"\"Vector: Residual vector.\"\"\"\n",
    "    p: Tensor\n",
    "    \"\"\"Vector: Search direction.\"\"\"\n",
    "    q: Tensor\n",
    "    \"\"\"Vector: \"\"\"\n",
    "    r0star: Tensor\n",
    "    \"\"\"Vector: Initial dual residual vector.\"\"\"\n",
    "    rho: Tensor\n",
    "    \"\"\"Scalar: Inner Product between r and r0star.\"\"\"\n",
    "\n",
    "\n",
    "def cgs_step(state: CGS_STATE) -> CGS_STATE:\n",
    "    \"\"\"Perform a single step of the conjugate gradient squared method.\"\"\"\n",
    "    # unpack state\n",
    "    L = state.L\n",
    "    x = state.x\n",
    "    r = state.r\n",
    "    p = state.p\n",
    "    q = state.q\n",
    "    r0star = state.r0star\n",
    "    rho_old = state.rho\n",
    "\n",
    "    # perform iteration\n",
    "    rho = dot(r, r0star)\n",
    "    beta = rho / rho_old\n",
    "    u = r + beta * q\n",
    "    p = u + beta * (q + beta * p)\n",
    "    v = L(p)\n",
    "    sigma = dot(v, r0star)\n",
    "    alpha = rho / sigma\n",
    "    q = u - alpha * v\n",
    "    r = r - alpha * L(u + q)\n",
    "    x = x + alpha * (u + q)\n",
    "    return CGS_STATE(L=L, x=x, r=r, p=p, q=q, r0star=r0star, rho=rho)\n",
    "\n",
    "\n",
    "def cgs(\n",
    "    L: nn.Module,\n",
    "    y: Tensor,\n",
    "    x0: Optional[Tensor] = None,\n",
    "    r0star: Optional[Tensor] = None,\n",
    "    maxiter: int = 100,\n",
    "    atol: float = 10**-8,\n",
    "    rtol: float = 10**-5,\n",
    ") -> CGS_STATE:\n",
    "    \"\"\"Solves linear equation L(x)=y.\"\"\"\n",
    "    tol = max(atol, rtol * y.norm())\n",
    "    x0 = torch.zeros_like(y) if x0 is None else x0\n",
    "    r0 = y - L(x0)\n",
    "    r0star = r0.clone() if r0star is None else r0star\n",
    "    p0 = torch.zeros_like(r0)\n",
    "    q0 = torch.zeros_like(r0)\n",
    "    rho0 = 1.0  # dot(r0, r0star)\n",
    "    state = CGS_STATE(L=L, x=x0, r=r0, p=p0, q=q0, r0star=r0star, rho=rho0)\n",
    "\n",
    "    for it in range(maxiter):\n",
    "        state = cgs_step(state)\n",
    "\n",
    "        if state.r.norm() <= tol:\n",
    "            print(f\"Converged after {it} iterations.\")\n",
    "            break\n",
    "    else:\n",
    "        warnings.warn(f\"No convergence after {maxiter} iterations.\")\n",
    "\n",
    "    residual = (y - L(state.x)).norm().item()\n",
    "    print(f\"Final {residual=:.4}  (r={state.r.norm().item():.4})\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test whether CGS works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 8\n",
    "L = nn.Linear(N, N, bias=False)\n",
    "# L.weight = nn.Parameter(torch.eye(N) + torch.randn(N, N) / np.sqrt(N))\n",
    "y = torch.randn(N)\n",
    "x0 = torch.zeros_like(y)\n",
    "x_cgs = cgs(L, y).x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify against scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import cgs as cgs_scipy\n",
    "\n",
    "A = L.weight.detach().numpy()\n",
    "b = y.numpy()\n",
    "x_ref, r = cgs_scipy(A, b, x0=np.zeros_like(b))\n",
    "print(f\"Final residual: {np.linalg.norm(A @ x_ref - b)}\")\n",
    "\n",
    "diff = np.mean((x_cgs.detach().numpy() - x_ref) ** 2)\n",
    "print(f\"MSE between custom and reference solution: {diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Test on a model\n",
    "\n",
    "We compute gradients for $â€–\\text{deq-layer}(x)â€–^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fixed_point_iteration(\n",
    "    f: Callable[tuple[Tensor, Tensor], Tensor],\n",
    "    x: Tensor,\n",
    "    z0: Optional[Tensor] = None,\n",
    "    maxiter: int = 100,\n",
    "    atol: float = 10**-8,\n",
    "    rtol: float = 10**-5,\n",
    ") -> Tensor:\n",
    "    \"\"\"Solves $zâ=f(xï¼Œzâ)$ via FP iteration.\"\"\"\n",
    "    if isinstance(f, nn.Module) and z0 is None:\n",
    "        z = torch.zeros(f.hidden_size)\n",
    "    elif z0 is None:\n",
    "        z = torch.zeros_like(x)\n",
    "    else:\n",
    "        z = z0\n",
    "\n",
    "    for it in range(maxiter):\n",
    "        z_new = f(x, z)\n",
    "        converged = (z_new - z).norm() <= rtol * z.norm() + atol\n",
    "        z = z_new\n",
    "        if converged:\n",
    "            print(f\"Converged after {it} iterations.\")\n",
    "            break\n",
    "    else:\n",
    "        warnings.warn(f\"No convergence after {maxiter} iterations.\")\n",
    "    return z\n",
    "\n",
    "\n",
    "input_size, hidden_size = 4, 3\n",
    "model = nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
    "x = torch.randn(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Automatic differentiation (through the fixed-point iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.zero_grad(set_to_none=True)\n",
    "\n",
    "# forward: fixed point iteration\n",
    "z = fixed_point_iteration(model, x)\n",
    "\n",
    "# backward\n",
    "z.norm().pow(2).backward()\n",
    "reference_gradients = [w.grad for w in model.parameters()]\n",
    "print(reference_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.zero_grad(set_to_none=True)\n",
    "\n",
    "# forward: fixed point iteration\n",
    "with torch.no_grad():\n",
    "    z = fixed_point_iteration(model, x)\n",
    "\n",
    "# backward setup.\n",
    "outer_grad = 2 * z  # âˆ‚â€–zâ€–Â²/âˆ‚z = 2z\n",
    "zstar = z.requires_grad_()  # must enable grad to compute âˆ‚f/âˆ‚zâ\n",
    "fstar = model(x, zstar)\n",
    "\n",
    "# backward step 1: solve for $g â‰” \\Bigl(ğ•€ - \\frac{âˆ‚f}{âˆ‚zâ}\\Bigr)^{-âŠ¤} y$\n",
    "\n",
    "\n",
    "def L(g):\n",
    "    return g - grad(fstar, zstar, g, retain_graph=True)[0]\n",
    "\n",
    "\n",
    "gstar = cgs(L, outer_grad).x\n",
    "\n",
    "# compute the outer grad\n",
    "manual_gradients = grad(fstar, model.parameters(), gstar)\n",
    "\n",
    "print(\"MSE between automatic gradients to manual gradients:\")\n",
    "for g1, g2 in zip(reference_gradients, manual_gradients):\n",
    "    print((g1 - g2).pow(2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 3. Using `register_hook` (https://implicit-layers-tutorial.org/deep_equilibrium_models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DEQFixedPoint(nn.Module):\n",
    "    def __init__(self, f, solver):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.solver = solver\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute forward pass and re-engage autograd tape\n",
    "        with torch.no_grad():\n",
    "            z = fixed_point_iteration(self.f, x)\n",
    "        z = self.f(x, z)\n",
    "\n",
    "        # set up Jacobian vector product (without additional forward calls)\n",
    "        zstar = z.clone().detach().requires_grad_()\n",
    "        fstar = self.f(x, zstar)\n",
    "\n",
    "        def backward_hook(outer_grad):\n",
    "            def L(g):\n",
    "                return g - grad(fstar, zstar, g, retain_graph=True)[0]\n",
    "\n",
    "            gstar = self.solver(L, outer_grad).x\n",
    "            return gstar\n",
    "\n",
    "        z.register_hook(backward_hook)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEQ = DEQFixedPoint(model, cgs)\n",
    "DEQ.zero_grad(set_to_none=True)\n",
    "\n",
    "print([w.grad for w in DEQ.parameters()])\n",
    "\n",
    "# forward\n",
    "y = DEQ(x)\n",
    "\n",
    "# backward\n",
    "y.norm().pow(2).backward()\n",
    "gradients_deq = [w.grad for w in DEQ.parameters()]\n",
    "\n",
    "print(\"MSE between automatic gradients to manual gradients:\")\n",
    "for g1, g2 in zip(reference_gradients, gradients_deq):\n",
    "    print((g1 - g2).pow(2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 4. Using a custom autograd function - using [Chillee's](https://github.com/Chillee) suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "import torch.utils._pytree as pytree\n",
    "\n",
    "\n",
    "def rpartial(func, /, *fixed_args, **fixed_kwargs):\n",
    "    r\"\"\"Fix positional arguments from the right.\"\"\"\n",
    "\n",
    "    @wraps(func)\n",
    "    def _wrapper(*func_args, **func_kwargs):\n",
    "        return func(*(func_args + fixed_args), **(func_kwargs | fixed_kwargs))\n",
    "\n",
    "    return _wrapper\n",
    "\n",
    "\n",
    "def deq_layer_factory(\n",
    "    module: nn.Module,\n",
    "    fsolver=fixed_point_iteration,\n",
    "    fsolver_kwargs={},\n",
    "    bsolver=cgs,\n",
    "    bsolver_kwargs={},\n",
    ") -> Callable[[Tensor], Tensor]:\n",
    "    \"\"\"Create functional deq_layer for the given module.\"\"\"\n",
    "\n",
    "    params, param_spec = pytree.tree_flatten({\n",
    "        **dict(module.named_parameters()), **dict(module.named_buffers())\n",
    "    })\n",
    "\n",
    "    fsolver_kwargs = fsolver_kwargs | {\"z0\": torch.zeros(module.hidden_size)}\n",
    "\n",
    "    def func(x: Tensor, z: Tensor, *params_and_buffers) -> Tensor:\n",
    "        \"\"\"Function call $f(x, z, Î¸)$.\"\"\"\n",
    "        theta = pytree.tree_unflatten(params_and_buffers, param_spec)\n",
    "        return torch.func.functional_call(module, theta, (x, z))\n",
    "\n",
    "    class DEQ_Layer(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(x: Tensor, *params_and_buffers) -> Tensor:\n",
    "            \"\"\"Compute the fixed point $zâ = f(x, zâ)$.\"\"\"\n",
    "            f = rpartial(func, *params_and_buffers)\n",
    "            return fsolver(f, x, **fsolver_kwargs)\n",
    "\n",
    "        @staticmethod\n",
    "        def setup_context(ctx, inputs, output):\n",
    "            x, *params_and_buffers = inputs\n",
    "            zstar = output\n",
    "            f = rpartial(func, *params_and_buffers)\n",
    "\n",
    "            with torch.enable_grad():\n",
    "                # NOTE: without detach, we get an infinite loop.\n",
    "                zstar = zstar.detach().requires_grad_()\n",
    "                fstar = f(x, zstar)\n",
    "\n",
    "            ctx.save_for_backward(fstar, zstar, x)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            fstar, zstar, x = ctx.saved_tensors\n",
    "\n",
    "            # solve the linear system $(ğ•€ - âˆ‚f(xï¼Œzâ)/âˆ‚zâ)áµ€g = y$\n",
    "            def L(g):\n",
    "                return g - grad(fstar, zstar, g, retain_graph=True)[0]\n",
    "\n",
    "            gstar = bsolver(L, grad_output, **bsolver_kwargs).x\n",
    "\n",
    "            # compute the outer grads\n",
    "            grad_x = (\n",
    "                grad(fstar, x, gstar, retain_graph=True) if x.requires_grad else None\n",
    "            )\n",
    "            grad_f = grad(fstar, params, gstar)\n",
    "            return grad_x, *grad_f\n",
    "\n",
    "    return rpartial(DEQ_Layer.apply, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deq_layer = deq_layer_factory(model)\n",
    "\n",
    "model.zero_grad(set_to_none=True)\n",
    "y = deq_layer(x).norm().pow(2)\n",
    "y.backward()\n",
    "\n",
    "gradients_function = [w.grad for w in model.parameters()]\n",
    "print(\"MSE between automatic gradients to manual gradients:\")\n",
    "for g1, g2 in zip(reference_gradients, gradients_function):\n",
    "    print((g1 - g2).pow(2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 5. Wrapping layer factory into an `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DEQ_Module(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        f,\n",
    "        fsolver=fixed_point_iteration,\n",
    "        fsolver_kwargs={},\n",
    "        bsolver=cgs,\n",
    "        bsolver_kwargs={},\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.fsolver = rpartial(fsolver, **fsolver_kwargs)\n",
    "        self.bsolver = rpartial(bsolver, **bsolver_kwargs)\n",
    "        self.deq_layer = deq_layer_factory(\n",
    "            self.f, fsolver=self.fsolver, bsolver=self.bsolver\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.deq_layer(x)\n",
    "\n",
    "\n",
    "DEQ = DEQ_Module(model)\n",
    "\n",
    "DEQ.zero_grad(set_to_none=True)\n",
    "y = DEQ(x)\n",
    "y.norm().pow(2).backward()\n",
    "gradients_module = [w.grad for w in DEQ.parameters()]\n",
    "print(\"MSE between automatic gradients to manual gradients:\")\n",
    "for g1, g2 in zip(reference_gradients, gradients_module):\n",
    "    print((g1 - g2).pow(2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: DEQ models\n",
    "\n",
    "In a Deep Equilibrium model, given input $x$, we use out model $f$, parameterized by $Î¸$ to compute the fixed point\n",
    "\n",
    "\n",
    "$$ zâ = f(zâï¼Œxï¼ŒÎ¸) $$\n",
    "\n",
    "Now, we need the gradients $\\frac{âˆ‚zâ}{âˆ‚Î¸}$. Computing the derivative on both sides yields\n",
    "\n",
    "$$ \\frac{âˆ‚zâ}{âˆ‚Î¸}\n",
    "= \\frac{âˆ‚f}{âˆ‚zâ}\\frac{âˆ‚zâ}{âˆ‚Î¸} + \\frac{âˆ‚f}{âˆ‚x}\\frac{âˆ‚x}{âˆ‚Î¸} + \\frac{âˆ‚f}{âˆ‚Î¸}\\frac{âˆ‚Î¸}{âˆ‚Î¸}\n",
    "= \\frac{âˆ‚f}{âˆ‚zâ}\\frac{âˆ‚zâ}{âˆ‚Î¸} + \\frac{âˆ‚f}{âˆ‚Î¸}\n",
    "âŸ¹ \\Bigl(ğ•€ - \\frac{âˆ‚f}{âˆ‚zâ}\\Bigr)\\frac{âˆ‚zâ}{âˆ‚Î¸} = \\frac{âˆ‚f}{âˆ‚Î¸}\n",
    "$$\n",
    "\n",
    "In particular, the VJP given outer gradient $g$ is\n",
    "\n",
    "$$ \\frac{âˆ‚zâ}{âˆ‚Î¸}^âŠ¤ y = \\frac{âˆ‚f}{âˆ‚Î¸}^âŠ¤ \\Bigl(ğ•€ - \\frac{âˆ‚f}{âˆ‚zâ}\\Bigr)^{-âŠ¤} y $$\n",
    "\n",
    "so, as an intermediate we need to compute\n",
    "\n",
    "$$ g â‰” \\Bigl(ğ•€ - \\frac{âˆ‚f}{âˆ‚zâ}\\Bigr)^{-âŠ¤} y âŸº \\Bigl(ğ•€ - \\frac{âˆ‚f}{âˆ‚zâ}\\Bigr)^âŠ¤ g = y âŸº g + \\text{VJP}(f, zâ, g) = y$$\n",
    "\n",
    "Once we have $g$ we can compute\n",
    "\n",
    "$$\\text{VJP(zâï¼ŒÎ¸ï¼Œy)} = \\text{VJP}(fï¼ŒÎ¸ï¼Œg)$$\n",
    "\n",
    "In summary, the steps for computing the gradients are:\n",
    "\n",
    "1. **Forward:** given input $x$, return solution  $zâ$  of the Fixed Point equation $z = f(zï¼Œxï¼ŒÎ¸)$.\n",
    "2. **Backward:** Given outer gradients y, we need  to compute the gradients $\\frac{âˆ‚zâ}{âˆ‚Î¸}$.\n",
    "   1. Compute solution $gâ$ of the linear system $g+\\text{VJP}(f, zâï¼Œg) = y$.\n",
    "   2. Compute $\\text{VJP(zâï¼ŒÎ¸ï¼Œy)} = \\text{VJP}(fï¼ŒÎ¸ï¼Œgâ)$.In summary, the steps for computing the gradients are:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
