{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some sample text, and a first block of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:14:56.519678Z",
     "iopub.status.busy": "2023-05-04T18:14:56.519584Z",
     "iopub.status.idle": "2023-05-04T18:14:56.524960Z",
     "shell.execute_reply": "2023-05-04T18:14:56.524674Z",
     "shell.execute_reply.started": "2023-05-04T18:14:56.519667Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:14:56.525689Z",
     "iopub.status.busy": "2023-05-04T18:14:56.525538Z",
     "iopub.status.idle": "2023-05-04T18:14:57.367385Z",
     "shell.execute_reply": "2023-05-04T18:14:57.366804Z",
     "shell.execute_reply.started": "2023-05-04T18:14:56.525678Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:14:57.368014Z",
     "iopub.status.busy": "2023-05-04T18:14:57.367858Z",
     "iopub.status.idle": "2023-05-04T18:14:57.378230Z",
     "shell.execute_reply": "2023-05-04T18:14:57.377844Z",
     "shell.execute_reply.started": "2023-05-04T18:14:57.368003Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import KW_ONLY, dataclass, field\n",
    "from typing import ClassVar, Final, NamedTuple, Optional, Protocol\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, dot, norm, rsqrt, sqrt\n",
    "from torch.linalg import solve\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Solver(Protocol):\n",
    "    \"\"\"Protocol for solvers.\"\"\"\n",
    "\n",
    "    maxiter: int\n",
    "    \"\"\"Maximum number of iterations.\"\"\"\n",
    "    atol: float\n",
    "    \"\"\"Absolute tolerance.\"\"\"\n",
    "    rtol: float\n",
    "    \"\"\"Relative tolerance.\"\"\"\n",
    "\n",
    "    def __call__(self, x0: Tensor) -> Tensor:\n",
    "        \"\"\"Solve the linear system.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def step(state: tuple[Tensor, ...]) -> tuple[Tensor, ...]:\n",
    "        \"\"\"Pure function representing the state transition.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class State(Protocol):\n",
    "    \"\"\"Protocol for solver states.\"\"\"\n",
    "\n",
    "    x: Tensor\n",
    "    \"\"\"Current iterate.\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseSolver(ABC):\n",
    "    \"\"\"Base class for solvers.\"\"\"\n",
    "\n",
    "    requires_transpose: ClassVar[bool]\n",
    "    \"\"\"Whether the solver requires the transpose of the operator.\"\"\"\n",
    "    requires_symmetric: ClassVar[bool]\n",
    "    \"\"\"Whether the solver requires the operator to be symmetric.\"\"\"\n",
    "    requires_positive_definite: ClassVar[bool]\n",
    "    \"\"\"Whether the solver requires the operator to be positive definite.\"\"\"\n",
    "    requires_finite_steps: ClassVar[bool]\n",
    "    \"\"\"Whether the solver terminates in a finite number of steps.\"\"\"\n",
    "\n",
    "    L: nn.Module\n",
    "    \"\"\"Linear operator.\"\"\"\n",
    "    y: Tensor\n",
    "    \"\"\"Right-hand side of the linear system.\"\"\"\n",
    "\n",
    "    _: KW_ONLY\n",
    "\n",
    "    maxiter: Final[int] = 1000\n",
    "    \"\"\"Maximum number of iterations.\"\"\"\n",
    "    atol: Final[float] = 1e-8\n",
    "    \"\"\"Absolute tolerance.\"\"\"\n",
    "    rtol: Final[float] = 1e-5\n",
    "    \"\"\"Relative tolerance.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def initial_state(self, x0: Optional[Tensor] = None) -> State:\n",
    "        \"\"\"Initialize the solver state.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, state: State) -> State:\n",
    "        \"\"\"Perform a single step of the solver.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def condition(self, new_state: State, old_state: State) -> bool:\n",
    "        \"\"\"Check if the solver has converged.\"\"\"\n",
    "        x_new = new_state.x\n",
    "        x_old = old_state.x\n",
    "        return (x_new - x_old).norm() < self.atol * x_old.norm() + self.rtol\n",
    "\n",
    "    def solve(self, x0: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"Solve the linear system.\"\"\"\n",
    "        state = self.initial_state(x0)\n",
    "\n",
    "        for it in range(self.maxiter):\n",
    "            new_state = self.step(state)\n",
    "            converged = self.condition(new_state, state)\n",
    "            state = new_state\n",
    "\n",
    "            if converged:\n",
    "                logger.info(\"Converged after %s iterations.\", it)\n",
    "                break\n",
    "        else:\n",
    "            logger.warning(\"No convergence after %s iterations.\", self.maxiter)\n",
    "\n",
    "        return state.x\n",
    "\n",
    "\n",
    "class CGS_STATE(NamedTuple):\n",
    "    \"\"\"State of the conjugate gradient squared solver.\"\"\"\n",
    "\n",
    "    x: Tensor\n",
    "    \"\"\"Vector: Current iterate.\"\"\"\n",
    "    r: Tensor\n",
    "    \"\"\"Vector: Residual vector.\"\"\"\n",
    "    p: Tensor\n",
    "    \"\"\"Vector: Search direction.\"\"\"\n",
    "    u: Tensor\n",
    "    \"\"\"Vector: Auxiliary vector.\"\"\"\n",
    "    rho: Tensor\n",
    "    \"\"\"Scalar: Inner Product between r and rstar.\"\"\"\n",
    "\n",
    "\n",
    "class CGS_Solver(BaseSolver):\n",
    "    \"\"\"Conjugate Gradient Squared solver.\"\"\"\n",
    "\n",
    "    requires_transpose: ClassVar[bool] = NotImplemented\n",
    "    requires_symmetric: ClassVar[bool] = NotImplemented\n",
    "    requires_positive_definite: ClassVar[bool] = NotImplemented\n",
    "\n",
    "    rstar: Tensor\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, state: CGS_STATE) -> CGS_STATE:\n",
    "        # unpack state\n",
    "        x = state.x\n",
    "        r = state.r\n",
    "        p = state.p\n",
    "        u = state.u\n",
    "        rho_old = state.rho\n",
    "\n",
    "        # perform iteration\n",
    "        v = self.L(p)\n",
    "        alpha = rho_old / dot(v, self.rstar)\n",
    "        q = u - alpha * v\n",
    "        x += alpha * (u + q)\n",
    "        r -= alpha * self.L(u + q)\n",
    "        rho = dot(r, self.rstar)\n",
    "        beta = rho / rho_old\n",
    "        u = r + beta * q\n",
    "        p = u + beta * (q + beta * p)\n",
    "\n",
    "        return CGS_STATE(x=x, r=r, p=p, u=u, rho=rho)\n",
    "\n",
    "    def initial_state(self, x0: Optional[Tensor] = None) -> CGS_STATE:\n",
    "        r0 = self.y - self.L(x0)\n",
    "        p0 = r0.clone()\n",
    "        u0 = r0.clone()\n",
    "        rho0 = dot(r0, self.rstar)\n",
    "        return CGS_STATE(x=x0, r=r0, p=p0, u=u0, rho=rho0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:17:25.746942Z",
     "iopub.status.busy": "2023-05-04T18:17:25.746773Z",
     "iopub.status.idle": "2023-05-04T18:17:25.767885Z",
     "shell.execute_reply": "2023-05-04T18:17:25.767344Z",
     "shell.execute_reply.started": "2023-05-04T18:17:25.746929Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "L = nn.Linear(5, 5)\n",
    "y = torch.randn(5)\n",
    "x0 = torch.zeros(L.in_features)\n",
    "solver = CGS_Solver(L, y)\n",
    "solver.solve(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:13:46.855508Z",
     "iopub.status.busy": "2023-05-04T18:13:46.855311Z",
     "iopub.status.idle": "2023-05-04T18:13:47.100674Z",
     "shell.execute_reply": "2023-05-04T18:13:47.100001Z",
     "shell.execute_reply.started": "2023-05-04T18:13:46.855496Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    CGS_Solver(L, g).solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:00:17.804819Z",
     "iopub.status.busy": "2023-05-04T18:00:17.804644Z",
     "iopub.status.idle": "2023-05-04T18:00:17.815190Z",
     "shell.execute_reply": "2023-05-04T18:00:17.814734Z",
     "shell.execute_reply.started": "2023-05-04T18:00:17.804807Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T17:52:18.487355Z",
     "iopub.status.busy": "2023-05-04T17:52:18.487074Z",
     "iopub.status.idle": "2023-05-04T17:52:18.489595Z",
     "shell.execute_reply": "2023-05-04T17:52:18.489306Z",
     "shell.execute_reply.started": "2023-05-04T17:52:18.487343Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def solve(f: nn.Module, y: Tensor, x0: Optional[Tensor] = None) -> Tensor:\n",
    "    \"\"\"Given a linear function f, solve f(x)=y with initial guess x0.\"\"\"\n",
    "    x0 = torch.zeros(f.input_size) if x0 is None else x0\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:14:09.942424Z",
     "iopub.status.busy": "2023-05-04T18:14:09.942201Z",
     "iopub.status.idle": "2023-05-04T18:14:09.946444Z",
     "shell.execute_reply": "2023-05-04T18:14:09.946118Z",
     "shell.execute_reply.started": "2023-05-04T18:14:09.942411Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DEQ_Layer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(f: nn.Module, x: Tensor) -> Tensor:\n",
    "        # We wish to save dx for backward. In order to do so, it must\n",
    "        # be returned as an output.\n",
    "        z = torch.zeros(f.hidden_size)\n",
    "        with torch.no_grad():\n",
    "            for k in range(100):\n",
    "                z = f(x, z)\n",
    "        return z.requires_grad_()\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, outputs):\n",
    "        f, x = inputs\n",
    "        z = outputs\n",
    "        z0 = z.clone().detach().requires_grad_()\n",
    "        f0 = f(x, z0).requires_grad_()\n",
    "        ctx.save_for_backward(z0, f0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        z0, f0 = ctx.saved_tensors\n",
    "        g = grad_output\n",
    "        L = lambda y: grad(f0, z0, y, retain_graph=True)[0] + g\n",
    "        return CGS_Solver(L, g).solve()\n",
    "\n",
    "\n",
    "# Wrap MyCube in a function so that it is clearer what the output is\n",
    "def deq_layer(f: nn.Module, x: Tensor):\n",
    "    result = DEQ_Layer.apply(f, x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:14:11.256553Z",
     "iopub.status.busy": "2023-05-04T18:14:11.256399Z",
     "iopub.status.idle": "2023-05-04T18:14:11.259985Z",
     "shell.execute_reply": "2023-05-04T18:14:11.259581Z",
     "shell.execute_reply.started": "2023-05-04T18:14:11.256543Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = nn.RNNCell(input_size=5, hidden_size=5)\n",
    "x = torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:14:21.641020Z",
     "iopub.status.busy": "2023-05-04T18:14:21.640761Z",
     "iopub.status.idle": "2023-05-04T18:14:21.647365Z",
     "shell.execute_reply": "2023-05-04T18:14:21.646965Z",
     "shell.execute_reply.started": "2023-05-04T18:14:21.640995Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = deq_layer(f, x).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:14:12.827006Z",
     "iopub.status.busy": "2023-05-04T18:14:12.826760Z",
     "iopub.status.idle": "2023-05-04T18:14:12.833679Z",
     "shell.execute_reply": "2023-05-04T18:14:12.833271Z",
     "shell.execute_reply.started": "2023-05-04T18:14:12.826984Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    ".backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:07:21.467436Z",
     "iopub.status.busy": "2023-05-04T18:07:21.467159Z",
     "iopub.status.idle": "2023-05-04T18:07:21.470373Z",
     "shell.execute_reply": "2023-05-04T18:07:21.470110Z",
     "shell.execute_reply.started": "2023-05-04T18:07:21.467422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:11:08.390475Z",
     "iopub.status.busy": "2023-05-04T18:11:08.390316Z",
     "iopub.status.idle": "2023-05-04T18:11:08.393099Z",
     "shell.execute_reply": "2023-05-04T18:11:08.392793Z",
     "shell.execute_reply.started": "2023-05-04T18:11:08.390464Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "z = torch.zeros(f.hidden_size)\n",
    "z = f(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T18:11:17.300992Z",
     "iopub.status.busy": "2023-05-04T18:11:17.300831Z",
     "iopub.status.idle": "2023-05-04T18:11:17.304562Z",
     "shell.execute_reply": "2023-05-04T18:11:17.304230Z",
     "shell.execute_reply.started": "2023-05-04T18:11:17.300980Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "z = f(x, z)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
